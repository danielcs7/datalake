[2025-02-24T08:37:08.323+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-24T07:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-24T07:20:00+00:00'
[2025-02-24T08:37:08.326+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-24T08:37:08.381+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-24T08:37:08.385+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-24T08:37:08.506+0000] {subprocess.py:99} INFO - Output:
[2025-02-24T08:37:08.534+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-24T08:37:08.686+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-24T08:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-24T08:20:00+00:00'
[2025-02-24T08:37:08.696+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-24T08:37:08.773+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-24T08:37:08.779+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-24T08:37:08.859+0000] {subprocess.py:99} INFO - Output:
[2025-02-24T08:37:08.910+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
cessamento UPLOAD BRONZE...
[2025-02-24T08:37:08.162+0000] {subprocess.py:99} INFO - Output:
[2025-02-24T08:37:08.185+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
4T08:37:04.279+0000] {logging_mixin.py:190} INFO - [2025-02-24T08:37:04.278+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-24 08:30:00+00:00, run_after=2025-02-24 08:40:00+00:00
[2025-02-24T08:37:04.534+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.913 seconds
"ocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/1_uploadBronze/1_upsertBronze.py"", line 75, in <module>"
[2025-02-24T07:27:03.918+0000] {subprocess.py:106} INFO -     download_file(landzone_bucket, file_path, local_landzone_file)
"[2025-02-24T07:27:03.961+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/1_uploadBronze/1_upsertBronze.py"", line 21, in download_file"
[2025-02-24T07:27:04.035+0000] {subprocess.py:106} INFO -     minio_client.fget_object(bucket_name, file_path, local_path)
"[2025-02-24T07:27:04.045+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1150, in fget_object"
[2025-02-24T07:27:04.047+0000] {subprocess.py:106} INFO -     for data in response.stream(amt=1024 * 1024):
[2025-02-24T07:27:04.051+0000] {subprocess.py:106} INFO -                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T07:27:04.052+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/urllib3/response.py"", line 1066, in stream"
[2025-02-24T07:27:04.054+0000] {subprocess.py:106} INFO -     data = self.read(amt=amt, decode_content=decode_content)
[2025-02-24T07:27:04.056+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T07:27:04.058+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/urllib3/response.py"", line 983, in read"
[2025-02-24T07:27:04.060+0000] {subprocess.py:106} INFO -     data = self._raw_read(amt)
[2025-02-24T07:27:04.093+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^^^^
"[2025-02-24T07:27:04.094+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/urllib3/response.py"", line 878, in _raw_read"
[2025-02-24T07:27:04.096+0000] {subprocess.py:106} INFO -     with self._error_catcher():
[2025-02-24T07:27:04.098+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T07:27:04.100+0000] {subprocess.py:106} INFO -   File ""/usr/local/lib/python3.12/contextlib.py"", line 158, in __exit__"
[2025-02-24T07:27:04.106+0000] {subprocess.py:106} INFO -     self.gen.throw(value)
"[2025-02-24T07:27:04.136+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/urllib3/response.py"", line 778, in _error_catcher"
[2025-02-24T07:27:04.140+0000] {subprocess.py:106} INFO -     raise ProtocolError(arg, e) from e
[2025-02-24T07:27:04.142+0000] {subprocess.py:106} INFO - urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(130488 bytes read, 11645563 more expected)', IncompleteRead(130488 bytes read, 11645563 more expected))
[2025-02-24T07:27:04.836+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-24T07:27:04.840+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-24T07:27:04.902+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T07:27:04.949+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T23:00:00+00:00, execution_date=20250223T230000, start_date=20250223T231355, end_date=20250224T072704
[2025-02-24T07:27:05.130+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-24T07:27:05.134+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 111 for task process_bronze (Bash command failed. The command returned a non-zero exit code 1.; 2775)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T07:27:05.228+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-24T07:27:59.601+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-24T07:27:59.623+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
4T07:26:31.951+0000] {subprocess.py:106} INFO -     conn = duckdb.connect(db_file)
[2025-02-24T07:26:31.953+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T07:26:31.955+0000] {subprocess.py:106} INFO - duckdb.duckdb.IOException: IO Error: Could not set lock on file ""/Volumes/MACBACKUP/workspaceDlake/dlake/db1.duckdb"": Conflicting lock is held in /usr/local/bin/python3.12 (PID 2880). See also https://duckdb.org/docs/connect/concurrency"
[2025-02-24T07:26:32.259+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-24T07:26:32.265+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-24T07:26:32.309+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T07:26:32.328+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-24T07:10:00+00:00, execution_date=20250224T071000, start_date=20250224T072616, end_date=20250224T072632
[2025-02-24T07:26:32.400+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-24T07:26:32.401+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 121 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2876)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T07:26:32.469+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-24T07:26:32.554+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-24T07:26:32.580+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T01:15:48.528+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-24T01:00:00+00:00, execution_date=20250224T010000, start_date=20250224T011532, end_date=20250224T011548
[2025-02-24T01:15:48.571+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-24T01:15:48.591+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-24T01:15:48.581+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 113 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2789)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T01:15:48.595+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 112 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2787)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T01:15:48.674+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-24T01:15:48.686+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-24T01:15:48.857+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-24T01:15:48.862+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 114 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2790)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T01:15:48.950+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-24T01:15:48.974+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-24T01:15:48.994+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-24T01:15:49.004+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-24T01:15:49.026+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-24T01:15:49.117+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-24T01:15:49.123+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-24T05:15:33.948+0000] {job.py:229} INFO - Heartbeat recovered after 14385.67 seconds
35.240+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:46:35.898+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:46:35.896+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:30:00+00:00, run_after=2025-02-23 14:40:00+00:00
[2025-02-23T14:46:36.010+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.125 seconds
[2025-02-23T14:47:06.714+0000] {processor.py:186} INFO - Started process (PID=71) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:47:06.769+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:47:06.780+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:47:06.778+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:47:08.397+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:47:08.899+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:47:08.897+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:47:09.002+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:47:09.001+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:30:00+00:00, run_after=2025-02-23 14:40:00+00:00
[2025-02-23T14:47:09.171+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.519 seconds
[2025-02-23T14:47:40.269+0000] {processor.py:186} INFO - Started process (PID=85) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:47:40.518+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:47:40.554+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:47:40.550+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:47:46.442+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:47:46.801+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:47:46.800+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:47:47.543+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:47:47.542+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:40:00+00:00, run_after=2025-02-23 14:50:00+00:00
[2025-02-23T14:47:48.184+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 8.185 seconds
[2025-02-23T14:48:20.969+0000] {processor.py:186} INFO - Started process (PID=91) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:48:21.045+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:48:21.159+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:48:21.055+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:48:28.042+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:48:28.930+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:48:28.928+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:48:29.442+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:48:29.441+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:40:00+00:00, run_after=2025-02-23 14:50:00+00:00
[2025-02-23T14:48:29.786+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 9.109 seconds
[2025-02-23T14:49:02.207+0000] {processor.py:186} INFO - Started process (PID=93) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:49:02.258+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags[2025-02-23T18:45:58.903+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 18:30:00+00:00
ag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:49:10.499+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:49:11.048+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:49:11.047+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:49:11.446+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:49:11.440+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:40:00+00:00, run_after=2025-02-23 14:50:00+00:00
[2025-02-23T14:49:11.815+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 9.953 seconds
[2025-02-23T14:49:43.670+0000] {processor.py:186} INFO - Started process (PID=95) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:49:43.838+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:49:44.097+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:49:44.090+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:49:54.196+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:49:55.236+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:49:55.235+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:50:09.550+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:50:09.544+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:50:10.824+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 27.567 seconds
[2025-02-23T14:50:43.925+0000] {processor.py:186} INFO - Started process (PID=97) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:50:44.069+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:50:44.148+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:50:44.144+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:51:14.863+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:51:14.861+0000] {timeout.py:68} ERROR - Process timed out, PID: 97
[2025-02-23T14:51:16.412+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:51:15.427+0000] {dagbag.py:387} ERROR - Failed to import: /opt/airflow/dags/dag_scheduler_v2.py
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/dagbag.py"", line 383, in parse"
    loader.exec_module(new_module)
"  File ""<frozen importlib._bootstrap_external>"", line 999, in exec_module"
"  File ""<frozen importlib._bootstrap>"", line 488, in _call_with_frames_removed"
"  File ""/opt/airflow/dags/dag_scheduler_v2.py"", line 7, in <module>"
    from minio import Minio # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/minio/__init__.py"", line 45, in <module>"
    from .minioadmin import MinioAdmin as MinioAdmin
"  File ""/home/airflow/.local/lib/python3.12/site-packages/minio/minioadmin.py"", line 44, in <module>"
    from .crypto import decrypt, encrypt
"  File ""/home/airflow/.local/lib/python3.12/site-packages/minio/crypto.py"", line 24, in <module>"
    from Crypto.Cipher import AES, ChaCha20_Poly1305
"  File ""/home/airflow/.local/lib/python3.12/site-packages/Crypto/Cipher/__init__.py"", line 35, in <module>"
    from Crypto.Cipher._mode_siv import _create_siv_cipher
"  File ""/home/airflow/.local/lib/python3.12/site-packages/Crypto/Cipher/_mode_siv.py"", line 44, in <module>"
    from Crypto.Protocol.KDF import _S2V
"  File ""/home/airflow/.local/lib/python3.12/site-packages/Crypto/Protocol/KDF.py"", line 32, in <module>"
    from Crypto.Hash import SHA1, SHA256, HMAC, CMAC, BLAKE2s
"  File ""/home/airflow/.local/lib/python3.12/site-packages/Crypto/Hash/SHA1.py"", line 29, in <module>"
"    _raw_sha1_lib = load_pycryptodome_raw_lib(""Crypto.Hash._SHA1"","
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/Crypto/Util/_raw_api.py"", line 312, in load_pycryptodome_raw_lib"
    return load_lib(full_name, cdecl)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/Crypto/Util/_raw_api.py"", line 106, in load_lib"
    ffi.cdef(cdecl)
"  File ""/home/airflow/.local/lib/python3.12/site-packages/cffi/api.py"", line 112, in cdef"
    self._cdef(csource, override=override, packed=packed, pack=pack)
"  File ""/home/airflow/.local/lib/python3.12/site-packages/cffi/api.py"", line 126, in _cdef"
    self._parser.parse(csource, override=override, **options)
"  File ""/home/airflow/.local/lib/python3.12/site-packages/cffi/cparser.py"", line 390, in parse"
    self._internal_parse(csource)
"  File ""/home/airflow/.local/lib/python3.12/site-packages/cffi/cparser.py"", line 395, in _internal_parse"
    ast, macros, csource = self._parse(csource)
                           ^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/cffi/cparser.py"", line 337, in _parse"
    ast = _get_parser().parse(fullcsource)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/pycparser/c_parser.py"", line 147, in parse"
    return self.cparser.parse(
           ^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/pycparser/ply/yacc.py"", line 331, in parse"
    return self.parseopt_notrack(input, lexer, debug, tracking, tokenfunc)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/pycparser/ply/yacc.py"", line 1118, in parseopt_notrack"
    p.callable(pslice)
"  File ""/home/airflow/.local/lib/python3.12/site-packages/pycparser/c_parser.py"", line 1274, in p_pointer"
    coord = self._token_coord(p, 1)
            ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/pycparser/plyparser.py"", line 64, in _token_coord"
    return self._coord(p.lineno(token_idx), column)
                       ^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/pycparser/ply/yacc.py"", line 257, in lineno"
    def lineno(self, n):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/timeout.py"", line 69, in handle_timeout"
    raise AirflowTaskTimeout(self.error_message)
airflow.exceptions.AirflowTaskTimeout: DagBag import timeout for /opt/airflow/dags/dag_scheduler_v2.py after 30.0s.
Please take a look at these docs to improve your DAG import time:
* https://airflow.apache.org/docs/apache-airflow/2.10.5/best-practices.html#top-level-python-code
* https://airflow.apache.org/docs/apache-airflow/2.10.5/best-practices.html#reducing-dag-complexity, PID: 97
[2025-02-23T14:51:16.584+0000] {processor.py:927} WARNING - No viable dags retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:51:17.240+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 33.401 seconds
[2025-02-23T14:51:49.087+0000] {processor.py:186} INFO - Started process (PID=100) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:51:49.134+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:51:49.214+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:51:49.209+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:51:51.521+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:51:55.523+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:51:55.522+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:51:55.934+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:51:55.933+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:51:56.265+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 7.416 seconds
[2025-02-23T14:52:27.028+0000] {processor.py:186} INFO - Started process (PID=102) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:52:27.048+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:52:27.113+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:52:27.111+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:52:29.596+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:52:29.694+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:52:29.693+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:52:29.796+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:52:29.794+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:52:30.727+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.853 seconds
[2025-02-23T14:53:01.770+0000] {processor.py:186} INFO - Started process (PID=104) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:53:01.824+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:53:01.837+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:53:01.835+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:53:04.758+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:53:05.173+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:53:05.170+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:53:07.714+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:53:07.711+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:53:07.976+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 6.580 seconds
[2025-02-23T14:53:39.266+0000] {processor.py:186} INFO - Started process (PID=106) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:53:39.377+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:53:39.388+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:53:39.384+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:53:52.302+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:53:59.352+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:53:59.350+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:54:00.898+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:54:00.863+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:54:01.413+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 22.498 seconds
[2025-02-23T14:54:33.160+0000] {processor.py:186} INFO - Started process (PID=108) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:54:33.299+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:54:33.368+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:54:33.362+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:54:38.617+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:54:39.244+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:54:39.244+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:54:39.521+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:54:39.517+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:54:39.697+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 6.652 seconds
[2025-02-23T14:55:10.829+0000] {processor.py:186} INFO - Started process (PID=110) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:55:10.857+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:55:10.863+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:55:10.862+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:55:12.369+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:55:12.589+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:55:12.587+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:55:12.721+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:55:12.720+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:55:12.843+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.047 seconds
[2025-02-23T14:55:43.083+0000] {processor.py:186} INFO - Started process (PID=113) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:55:43.086+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:55:43.095+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:55:43.094+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:55:44.385+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:55:44.471+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:55:44.470+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:55:44.561+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:55:44.560+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:55:44.627+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.580 seconds
[2025-02-23T14:56:15.015+0000] {processor.py:186} INFO - Started process (PID=115) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:56:15.024+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:56:15.033+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:56:15.031+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:56:15.940+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:56:16.063+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:56:16.061+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:56:17.310+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:56:17.310+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:56:17.394+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.438 seconds
[2025-02-23T14:56:47.973+0000] {processor.py:186} INFO - Started process (PID=117) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:56:48.011+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:56:48.018+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:56:48.017+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:56:48.538+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:56:49.373+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:56:49.372+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:56:49.527+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:56:49.527+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:56:49.807+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.852 seconds
[2025-02-23T14:57:20.085+0000] {processor.py:186} INFO - Started process (PID=119) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:57:20.088+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:57:20.092+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:57:20.091+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:57:21.000+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:57:21.084+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:57:21.083+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:57:21.175+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:57:21.174+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:57:21.226+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.160 seconds
[2025-02-23T14:57:51.708+0000] {processor.py:186} INFO - Started process (PID=125) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:57:51.711+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:57:51.718+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:57:51.716+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:57:57.708+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:57:58.328+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:57:58.326+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:57:58.827+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:57:58.826+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:57:59.873+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 8.196 seconds
[2025-02-23T14:58:28.542+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T14:58:28.779+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze manual__2025-02-23T14:47:15.591803+00:00 [queued]>
[2025-02-23T14:58:28.840+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze manual__2025-02-23T14:47:15.591803+00:00 [queued]>
[2025-02-23T14:58:28.843+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T14:58:28.945+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 14:47:15.591803+00:00
[2025-02-23T14:58:28.971+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=154) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T14:58:28.986+0000] {standard_task_runner.py:72} INFO - Started process 155 to run task
[2025-02-23T14:58:28.983+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'manual__2025-02-23T14:47:15.591803+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpbeo3szrv']
[2025-02-23T14:58:29.000+0000] {standard_task_runner.py:105} INFO - Job 5: Subtask process_bronze
[2025-02-23T14:58:30.649+0000] {processor.py:186} INFO - Started process (PID=127) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:58:30.736+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:58:30.757+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:58:30.754+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:58:32.316+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:58:32.555+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:58:32.553+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:58:32.796+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:58:32.794+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:58:33.541+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.955 seconds
[2025-02-23T14:58:37.721+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.parquet' baixado para 'temp_landzone_fornecedores_db1.parquet'.
[2025-02-23T14:58:37.725+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db1.parquet' enviado para 'datalake/bronze/db1/fornecedores_db1.parquet'.
[2025-02-23T14:58:37.729+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.parquet' baixado para 'temp_landzone_fornecedores_db2.parquet'.
[2025-02-23T14:58:37.733+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db2.parquet' enviado para 'datalake/bronze/db2/fornecedores_db2.parquet'.
[2025-02-23T14:58:37.738+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.parquet' baixado para 'temp_landzone_fornecedores_db3.parquet'.
[2025-02-23T14:58:37.744+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db3.parquet' enviado para 'datalake/bronze/db3/fornecedores_db3.parquet'.
[2025-02-23T14:58:37.747+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.parquet' baixado para 'temp_landzone_produtos_db1.parquet'.
[2025-02-23T14:58:37.750+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db1.parquet' enviado para 'datalake/bronze/db1/produtos_db1.parquet'.
[2025-02-23T14:58:37.753+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.parquet' baixado para 'temp_landzone_produtos_db2.parquet'.
[2025-02-23T14:58:37.767+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db2.parquet' enviado para 'datalake/bronze/db2/produtos_db2.parquet'.
[2025-02-23T14:58:37.769+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.parquet' baixado para 'temp_landzone_produtos_db3.parquet'.
[2025-02-23T14:58:37.771+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db3.parquet' enviado para 'datalake/bronze/db3/produtos_db3.parquet'.
[2025-02-23T14:58:37.774+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.parquet' baixado para 'temp_landzone_vendas_db1.parquet'.
[2025-02-23T14:58:37.776+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db1.parquet' enviado para 'datalake/bronze/db1/vendas_db1.parquet'.
[2025-02-23T14:58:37.780+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.parquet' baixado para 'temp_landzone_vendas_db2.parquet'.
[2025-02-23T14:58:37.781+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db2.parquet' enviado para 'datalake/bronze/db2/vendas_db2.parquet'.
[2025-02-23T14:58:37.784+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.parquet' baixado para 'temp_landzone_vendas_db3.parquet'.
[2025-02-23T14:58:37.786+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db3.parquet' enviado para 'datalake/bronze/db3/vendas_db3.parquet'.
[2025-02-23T14:58:37.788+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.parquet' baixado para 'temp_landzone_vendedores_db1.parquet'.
[2025-02-23T14:58:37.790+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db1.parquet' enviado para 'datalake/bronze/db1/vendedores_db1.parquet'.
[2025-02-23T14:58:37.792+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.parquet' baixado para 'temp_landzone_vendedores_db2.parquet'.
[2025-02-23T14:58:37.794+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db2.parquet' enviado para 'datalake/bronze/db2/vendedores_db2.parquet'.
[2025-02-23T14:58:37.800+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.parquet' baixado para 'temp_landzone_vendedores_db3.parquet'.
[2025-02-23T14:58:37.808+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db3.parquet' enviado para 'datalake/bronze/db3/vendedores_db3.parquet'.
[2025-02-23T14:58:37.839+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T14:58:37.843+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T14:58:37.963+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T14:58:37.965+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=manual__2025-02-23T14:47:15.591803+00:00, execution_date=20250223T144715, start_date=20250223T145828, end_date=20250223T145837
[2025-02-23T14:58:38.065+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T14:58:38.148+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T14:58:38.154+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T14:58:40.721+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T14:58:40.789+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver manual__2025-02-23T14:47:15.591803+00:00 [queued]>
[2025-02-23T14:58:40.828+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver manual__2025-02-23T14:47:15.591803+00:00 [queued]>
[2025-02-23T14:58:40.829+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T14:58:40.895+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 14:47:15.591803+00:00
[2025-02-23T14:58:40.909+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=160) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T14:58:40.915+0000] {standard_task_runner.py:72} INFO - Started process 161 to run task
[2025-02-23T14:58:40.918+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'manual__2025-02-23T14:47:15.591803+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpkriiq842']
[2025-02-23T14:58:40.923+0000] {standard_task_runner.py:105} INFO - Job 6: Subtask process_silver
[2025-02-23T14:58:41.087+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver manual__2025-02-23T14:47:15.591803+00:00 [running]> on host b06401153325
[2025-02-23T14:58:41.398+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T14:47:15.591803+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-02-23T14:47:15.591803+00:00'
[2025-02-23T14:58:41.402+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T14:58:41.451+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T14:58:41.453+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T14:58:41.483+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T14:58:41.539+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T14:58:46.714+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T14:58:46.716+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T14:58:46.720+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T14:58:46.726+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T14:58:46.729+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T14:58:46.732+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T14:58:46.736+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T14:58:46.740+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T14:58:46.742+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T14:58:46.746+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T14:58:46.749+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T14:58:46.752+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T14:58:46.760+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T14:58:46.765+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T14:58:46.773+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T14:58:46.779+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T14:58:46.797+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T14:58:46.802+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T14:58:46.811+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T14:58:46.817+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T14:58:46.830+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T14:58:46.833+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T14:58:46.837+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T14:58:47.220+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T14:58:47.223+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T14:58:47.270+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T14:58:47.625+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=manual__2025-02-23T14:47:15.591803+00:00, execution_date=20250223T144715, start_date=20250223T145840, end_date=20250223T145847
[2025-02-23T14:58:47.716+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T14:58:47.718+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 6 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 161)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T14:58:47.780+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T14:58:47.880+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T14:58:47.886+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T14:59:04.412+0000] {processor.py:186} INFO - Started process (PID=129) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:59:04.415+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:59:04.417+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:59:04.417+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:59:05.049+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:59:05.156+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:59:05.156+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:59:05.236+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:59:05.235+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:59:05.276+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.876 seconds
[2025-02-23T14:59:35.629+0000] {processor.py:186} INFO - Started process (PID=131) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:59:35.635+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T14:59:35.643+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:59:35.640+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:59:36.484+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T14:59:37.297+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:59:37.296+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T14:59:37.394+0000] {logging_mixin.py:190} INFO - [2025-02-23T14:59:37.393+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 14:50:00+00:00, run_after=2025-02-23 15:00:00+00:00
[2025-02-23T14:59:37.507+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.910 seconds
[2025-02-23T15:00:05.622+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:00:05.727+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T14:50:00+00:00 [queued]>
[2025-02-23T15:00:05.787+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T14:50:00+00:00 [queued]>
[2025-02-23T15:00:05.790+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:00:05.880+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 14:50:00+00:00
[2025-02-23T15:00:05.913+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=173) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:00:05.922+0000] {standard_task_runner.py:72} INFO - Started process 174 to run task
[2025-02-23T15:00:05.923+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T14:50:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmphzt1ugbz']
[2025-02-23T15:00:05.931+0000] {standard_task_runner.py:105} INFO - Job 7: Subtask process_landzone
[2025-02-23T15:00:08.588+0000] {processor.py:186} INFO - Started process (PID=133) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:00:08.604+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:00:08.611+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:00:08.610+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:00:11.948+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:00:12.184+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:00:12.184+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:00:12.323+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:00:12.321+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:00:12.433+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.910 seconds
[2025-02-23T15:00:25.744+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:00:25.905+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T14:50:00+00:00 [queued]>
[2025-02-23T15:00:25.957+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T14:50:00+00:00 [queued]>
[2025-02-23T15:00:25.960+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:00:26.037+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 14:50:00+00:00
[2025-02-23T15:00:26.093+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=219) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:00:26.113+0000] {standard_task_runner.py:72} INFO - Started process 221 to run task
[2025-02-23T15:00:26.102+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T14:50:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpqdgb6buf']
[2025-02-23T15:00:26.126+0000] {standard_task_runner.py:105} INFO - Job 8: Subtask process_bronze
[2025-02-23T15:00:26.364+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T14:50:00+00:00 [running]> on host b06401153325
[2025-02-23T15:00:26.756+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T14:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T14:50:00+00:00'
[2025-02-23T15:00:26.760+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T15:00:26.866+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T15:00:26.872+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T15:00:26.919+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T15:00:26.942+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T15:00:27.039+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:00:27.643+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T14:40:00+00:00 [running]> on host b06401153325
[2025-02-23T15:00:28.187+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T14:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T14:40:00+00:00'
[2025-02-23T15:00:28.193+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T15:00:28.280+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T15:00:28.287+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T15:00:28.350+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T15:00:28.381+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T15:00:31.982+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.parquet' baixado para 'temp_landzone_fornecedores_db1.parquet'.
[2025-02-23T15:00:31.999+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db1.parquet' enviado para 'datalake/bronze/db1/fornecedores_db1.parquet'.
[2025-02-23T15:00:32.019+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.parquet' baixado para 'temp_landzone_fornecedores_db2.parquet'.
[2025-02-23T15:00:32.028+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db2.parquet' enviado para 'datalake/bronze/db2/fornecedores_db2.parquet'.
[2025-02-23T15:00:32.034+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.parquet' baixado para 'temp_landzone_fornecedores_db3.parquet'.
[2025-02-23T15:00:32.041+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db3.parquet' enviado para 'datalake/bronze/db3/fornecedores_db3.parquet'.
[2025-02-23T15:00:32.049+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.parquet' baixado para 'temp_landzone_produtos_db1.parquet'.
[2025-02-23T15:00:32.053+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db1.parquet' enviado para 'datalake/bronze/db1/produtos_db1.parquet'.
[2025-02-23T15:00:32.056+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.parquet' baixado para 'temp_landzone_produtos_db2.parquet'.
[2025-02-23T15:00:32.060+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db2.parquet' enviado para 'datalake/bronze/db2/produtos_db2.parquet'.
[2025-02-23T15:00:32.066+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.parquet' baixado para 'temp_landzone_produtos_db3.parquet'.
[2025-02-23T15:00:32.074+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db3.parquet' enviado para 'datalake/bronze/db3/produtos_db3.parquet'.
[2025-02-23T15:00:32.079+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.parquet' baixado para 'temp_landzone_vendas_db1.parquet'.
[2025-02-23T15:00:32.084+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db1.parquet' enviado para 'datalake/bronze/db1/vendas_db1.parquet'.
[2025-02-23T15:00:32.110+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.parquet' baixado para 'temp_landzone_vendas_db2.parquet'.
[2025-02-23T15:00:32.114+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db2.parquet' enviado para 'datalake/bronze/db2/vendas_db2.parquet'.
[2025-02-23T15:00:32.119+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.parquet' baixado para 'temp_landzone_vendas_db3.parquet'.
[2025-02-23T15:00:32.124+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db3.parquet' enviado para 'datalake/bronze/db3/vendas_db3.parquet'.
[2025-02-23T15:00:32.137+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.parquet' baixado para 'temp_landzone_vendedores_db1.parquet'.
[2025-02-23T15:00:32.146+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db1.parquet' enviado para 'datalake/bronze/db1/vendedores_db1.parquet'.
[2025-02-23T15:00:32.153+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.parquet' baixado para 'temp_landzone_vendedores_db2.parquet'.
[2025-02-23T15:00:32.156+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db2.parquet' enviado para 'datalake/bronze/db2/vendedores_db2.parquet'.
[2025-02-23T15:00:32.158+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.parquet' baixado para 'temp_landzone_vendedores_db3.parquet'.
[2025-02-23T15:00:32.165+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db3.parquet' enviado para 'datalake/bronze/db3/vendedores_db3.parquet'.
[2025-02-23T15:00:32.191+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T15:00:32.197+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T15:00:32.341+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:00:32.347+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T14:50:00+00:00, execution_date=20250223T145000, start_date=20250223T150025, end_date=20250223T150032
[2025-02-23T15:00:32.893+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T15:00:33.072+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:00:33.086+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[20[2025-02-23T18:45:57.101+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
0:37.065+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.071+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T15:00:37.077+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.084+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T15:00:37.087+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.090+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T15:00:37.100+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.103+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T15:00:37.114+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.117+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T15:00:37.119+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.123+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T15:00:37.126+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.129+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T15:00:37.138+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.155+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T15:00:37.165+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.170+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T15:00:37.173+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.177+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T15:00:37.180+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.184+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T15:00:37.186+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T15:00:37.596+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T15:00:37.601+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T15:00:37.712+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:00:37.751+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:00:37.754+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T14:40:00+00:00, execution_date=20250223T144000, start_date=20250223T150027, end_date=20250223T150037
[2025-02-23T15:00:38.086+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T14:50:00+00:00 [queued]>
[2025-02-23T15:00:38.138+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T14:50:00+00:00 [queued]>
[2025-02-23T15:00:38.140+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:00:38.207+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T15:00:38.286+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 14:50:00+00:00
[2025-02-23T15:00:38.312+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=235) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:00:38.319+0000] {standard_task_runner.py:72} INFO - Started process 272 to run task
[2025-02-23T15:00:38.332+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T14:50:00+00:00', '--job-id', '10', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpytwsczek']
[2025-02-23T15:00:38.342+0000] {standard_task_runner.py:105} INFO - Job 10: Subtask process_silver
[2025-02-23T15:00:38.356+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:00:38.392+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:00:43.220+0000] {processor.py:186} INFO - Started process (PID=135) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:00:43.224+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:00:43.231+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:00:43.229+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:00:44.504+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:00:44.707+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:00:44.706+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:00:44.849+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:00:44.848+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:00:44.960+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.768 seconds
T15:00:43.128+0000] {standard_task_runner.py:72} INFO - Started process 282 to run task
[2025-02-23T15:00:43.132+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T14:40:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp7693s2hs']
[2025-02-23T15:00:43.137+0000] {standard_task_runner.py:105} INFO - Job 11: Subtask process_bronze
[2025-02-23T15:00:43.397+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T14:40:00+00:00 [running]> on host b06401153325
[2025-02-23T15:00:44.097+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T14:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T14:40:00+00:00'
[2025-02-23T15:00:44.104+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T15:00:44.192+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T15:00:44.201+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T15:00:44.265+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T15:00:44.307+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T15:00:45.067+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T15:00:45.072+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T15:00:45.076+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T15:00:45.078+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T15:00:45.085+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T15:00:45.089+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T15:00:45.092+0000] {[2025-02-23T18:45:57.244+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:45:57.238+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00
n read_parquet
[2025-02-23T15:00:45.098+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T15:00:45.100+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T15:00:45.103+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T15:00:45.105+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T15:00:45.107+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:00:45.110+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T15:00:45.115+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T15:00:45.117+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T15:00:45.120+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T15:00:45.122+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T15:00:45.124+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:00:45.128+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T15:00:45.130+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T15:00:45.132+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T15:00:45.136+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T15:00:45.410+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T15:00:45.413+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T15:00:45.453+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:00:45.481+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T14:50:00+00:00, execution_date=20250223T145000, start_date=20250223T150038, end_date=20250223T150045
[2025-02-23T15:00:45.621+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:00:45.623+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 10 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 272)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:00:45.667+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T15:00:45.968+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:00:45.988+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:00:48.765+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.parquet' baixado para 'temp_landzone_fornecedores_db1.parquet'.
[2025-02-23T15:00:48.784+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db1.parquet' enviado para 'datalake/bronze/db1/fornecedores_db1.parquet'.
[2025-02-23T15:00:48.799+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.parquet' baixado para 'temp_landzone_fornecedores_db2.parquet'.
[2025-02-23T15:00:48.803+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_d[2025-02-23T18:45:58.976+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1088) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
- Arquivo 'temp_landzone_fornecedores_db3.parquet' enviado para 'datalake/bronze/db3/fornecedores_db3.parquet'.
[2025-02-23T15:00:48.843+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.parquet' baixado para 'temp_landzone_produtos_db1.parquet'.
[2025-02-23T15:00:48.847+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db1.parquet' enviado para 'datalake/bronze/db1/produtos_db1.parquet'.
[2025-02-23T15:00:48.850+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.parquet' baixado para 'temp_landzone_produtos_db2.parquet'.
[2025-02-23T15:00:48.855+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db2.parquet' enviado para 'datalake/bronze/db2/produtos_db2.parquet'.
[2025-02-23T15:00:48.859+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.parquet' baixado para 'temp_landzone_produtos_db3.parquet'.
[2025-02-23T15:00:48.863+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db3.parquet' enviado para 'datalake/bronze/db3/produtos_db3.parquet'.
[2025-02-23T15:00:48.866+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.parquet' baixado para 'temp_landzone_vendas_db1.parquet'.
[2025-02-23T15:00:48.869+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db1.parquet' enviado para 'datalake/bronze/db1/vendas_db1.parquet'.
[2025-02-23T15:00:48.874+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.parquet' baixado para 'temp_landzone_vendas_db2.parquet'.
[2025-02-23T15:00:48.877+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db2.parquet' enviado para 'datalake/bronze/db2/vendas_db2.parquet'.
[2025-02-23T15:00:48.881+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.parquet' baixado para 'temp_landzone_vendas_db3.parquet'.
[2025-02-23T15:00:48.888+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db3.parquet' enviado para 'datalake/bronze/db3/vendas_db3.parquet'.
[2025-02-23T15:00:48.892+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.parquet' baixado para 'temp_landzone_vendedores_db1.parquet'.
[2025-02-23T15:00:48.896+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db1.parquet' enviado para 'datalake/bronze/db1/vendedores_db1.parquet'.
[2025-02-23T15:00:48.898+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.parquet' baixado para 'temp_landzone_vendedores_db2.parquet'.
[2025-02-23T15:00:48.902+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db2.parquet' enviado para 'datalake/bronze/db2/vendedores_db2.parquet'.
[2025-02-23T15:00:48.906+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.parquet' baixado para 'temp_landzone_vendedores_db3.parquet'.
[2025-02-23T15:00:48.909+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db3.parquet' enviado para 'datalake/bronze/db3/vendedores_db3.parquet'.
[2025-02-23T15:00:48.957+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T15:00:48.961+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T15:00:49.087+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:00:49.089+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T14:40:00+00:00, execution_date=20250223T144000, start_date=20250223T150042, end_date=20250223T150049
[2025-02-23T15:00:49.208+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T15:00:49.337+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:00:49.370+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:00:52.602+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:00:52.660+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T14:40:00+00:00 [queued]>
[2025-02-23T15:00:52.685+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T14:40:00+00:00 [queued]>
[2025-02-23T15:00:52.686+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:00:52.722+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 14:40:00+00:00
[2025-02-23T15:00:52.737+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=290) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:00:52.743+0000] {standard_task_runner.py:72} INFO - Started process 291 to run task
[2025-02-23T15:00:52.743+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T14:40:00+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpr9t3_hq6']
[2025-02-23T15:00:52.746+0000] {standard_task_runner.py:105} INFO - Job 12: Subtask process_silver
[2025-02-23T15:00:52.950+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T14:40:00+00:00 [running]> on host b06401153325
[2025-02-23T15:00:53.371+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T14:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T14:40:00+00:00'
[2025-02-23T15:00:53.379+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T15:00:53.473+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T15:00:53.476+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T15:00:53.505+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T15:00:53.522+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T15:00:57.559+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T15:00:57.566+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T15:00:57.569+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T15:00:57.573+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T15:00:57.579+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T15:00:57.581+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T15:00:57.584+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:00:57.586+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T15:00:57.589+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T15:00:57.595+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T15:00:57.611+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T15:00:57.631+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T15:00:57.637+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:00:57.653+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T15:00:57.655+0000] {subprocess.py:106} INFO -     datase[2025-02-23T18:45:59.219+0000] {standard_task_runner.py:72} INFO - Started process 1089 to run task
.py:3239} INFO - Sync 1 DAGs
_v1.py
[2025-02-23T18:45:19.200+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v1.py for tasks to queue
__
[2025-02-23T15:00:57.663+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T15:00:57.666+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:00:57.670+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T15:00:57.673+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T15:00:57.676+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T15:00:57.679+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T15:00:58.233+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T15:00:58.237+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T15:00:58.303+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:00:58.344+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T14:40:00+00:00, execution_date=20250223T144000, start_date=20250223T150052, end_date=20250223T150058
[2025-02-23T15:00:58.458+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:00:58.460+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 12 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 291)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:00:58.506+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T15:00:58.581+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:00:58.584+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:01:15.650+0000] {processor.py:186} INFO - Started process (PID=137) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:01:15.653+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:01:15.658+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:01:15.657+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:01:16.088+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:01:16.183+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:01:16.183+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:01:16.285+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:01:16.284+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:01:16.589+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.959 seconds
[2025-02-23T15:01:47.092+0000] {processor.py:186} INFO - Started process (PID=139) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:01:47.095+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:01:47.100+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:01:47.099+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:01:47.886+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:01:47.959+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:01:47.958+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:01:48.013+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:01:48.012+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:01:48.052+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.981 seconds
[2025-02-23T15:02:18.461+0000] {processor.py:186} INFO - Started process (PID=141) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:02:18.463+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:02:18.466+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:02:18.465+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:02:18.855+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:02:19.217+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:02:19.216+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:02:19.271+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:02:19.270+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:02:19.344+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.896 seconds
[2025-02-23T15:02:49.721+0000] {processor.py:186} INFO - Started process (PID=143) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:02:49.723+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:02:49.726+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:02:49.725+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:02:50.490+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:02:50.542+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:02:50.542+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:02:50.598+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:02:50.597+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:02:50.636+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.930 seconds
[2025-02-23T15:03:20.703+0000] {processor.py:186} INFO - Started process (PID=145) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:03:20.705+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:03:20.708+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:03:20.708+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:03:21.151+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:03:21.211+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:03:21.210+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:03:21.261+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:03:21.260+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:03:21.336+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.647 seconds
[2025-02-23T15:03:52.075+0000] {processor.py:186} INFO - Started process (PID=147) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:03:52.077+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:03:52.080+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:03:52.080+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:03:52.467+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:03:52.520+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:03:52.520+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:03:52.570+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:03:52.570+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:03:52.868+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.809 seconds
[2025-02-23T15:04:23.046+0000] {processor.py:186} INFO - Started process (PID=149) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:04:23.048+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:04:23.052+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:04:23.051+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:04:23.778+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:04:23.840+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:04:23.840+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:04:23.891+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:04:23.890+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:04:23.940+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.912 seconds
[2025-02-23T15:04:54.334+0000] {processor.py:186} INFO - Started process (PID=151) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:04:54.336+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:04:54.339+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:04:54.338+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:04:54.695+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:04:54.967+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:04:54.966+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:04:55.059+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:04:55.058+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:04:55.103+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.784 seconds
[2025-02-23T15:05:25.527+0000] {processor.py:186} INFO - Started process (PID=153) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:05:25.534+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:05:25.541+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:05:25.539+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:05:26.742+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:05:26.852+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:05:26.851+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:05:26.951+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:05:26.950+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:05:27.031+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.536 seconds
[2025-02-23T15:05:57.121+0000] {processor.py:186} INFO - Started process (PID=155) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:05:57.125+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:05:57.130+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:05:57.129+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/a[2025-02-23T18:45:59.209+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T18:30:00+00:00', '--job-id', '52', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpu6g6rcpe']
 DAGs
[2025-02-23T15:05:58.371+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:05:58.370+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:05:58.453+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.349 seconds
[2025-02-23T15:06:28.669+0000] {processor.py:186} INFO - Started process (PID=157) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:06:28.671+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:06:28.674+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:06:28.673+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:06:29.357+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:06:29.416+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:06:29.415+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:06:29.469+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:06:29.468+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:06:29.762+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.114 seconds
[2025-02-23T15:07:00.212+0000] {processor.py:186} INFO - Started process (PID=159) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:07:00.214+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:07:00.217+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:07:00.216+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:07:00.954+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:07:01.013+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:07:01.012+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:07:01.069+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:07:01.068+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:07:01.148+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.951 seconds
[2025-02-23T15:07:31.742+0000] {processor.py:186} INFO - Started process (PID=161) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:07:31.745+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:07:31.750+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:07:31.749+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:07:32.611+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:07:32.670+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:07:32.670+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:07:33.043+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:07:33.042+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:07:33.091+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.376 seconds
[2025-02-23T15:08:03.232+0000] {processor.py:186} INFO - Started process (PID=164) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:08:03.236+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:08:03.247+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:08:03.245+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:08:04.412+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:08:05.241+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:08:05.240+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:08:05.357+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:08:05.356+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:08:05.468+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.268 seconds
[2025-02-23T15:08:35.788+0000] {processor.py:186} INFO - Started process (PID=166) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:08:35.791+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:08:35.798+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:08:35.797+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:08:36.528+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:08:36.579+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:08:36.578+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:08:36.626+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:08:36.626+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:08:36.685+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.920 seconds
[2025-02-23T15:08:49.431+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:08:49.530+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver manual__2025-02-23T14:47:15.591803+00:00 [queued]>
[2025-02-23T15:08:49.562+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver manual__2025-02-23T14:47:15.591803+00:00 [queued]>
[2025-02-23T15:08:49.563+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T15:08:49.648+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 14:47:15.591803+00:00
[2025-02-23T15:08:49.660+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=303) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:08:49.664+0000] {standard_task_runner.py:72} INFO - Started process 304 to run task
[2025-02-23T15:08:49.665+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'manual__2025-02-23T14:47:15.591803+00:00', '--job-id', '13', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpu94d3h44']
[2025-02-23T15:08:49.668+0000] {standard_task_runner.py:105} INFO - Job 13: Subtask process_silver
[2025-02-23T15:08:49.784+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver manual__2025-02-23T14:47:15.591803+00:00 [running]> on host b06401153325
[2025-02-23T15:08:50.050+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T14:47:15.591803+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-02-23T14:47:15.591803+00:00'
[2025-02-23T15:08:50.052+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T15:08:50.091+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T15:08:50.095+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T15:08:50.119+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T15:08:50.133+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T15:08:53.423+0000] {subprocess.py:106} I[2025-02-23T18:45:19.213+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:45:19.207+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v1.py
ecent call last):
"[2025-02-23T15:08:53.426+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T15:08:53.427+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T15:08:53.428+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T15:08:53.430+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T15:08:53.432+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:08:53.433+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T15:08:53.435+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T15:08:53.436+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T15:08:53.438+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T15:08:53.440+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T15:08:53.442+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:08:53.444+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T15:08:53.445+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T15:08:53.447+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T15:08:53.449+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T15:08:53.450+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T15:08:53.452+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:08:53.453+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T15:08:53.455+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T15:08:53.456+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T15:08:53.458+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T15:08:53.621+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T15:08:53.624+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T15:08:53.673+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:08:53.695+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_silver, run_id=manual__2025-02-23T14:47:15.591803+00:00, execution_date=20250223T144715, start_date=20250223T150849, end_date=20250223T150853
[2025-02-23T15:08:53.894+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:08:53.896+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 13 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 304)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:08:53.946+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T15:08:54.040+0000] {taskinstan[2025-02-23T18:45:59.405+0000] {standard_task_runner.py:105} INFO - Job 52: Subtask process_silver
097+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:09:07.475+0000] {processor.py:186} INFO - Started process (PID=168) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:09:07.479+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:09:07.484+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:09:07.483+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:09:08.243+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:09:08.306+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:09:08.306+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:09:08.359+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:09:08.359+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:09:08.444+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.000 seconds
[2025-02-23T15:09:39.082+0000] {processor.py:186} INFO - Started process (PID=170) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:09:39.091+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:09:39.112+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:09:39.111+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:09:40.134+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:09:40.246+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:09:40.245+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:09:40.319+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:09:40.318+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:00:00+00:00, run_after=2025-02-23 15:10:00+00:00
[2025-02-23T15:09:40.361+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.310 seconds
[2025-02-23T15:10:03.664+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:10:03.746+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T15:00:00+00:00 [queued]>
[2025-02-23T15:10:03.780+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T15:00:00+00:00 [queued]>
[2025-02-23T15:10:03.783+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:10:03.849+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 15:00:00+00:00
[2025-02-23T15:10:03.872+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=316) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:10:03.879+0000] {standard_task_runner.py:72} INFO - Started process 317 to run task
[2025-02-23T15:10:03.880+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T15:00:00+00:00', '--job-id', '14', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpi01f1bji']
[2025-02-23T15:10:03.885+0000] {standard_task_runner.py:105} INFO - Job 14: Subtask process_landzone
[2025-02-23T15:10:04.095+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T15:00:00+00:00 [running]> on host b06401153325
[2025-02-23T15:10:04.390+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T15:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T15:00:00+00:00'
[2025-02-23T15:10:04.392+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T15:10:04.424+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T15:10:04.427+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T15:10:04.445+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T15:10:04.455+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T15:10:10.928+0000] {processor.py:186} INFO - Started process (PID=172) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:10:10.935+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:10:10.946+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:10:10.940+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:10:12.066+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:10:12.232+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:10:12.231+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:10:12.973+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:10:12.972+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:10:13.063+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.170 seconds
[2025-02-23T15:10:20.123+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:10:20.390+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T15:00:00+00:00 [queued]>
[2025-02-23T15:10:20.441+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T15:00:00+00:00 [queued]>
[2025-02-23T15:10:20.443+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:10:20.511+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 15:00:00+00:00
[2025-02-23T15:10:20.529+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=362) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:10:20.534+0000] {standard_task_runner.py:72} INFO - Started process 363 to run task
[2025-02-23T15:10:20.536+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T15:00:00+00:00', '--job-id', '15', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp2y9l8hwy']
[2025-02-23T15:10:20.542+0000] {standard_task_runner.py:105} INFO - Job 15: Subtask process_bronze
[2025-02-23T15:10:25.335+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.parquet' baixado para 'temp_landzone_fornecedores_db1.parquet'.
[2025-02-23T15:10:25.339+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db1.parquet' enviado para 'datalake/bronze/db1/fornecedores_db1.parquet'.
[2025-02-23T15:10:25.342+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.parquet' baixado para 'temp_landzone_fornecedores_db2.parquet'.
[2025-02-23T15:10:25.346+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db2.parquet' enviado para 'datalake/bronze/db2/fornecedores_db2.parquet'.
[2025-02-23T15:10:25.349+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.parquet' baixado para 'temp_landzone_fornecedores_db3.parquet'.
[2025-02-23T15:10:25.352+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db3.parquet' enviado para 'datalake/bronze/db3/fornecedores_db3.parquet'.
[2025-02-23T15:10:25.355+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.parquet' baixado para 'temp_landzone_produtos_db1.parquet'.
[2025-02-23T15:10:25.358+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db1.parquet' enviado para 'datalake/bronze/db1/produtos_db1.parquet'.
[2025-02-23T15:10:25.361+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.parquet' baixado para 'temp_landzone_produtos_db2.parquet'.
[2025-02-23T15:10:25.364+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db2.parquet' enviado para 'datalake/bronze/db2/produtos_db2.parquet'.
[2025-02-23T15:10:25.367+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.parquet' baixado para 'temp_landzone_produtos_db3.parquet'.
[2025-02-23T15:10:25.369+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db3.parquet' enviado para 'datalake/bronze/db3/produtos_db3.parquet'.
[2025-02-23T15:10:25.372+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.parquet' baixado para 'temp_landzone_vendas_db1.parquet'.
[2025-02-23T15:10:25.375+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db1.parquet' enviado para 'datalake/bronze/db1/vendas_db1.parquet'.
[2025-02-23T15:10:25.378+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.parquet' baixado para 'temp_landzone_vendas_db2.parquet'.
[2025-02-23T15:10:25.380+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db2.parquet' enviado para 'datalake/bronze/db2/vendas_db2.parquet'.
[2025-02-23T15:10:25.382+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.parquet' baixado para 'temp_landzone_vendas_db3.parquet'.
[2025-02-23T15:10:25.385+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db3.parquet' enviado para 'datalake/bronze/db3/vendas_db3.parquet'.
[2025-02-23T15:10:25.388+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.parquet' baixado para 'temp_landzone_vendedores_db1.parquet'.
[2025-02-23T15:10:25.390+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db1.parquet' enviado para 'datalake/bronze/db1/vendedores_db1.parquet'.
[2025-02-23T15:10:25.393+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.parquet' baixado para 'temp_landzone_vendedores_db2.parquet'.
[2025-02-23T15:10:25.396+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db2.parquet' enviado para 'datalake/bronze/db2/vendedores_db2.parquet'.
[2025-02-23T15:10:25.398+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.parquet' baixado para 'temp_landzone_vendedores_db3.parquet'.
[2025-02-23T15:10:25.401+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db3.parquet' enviado para 'datalake/bronze/db3/vendedores_db3.parquet'.
[2025-02-23T15:10:25.448+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T15:10:25.450+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T15:10:25.570+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:10:25.578+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T15:00:00+00:00, execution_date=20250223T150000, start_date=20250223T151020, end_date=20250223T151025
[2025-02-23T15:10:25.901+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T15:10:26.082+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:10:26.121+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:10:30.008+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:10:30.086+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:00:00+00:00 [queued]>
[2025-02-23T15:10:30.119+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:00:00+00:00 [queued]>
[2025-02-23T15:10:30.120+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:10:30.161+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 15:00:00+00:00
[2025-02-23T15:10:30.176+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=368) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:10:30.192+0000] {standard_task_runner.py:72} INFO - Started process 369 to run task
[2025-02-23T15:10:30.187+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T15:00:00+00:00', '--job-id', '16', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpfr51si21']
[2025-02-23T15:10:30.205+0000] {standard_task_runner.py:105} INFO - Job 16: Subtask process_silver
[2025-02-23T15:10:30.418+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:00:00+00:00 [running]> on host b06401153325
[2025-02-23T15:10:30.928+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T15:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T15:00:00+00:00'
[2025-02-23T15:10:30.932+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T15:10:31.027+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T15:10:31.032+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T15:10:31.081+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T15:10:31.097+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T15:10:43.678+0000] {processor.py:186} INFO - Started process (PID=174) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:10:43.680+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:10:43.683+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:10:43.682+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:10:44.202+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:10:44.551+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:10:44.550+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:10:44.623+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:10:44.623+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:10:44.728+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.064 seconds
[2025-02-23T15:10:51.552+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:10:51.705+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T14:50:00+00:00 [queued]>
[2025-02-23T15:10:51.789+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T14:50:00+00:00 [queued]>
[2025-02-23T15:10:51.807+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T15:10:52.130+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 14:50:00+00:00
[2025-02-23T15:10:52.252+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=381) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:10:52.268+0000] {standard_task_runner.py:72} INFO - Started process 382 to run task
[2025-02-23T15:10:52.264+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T14:50:00+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp7u6ofw6p']
[2025-02-23T15:10:52.279+0000] {standard_task_runner.py:105} INFO - Job 17: Subtask process_silver
[2025-02-23T15:11:02.546+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T15:11:02.554+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T15:11:02.557+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T15:11:02.563+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T15:11:02.571+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T15:11:02.577+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T15:11:02.598+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:11:02.602+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T15:11:02.620+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T15:11:02.625+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T15:11:02.630+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T15:11:02.633+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T15:11:02.637+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:11:02.642+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T15:11:02.645+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T15:11:02.648+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T15:11:02.650+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T15:11:02.653+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T15:11:02.655+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:11:02.659+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T15:11:02.661+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T15:11:02.664+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T15:11:02.667+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T15:11:03.160+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T15:11:03.164+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T15:11:03.326+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:11:03.348+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T14:50:00+00:00, execution_date=20250223T145000, start_date=20250223T151051, end_date=20250223T151103
[2025-02-23T15:11:03.566+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:11:03.571+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 17 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 382)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:11:03.640+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T15:11:03.720+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:11:03.732+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:11:04.933+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:11:05.000+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T14:40:00+00:00 [queued]>
[2025-02-23T15:11:05.028+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T14:40:00+00:00 [queued]>
[2025-02-23T15:11:05.030+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T15:11:05.086+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 14:40:00+00:00
[2025-02-23T15:11:05.108+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=391) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:11:05.115+0000] {standard_task_runner.py:72} INFO - Started process 395 to run task
[2025-02-23T15:11:05.120+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T14:40:00+00:00', '--job-id', '18', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmplv7ygzg1']
[2025-02-23T15:11:05.124+0000] {standard_task_runner.py:105} INFO - Job 18: Subtask process_silver
[2025-02-23T15:11:05.305+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T14:40:00+00:00 [running]> on host b06401153325
[2025-02-23T15:11:05.725+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T14:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T14:40:00+00:00'
[2025-02-23T15:11:05.735+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T15:11:05.858+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T15:11:05.865+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T15:11:05.925+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T15:11:05.954+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T15:11:12.016+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T15:11:12.018+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T15:11:12.020+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T15:11:12.022+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T15:11:12.024+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T15:11:12.026+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T15:11:12.029+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:11:12.032+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T15:11:12.035+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T15:11:12.038+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T15:11:12.040+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T15:11:12.043+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T15:11:12.045+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:11:12.047+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T15:11:12.051+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T15:11:12.053+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T15:11:12.056+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T15:11:12.058+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T15:11:12.061+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:11:12.064+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T15:11:12.066+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T15:11:12.068+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T15:11:12.072+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T15:11:12.303+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T15:11:12.307+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T15:11:12.376+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:11:12.403+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T14:40:00+00:00, execution_date=20250223T144000, start_date=20250223T151105, end_date=20250223T151112
[2025-02-23T15:11:12.687+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:11:12.696+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 18 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 395)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:11:12.753+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T15:11:12.837+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:11:12.865+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:11:14.925+0000] {processor.py:186} INFO - Started process (PID=176) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:11:14.929+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:11:14.934+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:11:14.933+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:11:16.062+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:11:16.136+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:11:16.135+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:11:16.214+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:11:16.214+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:11:16.272+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.391 seconds
[2025-02-23T15:11:46.670+0000] {processor.py:186} INFO - Started process (PID=178) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:11:46.672+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:11:46.675+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:11:46.674+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:11:47.318+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:11:47.377+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:11:47.376+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:11:47.446+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:11:47.445+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:11:47.548+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.891 seconds
[2025-02-23T15:12:17.952+0000] {processor.py:186} INFO - Started process (PID=180) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:12:17.955+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:12:17.958+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:12:17.957+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:12:18.555+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:12:18.603+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:12:18.603+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:12:18.665+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:12:18.665+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:12:18.759+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.823 seconds
[2025-02-23T15:12:49.554+0000] {processor.py:186} INFO - Started process (PID=182) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:12:49.556+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:12:49.559+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:12:49.558+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:12:50.207+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:12:50.262+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:12:50.262+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:12:50.309+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:12:50.308+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:12:50.357+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.819 seconds
[2025-02-23T15:13:21.182+0000] {processor.py:186} INFO - Started process (PID=184) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:13:21.185+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:13:21.188+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:13:21.187+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:13:21.598+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:13:21.894+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:13:21.893+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:13:21.968+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:13:21.968+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:13:22.010+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.840 seconds
[2025-02-23T15:13:52.760+0000] {processor.py:186} INFO - Started process (PID=186) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:13:52.762+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:13:52.765+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:13:52.764+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:13:53.393+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:13:53.449+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:13:53.448+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:13:53.493+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:13:53.492+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:13:53.534+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.787 seconds
[2025-02-23T15:14:24.402+0000] {processor.py:186} INFO - Started process (PID=188) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:14:24.404+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:14:24.407+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:14:24.406+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:14:24.990+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:14:25.038+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:14:25.037+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:14:25.086+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:14:25.085+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:14:25.176+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.788 seconds
[2025-02-23T15:14:55.942+0000] {processor.py:186} INFO - Started process (PID=190) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:14:55.944+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:14:55.947+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:14:55.946+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:14:56.620+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:14:56.664+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:14:56.664+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:14:56.710+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:14:56.709+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:14:56.751+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.822 seconds
[2025-02-23T15:15:27.145+0000] {processor.py:186} INFO - Started process (PID=192) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:15:27.147+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:15:27.150+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:15:27.149+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:15:27.725+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:15:27.777+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:15:27.777+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:15:27.818+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:15:27.818+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:15:27.862+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.729 seconds
[2025-02-23T15:15:58.688+0000] {processor.py:186} INFO - Started process (PID=194) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:15:58.690+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:15:58.694+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:15:58.693+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:15:59.096+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:15:59.370+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:15:59.370+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:15:59.413+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:15:59.412+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:15:59.458+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.784 seconds
[2025-02-23T15:16:29.790+0000] {processor.py:186} INFO - Started process (PID=196) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:16:29.792+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:16:29.796+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:16:29.795+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:16:30.662+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:16:30.757+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:16:30.756+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:16:30.857+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:16:30.856+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:16:30.914+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.139 seconds
[2025-02-23T15:17:01.334+0000] {processor.py:186} INFO - Started process (PID=198) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:17:01.336+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:17:01.339+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:17:01.339+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:17:01.911+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:17:01.957+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:17:01.956+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:17:01.999+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:17:01.998+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:17:02.039+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.718 seconds
[2025-02-23T15:17:32.813+0000] {processor.py:186} INFO - Started process (PID=200) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:17:32.816+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:17:32.820+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:17:32.819+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:17:33.611+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:17:33.656+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:17:33.655+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:17:33.698+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:17:33.698+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:17:33.737+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.941 seconds
[2025-02-23T15:18:04.425+0000] {processor.py:186} INFO - Started process (PID=202) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:18:04.427+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:18:04.432+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:18:04.431+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:18:05.345+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:18:05.444+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:18:05.443+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:18:05.512+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:18:05.511+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:18:05.568+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.161 seconds
[2025-02-23T15:18:36.166+0000] {processor.py:186} INFO - Started process (PID=204) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:18:36.168+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:18:36.171+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:18:36.170+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:18:36.718+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:18:37.091+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:18:37.090+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:18:37.186+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:18:37.186+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:18:37.241+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.090 seconds
[2025-02-23T15:19:08.857+0000] {processor.py:186} INFO - Started process (PID=206) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:19:08.868+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:19:08.962+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:19:08.955+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:19:14.336+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:19:14.783+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:19:14.781+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:19:15.003+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:19:15.002+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:19:15.356+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 7.141 seconds
[2025-02-23T15:19:46.285+0000] {processor.py:186} INFO - Started process (PID=208) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:19:46.291+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:19:46.301+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:19:46.299+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:19:48.864+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:19:48.982+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:19:48.981+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:19:49.097+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:19:49.093+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:10:00+00:00, run_after=2025-02-23 15:20:00+00:00
[2025-02-23T15:19:49.221+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.974 seconds
[2025-02-23T15:20:07.152+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:20:07.541+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T15:10:00+00:00 [queued]>
[2025-02-23T15:20:07.686+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T15:10:00+00:00 [queued]>
[2025-02-23T15:20:07.689+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:20:07.848+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 15:10:00+00:00
[2025-02-23T15:20:07.884+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=417) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:20:07.892+0000] {standard_task_runner.py:72} INFO - Started process 418 to run task
[2025-02-23T15:20:07.893+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T15:10:00+00:00', '--job-id', '19', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp7ddm41g1']
[2025-02-23T15:20:07.903+0000] {standard_task_runner.py:105} INFO - Job 19: Subtask process_landzone
[2025-02-23T15:20:20.142+0000] {processor.py:186} INFO - Started process (PID=210) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:20:20.162+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:20:20.197+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:20:20.195+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:20:24.505+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:20:24.655+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:20:24.654+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:20:24.820+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:20:24.819+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:20:24.931+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.830 seconds
[2025-02-23T15:20:32.686+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:20:33.400+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T15:10:00+00:00 [queued]>
[2025-02-23T15:20:33.584+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T15:10:00+00:00 [queued]>
[2025-02-23T15:20:33.599+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:20:33.826+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 15:10:00+00:00
[2025-02-23T15:20:33.867+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=463) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:20:33.878+0000] {standard_task_runner.py:72} INFO - Started process 464 to run task
[2025-02-23T15:20:33.883+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T15:10:00+00:00', '--job-id', '20', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpr0c4mqzz']
[2025-02-23T15:20:33.888+0000] {standard_task_runner.py:105} INFO - Job 20: Subtask process_bronze
[2025-02-23T15:20:44.894+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.parquet' baixado para 'temp_landzone_fornecedores_db1.parquet'.
[2025-02-23T15:20:44.898+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db1.parquet' enviado para 'datalake/bronze/db1/fornecedores_db1.parquet'.
[2025-02-23T15:20:44.902+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.parquet' baixado para 'temp_landzone_fornecedores_db2.parquet'.
[2025-02-23T15:20:44.932+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db2.parquet' enviado para 'datalake/bronze/db2/fornecedores_db2.parquet'.
[2025-02-23T15:20:44.935+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.parquet' baixado para 'temp_landzone_fornecedores_db3.parquet'.
[2025-02-23T15:20:45.056+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db3.parquet' enviado para 'datalake/bronze/db3/fornecedores_db3.parquet'.
[2025-02-23T15:20:45.062+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.parquet' baixado para 'temp_landzone_produtos_db1.parquet'.
[2025-02-23T15:20:45.091+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db1.parquet' enviado para 'datalake/bronze/db1/produtos_db1.parquet'.
[2025-02-23T15:20:45.098+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.parquet' baixado para 'temp_landzone_produtos_db2.parquet'.
[2025-02-23T15:20:45.102+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db2.parquet' enviado para 'datalake/bronze/db2/produtos_db2.parquet'.
[2025-02-23T15:20:45.107+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.parquet' baixado para 'temp_landzone_produtos_db3.parquet'.
[2025-02-23T15:20:45.110+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db3.parquet' enviado para 'datalake/bronze/db3/produtos_db3.parquet'.
[2025-02-23T15:20:45.117+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.parquet' baixado para 'temp_landzone_vendas_db1.parquet'.
[2025-02-23T15:20:45.120+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db1.parquet' enviado para 'datalake/bronze/db1/vendas_db1.parquet'.
[2025-02-23T15:20:45.126+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.parquet' baixado para 'temp_landzone_vendas_db2.parquet'.
[2025-02-23T15:20:45.128+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db2.parquet' enviado para 'datalake/bronze/db2/vendas_db2.parquet'.
[2025-02-23T15:20:45.135+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.parquet' baixado para 'temp_landzone_vendas_db3.parquet'.
[2025-02-23T15:20:45.140+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db3.parquet' enviado para 'datalake/bronze/db3/vendas_db3.parquet'.
[2025-02-23T15:20:45.145+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.parquet' baixado para 'temp_landzone_vendedores_db1.parquet'.
[2025-02-23T15:20:45.152+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db1.parquet' enviado para 'datalake/bronze/db1/vendedores_db1.parquet'.
[2025-02-23T15:20:45.158+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.parquet' baixado para 'temp_landzone_vendedores_db2.parquet'.
[2025-02-23T15:20:45.162+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db2.parquet' enviado para 'datalake/bronze/db2/vendedores_db2.parquet'.
[2025-02-23T15:20:45.169+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.parquet' baixado para 'temp_landzone_vendedores_db3.parquet'.
[2025-02-23T15:20:45.175+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db3.parquet' enviado para 'datalake/bronze/db3/vendedores_db3.parquet'.
[2025-02-23T15:20:45.358+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T15:20:45.364+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T15:20:45.798+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:20:45.803+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T15:10:00+00:00, execution_date=20250223T151000, start_date=20250223T152033, end_date=20250223T152045
[2025-02-23T15:20:46.000+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:20:46.122+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T15:20:46.246+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:00:00+00:00 [queued]>
[2025-02-23T15:20:46.398+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:20:46.413+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:00:00+00:00 [queued]>
[2025-02-23T15:20:46.441+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:20:46.419+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T15:20:46.748+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 15:00:00+00:00
[2025-02-23T15:20:46.792+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=469) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:20:46.801+0000] {standard_task_runner.py:72} INFO - Started process 470 to run task
[2025-02-23T15:20:46.802+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T15:00:00+00:00', '--job-id', '21', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpape08oup']
[2025-02-23T15:20:46.814+0000] {standard_task_runner.py:105} INFO - Job 21: Subtask process_silver
[2025-02-23T15:20:55.946+0000] {processor.py:186} INFO - Started process (PID=213) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:20:56.017+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:20:56.025+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:20:56.023+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:21:01.264+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:21:01.584+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:21:01.583+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:21:02.528+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:21:02.526+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:21:02.810+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 6.917 seconds
[2025-02-23T15:21:09.602+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T15:21:09.608+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T15:21:09.612+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T15:21:09.627+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T15:21:09.631+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T15:21:09.634+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T15:21:09.644+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:21:09.649+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T15:21:09.657+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T15:21:09.660+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T15:21:09.663+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T15:21:09.676+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T15:21:09.679+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:21:09.683+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T15:21:09.686+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T15:21:09.691+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T15:21:09.696+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T15:21:09.708+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T15:21:09.718+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:21:09.725+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T15:21:09.730+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T15:21:09.734+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T15:21:09.749+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T15:21:11.251+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T15:21:11.257+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T15:21:11.358+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:21:11.396+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T15:00:00+00:00, execution_date=20250223T150000, start_date=20250223T152046, end_date=20250223T152111
[2025-02-23T15:21:11.600+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:21:11.608+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 21 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 470)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:21:11.730+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T15:21:11.891+0000] {standard_task_runner.py:217} INFO - Process not found (most likely exited), stop collecting metrics
[2025-02-23T15:21:12.222+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:21:12.273+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:21:31.274+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T15:21:31.284+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T15:21:31.291+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T15:21:31.296+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T15:21:31.299+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T15:21:31.305+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T15:21:31.309+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:21:31.314+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T15:21:31.324+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T15:21:31.328+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T15:21:31.341+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T15:21:31.346+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T15:21:31.350+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:21:31.359+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T15:21:31.364+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T15:21:31.375+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T15:21:31.382+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T15:21:31.395+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T15:21:31.404+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:21:31.410+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T15:21:31.431+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T15:21:31.435+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T15:21:31.444+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T15:21:34.465+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T15:21:34.472+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T15:21:35.414+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:21:35.704+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T15:10:00+00:00, execution_date=20250223T151000, start_date=20250223T152101, end_date=20250223T152135
[2025-02-23T15:21:37.428+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:21:37.671+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 22 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 480)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:21:38.227+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T15:21:38.471+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:21:42.091+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:21:45.296+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:21:45.295+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:21:45.559+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:21:45.557+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:21:45.697+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 12.134 seconds
[2025-02-23T15:22:16.572+0000] {processor.py:186} INFO - Started process (PID=217) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:22:16.582+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:22:16.597+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:22:16.595+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:22:20.760+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:22:20.959+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:22:20.956+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:22:21.167+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:22:21.166+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:22:21.400+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.124 seconds
[2025-02-23T15:22:51.833+0000] {processor.py:186} INFO - Started process (PID=219) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:22:51.849+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:22:51.875+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:22:51.868+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:22:59.563+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:22:59.941+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:22:59.939+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:23:01.223+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:23:01.222+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:23:01.866+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 10.118 seconds
[2025-02-23T15:23:32.860+0000] {processor.py:186} INFO - Started process (PID=221) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:23:32.867+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:23:32.880+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:23:32.878+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:23:37.297+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:23:37.642+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:23:37.642+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:23:37.875+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:23:37.874+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:23:38.161+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.351 seconds
[2025-02-23T15:24:09.553+0000] {processor.py:186} INFO - Started process (PID=223) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:24:09.575+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:24:09.585+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:24:09.584+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:24:13.768+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:24:14.544+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:24:14.543+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:24:14.953+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:24:14.952+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:24:15.090+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.886 seconds
[2025-02-23T15:24:45.943+0000] {processor.py:186} INFO - Started process (PID=225) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:24:45.947+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:24:45.956+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:24:45.954+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:24:48.983+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:24:49.199+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:24:49.197+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:24:49.510+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:24:49.508+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:24:49.859+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.965 seconds
[2025-02-23T15:25:20.458+0000] {processor.py:186} INFO - Started process (PID=227) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:25:20.468+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:25:20.477+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:25:20.475+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:25:23.448+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:25:23.675+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:25:23.673+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:25:23.804+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:25:23.803+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:25:23.920+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.501 seconds
[2025-02-23T15:25:55.059+0000] {processor.py:186} INFO - Started process (PID=229) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:25:55.066+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:25:55.073+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:25:55.071+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:25:57.976+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:25:58.131+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:25:58.130+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:25:58.287+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:25:58.286+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:25:58.402+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.384 seconds
[2025-02-23T15:26:28.625+0000] {processor.py:186} INFO - Started process (PID=232) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:26:28.628+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:26:28.638+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:26:28.636+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:26:31.303+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:26:31.446+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:26:31.445+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:26:31.591+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:26:31.591+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:26:31.707+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.120 seconds
[2025-02-23T15:27:02.113+0000] {processor.py:186} INFO - Started process (PID=234) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:27:02.120+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:27:02.133+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:27:02.131+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:27:04.382+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:27:04.854+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:27:04.853+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:27:05.423+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:27:05.422+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:27:05.577+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.554 seconds
[2025-02-23T15:27:35.981+0000] {processor.py:186} INFO - Started process (PID=236) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:27:36.022+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:27:36.031+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:27:36.030+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:27:38.098+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:27:38.488+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:27:38.487+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:27:39.029+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:27:39.022+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:27:39.373+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.449 seconds
[2025-02-23T15:28:10.580+0000] {processor.py:186} INFO - Started process (PID=238) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:28:10.616+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:28:10.632+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:28:10.630+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:28:13.835+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:28:14.254+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:28:14.253+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:28:14.470+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:28:14.468+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:28:14.601+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.132 seconds
[2025-02-23T15:28:44.911+0000] {processor.py:186} INFO - Started process (PID=240) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:28:44.948+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:28:44.958+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:28:44.957+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:28:47.862+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:28:48.050+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:28:48.048+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:28:48.329+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:28:48.328+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:28:48.469+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.606 seconds
[2025-02-23T15:29:19.882+0000] {processor.py:186} INFO - Started process (PID=242) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:29:20.162+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:29:20.211+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:29:20.195+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:29:26.769+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:29:27.073+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:29:27.072+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:29:27.305+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:29:27.303+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:20:00+00:00, run_after=2025-02-23 15:30:00+00:00
[2025-02-23T15:29:27.439+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 7.729 seconds
[2025-02-23T15:29:57.774+0000] {processor.py:186} INFO - Started process (PID=244) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:29:57.814+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:29:57.824+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:29:57.822+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:30:01.141+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:30:01.469+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:30:01.466+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:30:02.000+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:30:01.999+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:30:00+00:00, run_after=2025-02-23 15:40:00+00:00
[2025-02-23T15:30:02.715+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.996 seconds
[2025-02-23T15:30:09.386+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 1.769 seconds
[2025-02-23T15:30:09.855+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:30:10.182+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T15:20:00+00:00 [queued]>
[2025-02-23T15:30:10.440+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T15:20:00+00:00 [queued]>
[2025-02-23T15:30:10.447+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:30:10.707+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 15:20:00+00:00
[2025-02-23T15:30:10.753+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=495) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:30:10.764+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T15:20:00+00:00', '--job-id', '23', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpjmx0uakp']
[2025-02-23T15:30:10.769+0000] {standard_task_runner.py:72} INFO - Started process 496 to run task
[2025-02-23T15:30:10.773+0000] {standard_task_runner.py:105} INFO - Job 23: Subtask process_landzone
[2025-02-23T15:30:33.321+0000] {processor.py:186} INFO - Started process (PID=246) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:30:33.333+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:30:33.342+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:30:33.340+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:30:37.699+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:30:37.909+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:30:37.908+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:30:38.149+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:30:38.148+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:30:00+00:00, run_after=2025-02-23 15:40:00+00:00
[2025-02-23T15:30:38.374+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.102 seconds
[2025-02-23T15:30:40.669+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:30:40.668+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00
[2025-02-23T15:30:40.871+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 0.991 seconds
[2025-02-23T15:30:41.048+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:30:41.298+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T15:20:00+00:00 [queued]>
[2025-02-23T15:30:41.403+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T15:20:00+00:00 [queued]>
[2025-02-23T15:30:41.413+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:30:41.555+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 15:20:00+00:00
[2025-02-23T15:30:41.589+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=541) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:30:41.603+0000] {standard_task_runner.py:72} INFO - Started process 542 to run task
[2025-02-23T15:30:41.605+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T15:20:00+00:00', '--job-id', '24', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmprf9s6wc4']
[2025-02-23T15:30:41.614+0000] {standard_task_runner.py:105} INFO - Job 24: Subtask process_bronze
[2025-02-23T15:30:52.636+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.parquet' baixado para 'temp_landzone_fornecedores_db1.parquet'.
[2025-02-23T15:30:52.643+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db1.parquet' enviado para 'datalake/bronze/db1/fornecedores_db1.parquet'.
[2025-02-23T15:30:52.673+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.parquet' baixado para 'temp_landzone_fornecedores_db2.parquet'.
[2025-02-23T15:30:52.684+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db2.parquet' enviado para 'datalake/bronze/db2/fornecedores_db2.parquet'.
[2025-02-23T15:30:52.688+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.parquet' baixado para 'temp_landzone_fornecedores_db3.parquet'.
[2025-02-23T15:30:52.694+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db3.parquet' enviado para 'datalake/bronze/db3/fornecedores_db3.parquet'.
[2025-02-23T15:30:52.700+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.parquet' baixado para 'temp_landzone_produtos_db1.parquet'.
[2025-02-23T15:30:52.704+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db1.parquet' enviado para 'datalake/bronze/db1/produtos_db1.parquet'.
[2025-02-23T15:30:52.707+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.parquet' baixado para 'temp_landzone_produtos_db2.parquet'.
[2025-02-23T15:30:52.714+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db2.parquet' enviado para 'datalake/bronze/db2/produtos_db2.parquet'.
[2025-02-23T15:30:52.719+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.parquet' baixado para 'temp_landzone_produtos_db3.parquet'.
[2025-02-23T15:30:52.723+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db3.parquet' enviado para 'datalake/bronze/db3/produtos_db3.parquet'.
[2025-02-23T15:30:52.730+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.parquet' baixado para 'temp_landzone_vendas_db1.parquet'.
[2025-02-23T15:30:52.735+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db1.parquet' enviado para 'datalake/bronze/db1/vendas_db1.parquet'.
[2025-02-23T15:30:52.738+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.parquet' baixado para 'temp_landzone_vendas_db2.parquet'.
[2025-02-23T15:30:52.746+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db2.parquet' enviado para 'datalake/bronze/db2/vendas_db2.parquet'.
[2025-02-23T15:30:52.750+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.parquet' baixado para 'temp_landzone_vendas_db3.parquet'.
[2025-02-23T15:30:52.772+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db3.parquet' enviado para 'datalake/bronze/db3/vendas_db3.parquet'.
[2025-02-23T15:30:52.802+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.parquet' baixado para 'temp_landzone_vendedores_db1.parquet'.
[2025-02-23T15:30:52.807+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db1.parquet' enviado para 'datalake/bronze/db1/vendedores_db1.parquet'.
[2025-02-23T15:30:52.814+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.parquet' baixado para 'temp_landzone_vendedores_db2.parquet'.
[2025-02-23T15:30:52.817+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db2.parquet' enviado para 'datalake/bronze/db2/vendedores_db2.parquet'.
[2025-02-23T15:30:52.824+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.parquet' baixado para 'temp_landzone_vendedores_db3.parquet'.
[2025-02-23T15:30:52.832+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db3.parquet' enviado para 'datalake/bronze/db3/vendedores_db3.parquet'.
[2025-02-23T15:30:52.922+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T15:30:52.929+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T15:30:53.109+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:30:53.113+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T15:20:00+00:00, execution_date=20250223T152000, start_date=20250223T153041, end_date=20250223T153053
[2025-02-23T15:30:54.241+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T15:30:54.503+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:30:54.522+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:31:02.998+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:31:03.220+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:20:00+00:00 [queued]>
[2025-02-23T15:31:03.294+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:20:00+00:00 [queued]>
[2025-02-23T15:31:03.298+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T15:31:03.388+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 15:20:00+00:00
[2025-02-23T15:31:03.429+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=547) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T15:31:03.444+0000] {standard_task_runner.py:72} INFO - Started process 548 to run task
[2025-02-23T15:31:03.449+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T15:20:00+00:00', '--job-id', '25', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpnwpumb85']
[2025-02-23T15:31:03.455+0000] {standard_task_runner.py:105} INFO - Job 25: Subtask process_silver
[2025-02-23T15:31:09.463+0000] {processor.py:186} INFO - Started process (PID=248) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:31:09.471+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:31:09.496+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:31:09.484+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:31:13.608+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:31:13.817+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:31:13.816+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:31:14.051+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:31:14.042+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:30:00+00:00, run_after=2025-02-23 15:40:00+00:00
[2025-02-23T15:31:14.303+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.918 seconds
[2025-02-23T15:31:21.054+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T15:31:21.060+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T15:31:21.070+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T15:31:21.075+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T15:31:21.080+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T15:31:21.084+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T15:31:21.087+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:31:21.095+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T15:31:21.101+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T15:31:21.105+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T15:31:21.112+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T15:31:21.118+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T15:31:21.122+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:31:21.127+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T15:31:21.131+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T15:31:21.134+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T15:31:21.138+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T15:31:21.142+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T15:31:21.156+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:31:21.167+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T15:31:21.170+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T15:31:21.174+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T15:31:21.183+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T15:31:22.272+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T15:31:22.280+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T15:31:22.373+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:31:22.446+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T15:20:00+00:00, execution_date=20250223T152000, start_date=20250223T153103, end_date=20250223T153122
[2025-02-23T15:31:22.757+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:31:22.763+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 25 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 548)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages[2025-02-23T18:45:57.972+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 4.667 seconds"
   ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:31:22.944+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T15:31:23.175+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:31:23.191+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:31:43.684+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T15:31:43.841+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:10:00+00:00 [queued]>
[2025-02-23T15:31:43.918+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:10:00+00:00 [queued]>
[2025-02-23T15:31:43.921+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T15:31:44.066+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 15:10:00+00:00
[2025-02-23T15:31:45.645+0000] {processor.py:186} INFO - Started process (PID=251) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:31:45.664+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:31:45.683+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:31:45.681+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:31:49.935+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:31:50.173+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:31:50.172+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:31:50.836+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:31:50.835+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:30:00+00:00, run_after=2025-02-23 15:40:00+00:00
[2025-02-23T15:31:50.998+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.478 seconds
[2025-02-23T15:32:00.771+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T15:32:00.791+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T15:32:00.841+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T15:32:00.854+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T15:32:00.863+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T15:32:00.869+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T15:32:00.874+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:32:00.882+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T15:32:00.887+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T15:32:00.895+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T15:32:00.907+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T15:32:00.940+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T15:32:00.951+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:32:00.962+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T15:32:00.967+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T15:32:00.972+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T15:32:00.980+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T15:32:00.987+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T15:32:00.991+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T15:32:00.998+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T15:32:01.002+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T15:32:01.012+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T15:32:01.017+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T15:32:01.877+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T15:32:01.881+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T15:32:01.973+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:32:02.000+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T15:10:00+00:00, execution_date=20250223T151000, start_date=20250223T153143, end_date=20250223T153201
[2025-02-23T15:32:02.173+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T15:32:02.183+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 26 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 561)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T15:32:02.269+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T15:32:02.454+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T15:32:02.497+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T15:32:21.348+0000] {processor.py:186} INFO - Started process (PID=253) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:32:21.355+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:32:21.365+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:32:21.363+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:32:23.619+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:32:24.082+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:32:24.081+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:32:24.814+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:32:24.812+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:30:00+00:00, run_after=2025-02-23 15:40:00+00:00
[2025-02-23T15:32:25.040+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.749 seconds
[2025-02-23T15:32:55.502+0000] {processor.py:186} INFO - Started process (PID=255) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:32:55.507+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:32:55.519+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:32:55.518+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:32:57.750+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:32:59.275+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:32:59.271+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:32:59.524+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:32:59.522+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:30:00+00:00, run_after=2025-02-23 15:40:00+00:00
[2025-02-23T15:32:59.673+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.259 seconds
[2025-02-23T15:33:30.085+0000] {processor.py:186} INFO - Started process (PID=257) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:33:30.094+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:33:30.102+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:33:30.100+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:33:32.593+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:33:32.861+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:33:32.860+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:33:33.141+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:33:33.140+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:30:00+00:00, run_after=2025-02-23 15:40:00+00:00
[2025-02-23T15:33:33.662+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.662 seconds
[2025-02-23T15:34:03.819+0000] {processor.py:186} INFO - Started process (PID=259) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:34:03.822+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:34:03.824+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:34:03.824+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:34:04.854+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:34:04.939+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:34:04.938+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:34:05.022+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:34:05.021+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:30:00+00:00, run_after=2025-02-23 15:40:00+00:00
[2025-02-23T15:34:05.086+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.279 seconds
[2025-02-23T15:36:00.017+0000] {processor.py:186} INFO - Started process (PID=260) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:36:00.022+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T15:36:00.028+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:36:00.027+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:36:03.477+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T15:36:03.588+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:36:03.588+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T15:36:03.720+0000] {logging_mixin.py:190} INFO - [2025-02-23T15:36:03.719+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 15:30:00+00:00, run_after=2025-02-23 15:40:00+00:00
[2025-02-23T15:36:03.800+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.087 seconds
[2025-02-23T17:56:19.486+0000] {processor.py:186} INFO - Started process (PID=266) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:56:19.502+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T17:56:19.511+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:56:19.510+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:56:21.771+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:56:21.877+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:56:21.877+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T17:56:21.973+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:56:21.972+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 17:50:00+00:00, run_after=2025-02-23 18:00:00+00:00
[2025-02-23T17:56:22.082+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.645 seconds
[2025-02-23T17:56:23.090+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T17:56:23.176+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T17:56:23.293+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T15:30:00+00:00 [queued]>
[2025-02-23T17:56:23.306+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:20:00+00:00 [queued]>
[2025-02-23T17:56:23.337+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T15:30:00+00:00 [queued]>
[2025-02-23T17:56:23.355+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T17:56:23.357+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:20:00+00:00 [queued]>
[2025-02-23T17:56:23.361+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T17:56:23.419+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 15:20:00+00:00
[2025-02-23T17:56:23.420+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 15:30:00+00:00
[2025-02-23T17:56:23.439+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=573) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T17:56:23.454+0000] {standard_task_runner.py:72} INFO - Started process 576 to run task
[2025-02-23T17:56:23.449+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T15:20:00+00:00', '--job-id', '27', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp1u8ywz59']
[2025-02-23T17:56:23.466+0000] {standard_task_runner.py:105} INFO - Job 27: Subtask process_silver
[2025-02-23T17:56:23.457+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=574) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T17:56:23.468+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T15:30:00+00:00', '--job-id', '28', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpnwqx4sd9']
[2025-02-23T17:56:23.480+0000] {standard_task_runner.py:105} INFO - Job 28: Subtask process_landzone
[2025-02-23T17:56:23.476+0000] {standard_task_runner.py:72} INFO - Started process 577 to run task
[2025-02-23T17:56:23.686+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T15:30:00+00:00 [running]> on host b06401153325
[2025-02-23T17:56:24.850+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T17:56:24.918+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T17:40:00+00:00 [queued]>
[2025-02-23T17:56:24.962+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T17:40:00+00:00 [queued]>
[2025-02-23T17:56:24.967+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T17:56:25.057+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 17:40:00+00:00
[2025-02-23T17:56:25.080+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=575) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T17:56:25.086+0000] {standard_task_runner.py:72} INFO - Started process 592 to run task
[2025-02-23T17:56:25.086+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T17:40:00+00:00', '--job-id', '29', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp8pukm8y7']
[2025-02-23T17:56:25.093+0000] {standard_task_runner.py:105} INFO - Job 29: Subtask process_landzone
[2025-02-23T17:56:25.272+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T17:40:00+00:00 [running]> on host b06401153325
[2025-02-23T17:56:25.652+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T17:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T17:40:00+00:00'
[2025-02-23T17:56:25.657+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T17:56:25.750+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T17:56:25.753+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T17:56:25.797+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T17:56:25.817+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T17:56:31.024+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T17:56:31.050+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T17:56:31.056+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T17:56:31.062+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T17:56:31.064+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T17:56:31.139+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T17:56:31.144+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T17:56:31.146+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T17:56:31.149+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T17:56:31.151+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T17:56:31.156+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T17:56:31.159+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T17:56:31.164+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T17:56:31.167+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T17:56:31.172+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T17:56:31.175+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T17:56:31.239+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T17:56:31.264+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T17:56:31.269+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T17:56:31.292+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T17:56:31.296+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T17:56:31.298+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T17:56:31.303+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T17:56:31.574+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T17:56:31.577+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T17:56:31.626+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T17:56:31.650+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T15:20:00+00:00, execution_date=20250223T152000, start_date=20250223T175623, end_date=20250223T175631
[2025-02-23T17:56:31.812+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T17:56:31.823+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 27 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 576)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T17:56:31.874+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T17:56:31.980+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T17:56:31.996+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T17:56:32.233+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T17:56:32.235+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.237+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T17:56:32.240+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.243+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T17:56:32.246+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.248+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T17:56:32.251+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.253+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T17:56:32.255+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.259+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T17:56:32.267+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.271+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T17:56:32.278+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.280+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T17:56:32.282+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.284+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T17:56:32.289+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.290+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T17:56:32.293+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.297+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T17:56:32.301+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.304+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T17:56:32.306+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:32.468+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T17:56:32.469+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T17:56:32.548+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T17:56:32.551+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T15:30:00+00:00, execution_date=20250223T153000, start_date=20250223T175623, end_date=20250223T175632
[2025-02-23T17:56:32.795+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T17:56:32.915+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T17:56:32.921+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T17:56:33.759+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T17:56:33.762+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:33.764+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T17:56:33.766+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:33.768+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T17:56:33.770+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:33.772+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T17:56:33.774+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:33.776+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T17:56:33.778+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:33.780+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T17:56:33.782+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:33.786+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T17:56:33.788+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:33.791+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T17:56:33.793+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:33.795+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T17:56:33.797+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:33.800+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T17:56:33.805+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:33.807+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T17:56:33.809+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:33.811+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T17:56:33.814+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T17:56:34.181+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T17:56:34.184+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T17:56:34.285+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T17:56:34.287+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T17:40:00+00:00, execution_date=20250223T174000, start_date=20250223T175624, end_date=20250223T175634
[2025-02-23T17:56:34.431+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T17:56:34.527+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T17:56:34.536+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T17:56:37.298+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T15:30:00+00:00 [running]> on host b06401153325
[2025-02-23T17:56:37.509+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T15:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T15:30:00+00:00'
[2025-02-23T17:56:37.512+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T17:56:37.549+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T17:56:37.553+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T17:56:37.580+0000] {subprocess.py:99} INFO - O[2025-02-23T18:45:58.058+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [queued]>
6:39.546+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T17:40:00+00:00 [queued]>
[2025-02-23T17:56:39.579+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T17:40:00+00:00 [queued]>
[2025-02-23T17:56:39.581+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T17:56:39.642+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 17:40:00+00:00
[2025-02-23T17:56:39.663+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=679) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T17:56:39.669+0000] {standard_task_runner.py:72} INFO - Started process 685 to run task
[2025-02-23T17:56:39.673+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T17:40:00+00:00', '--job-id', '31', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpmkch1z5m']
[2025-02-23T17:56:39.677+0000] {standard_task_runner.py:105} INFO - Job 31: Subtask process_bronze
[2025-02-23T17:56:39.891+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T17:40:00+00:00 [running]> on host b06401153325
[2025-02-23T17:56:40.467+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T17:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T17:40:00+00:00'
[2025-02-23T17:56:40.470+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T17:56:40.523+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T17:56:40.534+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T17:56:40.563+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T17:56:40.580+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T17:56:41.378+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.parquet' baixado para 'temp_landzone_fornecedores_db1.parquet'.
[2025-02-23T17:56:41.382+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db1.parquet' enviado para 'datalake/bronze/db1/fornecedores_db1.parquet'.
[2025-02-23T17:56:41.388+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.parquet' baixado para 'temp_landzone_fornecedores_db2.parquet'.
[2025-02-23T17:56:41.390+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db2.parquet' enviado para 'datalake/bronze/db2/fornecedores_db2.parquet'.
[2025-02-23T17:56:41.402+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.parquet' baixado para 'temp_landzone_fornecedores_db3.parquet'.
[2025-02-23T17:56:41.405+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db3.parquet' enviado para 'datalake/bronze/db3/fornecedores_db3.parquet'.
[2025-02-23T17:56:41.407+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.parquet' baixado para 'temp_landzone_produtos_db1.parquet'.
[2025-02-23T17:56:41.411+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db1.parquet' enviado para 'datalake/bronze/db1/produtos_db1.parquet'.
[2025-02-23T17:56:41.415+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.parquet' baixado para 'temp_landzone_produtos_db2.parquet'.
[2025-02-23T17:56:41.417+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db2.parquet' enviado para 'datalake/bronze/db2/produtos_db2.parquet'.
[2025-02-23T17:56:41.422+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.parquet' baixado para 'temp_landzone_produtos_db3.parquet'.
[2025-02-23T17:56:41.431+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db3.parquet' enviado para 'datalake/bronze/db3/produtos_db3.parquet'.
[2025-02-23T17:56:41.436+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.parquet' baixado para 'temp_landzone_vendas_db1.parquet'.
[2025-02-23T17:56:41.438+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db1.parquet' enviado para 'datalake/bronze/db1/vendas_db1.parquet'.
[2025-02-23T17:56:41.460+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.parquet' baixado para 'temp_landzone_vendas_db2.parquet'.
[2025-02-23T17:56:41.484+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db2.parquet' enviado para 'datalake/bronze/db2/vendas_db2.parquet'.
[2025-02-23T17:56:41.487+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.parquet' baixado para 'temp_landzone_vendas_db3.parquet'.
[2025-02-23T17:56:41.490+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db3.parquet' enviado para 'datalake/bronze/db3/vendas_db3.parquet'.
[2025-02-23T17:56:41.495+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.parquet' baixado para 'temp_landzone_vendedores_db1.parquet'.
[2025-02-23T17:56:41.503+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db1.parquet' enviado para 'datalake/bronze/db1/vendedores_db1.parquet'.
[2025-02-23T17:56:41.506+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.parquet' baixado para 'temp_landzone_vendedores_db2.parquet'.
[2025-02-23T17:56:41.511+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db2.parquet' enviado para 'datalake/bronze/db2/vendedores_db2.parquet'.
[2025-02-23T17:56:41.513+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.parquet' baixado para 'temp_landzone_vendedores_db3.parquet'.
[2025-02-23T17:56:41.517+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db3.parquet' enviado para 'datalake/bronze/db3/vendedores_db3.parquet'.
[2025-02-23T17:56:41.534+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T17:56:41.537+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T17:56:41.699+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T17:56:41.701+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T15:30:00+00:00, execution_date=20250223T153000, start_date=20250223T175636, end_date=20250223T175641
[2025-02-23T17:56:41.808+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T17:56:41.885+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T17:56:41.888+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T17:56:44.279+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.parquet' baixado para 'temp_landzone_fornecedores_db1.parquet'.
[2025-02-23T17:56:44.283+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db1.parquet' enviado para 'datalake/bronze/db1/fornecedores_db1.parquet'.
[2025-02-23T17:56:44.295+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.parquet' baixado para 'temp_landzone_fornecedores_db2.parquet'.
[2025-02-23T17:56:44.300+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db2.parquet' enviado para 'datalake/bronze/db2/fornecedores_db2.parquet'.
[2025-02-23T17:56:44.302+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.parquet' baixado para 'temp_landzone_fornecedores_db3.parquet'.
[2025-02-23T17:56:44.305+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db3.parquet' enviado para 'datalake/bronze/db3/fornecedores_db3.parquet'.
[2025-02-23T17:56:44.308+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.parquet' baixado para 'temp_landzone_produtos_db1.parquet'.
[2025-02-23T17:56:44.311+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db1.parquet' enviado para 'datalake/bronze/db1/produtos_db1.parquet'.
[2025-02-23T17:56:44.313+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.parquet' baixado para 'temp_landzone_produtos_db2.parquet'.
[2025-02-23T17:56:44.317+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db2.parquet' enviado para 'datalake/bronze/db2/produtos_db2.parquet'.
[2025-02-23T17:56:44.319+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.parquet' baixado para 'temp_landzone_produtos_db3.parquet'.
[2025-02-23T17:56:44.322+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db3.parquet' enviado para 'datalake/bronze/db3/produtos_db3.parquet'.
[2025-02-23T17:56:44.325+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.parquet' baixado para 'temp_landzone_vendas_db1.parquet'.
[2025-02-23T17:56:44.328+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db1.parquet' enviado para 'datalake/bronze/db1/vendas_db1.parquet'.
[2025-02-23T17:56:44.331+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.parquet' baixado para 'temp_landzone_vendas_db2.parquet'.
[2025-02-23T17:56:44.333+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db2.parquet' enviado para 'datalake/bronze/db2/vendas_db2.parquet'.
[2025-02-23T17:56:44.336+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.parquet' baixado para 'temp_landzone_vendas_db3.parquet'.
[2025-02-23T17:56:44.342+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db3.parquet' enviado para 'datalake/bronze/db3/vendas_db3.parquet'.
[2025-02-23T17:56:44.344+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.parquet' baixado para 'temp_landzone_vendedores_db1.parquet'.
[2025-02-23T17:56:44.346+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db1.parquet' enviado para 'datalake/bronze/db1/vendedores_db1.parquet'.
[2025-02-23T17:56:44.348+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.parquet' baixado para 'temp_landzone_vendedores_db2.parquet'.
[2025-02-23T17:56:44.351+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db2.parquet' enviado para 'datalake/bronze/db2/vendedores_db2.parquet'.
[2025-02-23T17:56:44.355+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.parquet' baixado para 'temp_landzone_vendedores_db3.parquet'.
[2025-02-23T17:56:44.359+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db3.parquet' enviado para 'datalake/bronze/db3/vendedores_db3.parquet'.
[2025-02-23T17:56:44.399+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T17:56:44.403+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T17:56:44.518+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T17:56:44.521+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T17:40:00+00:00, execution_date=20250223T174000, start_date=20250223T175639, end_date=20250223T175644
[2025-02-23T17:56:44.691+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T17:56:44.697+0000] {standard_task_runner.py:217} INFO - Process not found (most likely exited), stop collecting metrics
[2025-02-23T17:56:44.849+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T17:56:44.855+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T17:56:45.385+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T17:56:45.440+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:30:00+00:00 [queued]>
[2025-02-23T17:56:45.464+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:30:00+00:00 [queued]>
[2025-02-23T17:56:45.465+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T17:56:45.497+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 15:30:00+00:00
[2025-02-23T17:56:45.513+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=690) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T17:56:45.517+0000] {standard_task_runner.py:72} INFO - Started process 691 to run task
[2025-02-23T17:56:45.519+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T15:30:00+00:00', '--job-id', '32', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmptkhkvjex']
[2025-02-23T17:56:45.523+0000] {standard_task_runner.py:105} INFO - Job 32: Subtask process_silver
[2025-02-23T17:56:45.737+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:30:00+00:00 [running]> on host b06401153325
[2025-02-23T17:56:46.063+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T15:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T15:30:00+00:00'
[2025-02-23T17:56:46.065+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T17:56:46.117+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T17:56:46.120+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T17:56:46.146+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T17:56:46.159+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T17:56:48.337+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T17:56:48.437+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T17:40:00+00:00 [queued]>
[2025-02-23T17:56:48.473+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T17:40:00+00:00 [queued]>
[2025-02-23T17:56:48.475+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T17:56:48.515+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 17:40:00+00:00
[2025-02-23T17:56:48.535+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=693) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T17:56:48.541+0000] {standard_task_runner.py:72} INFO - Started process 701 to run task
[2025-02-23T17:56:48.542+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T17:40:00+00:00', '--job-id', '33', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpqfuq1g1k']
[2025-02-23T17:56:48.547+0000] {standard_task_runner.py:105} INFO - Job 33: Subtask process_silver
[2025-02-23T17:56:48.681+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T17:40:00+00:00 [running]> on host b06401153325
[2025-02-23T17:56:49.080+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T17:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T17:40:00+00:00'
[2025-02-23T17:56:49.086+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T17:56:49.163+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T17:56:49.16[2025-02-23T18:46:01.453+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325
ubprocess.py:99} INFO - Output:
[2025-02-23T17:56:49.231+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T17:56:51.873+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T17:56:51.875+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T17:56:51.877+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T17:56:51.879+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T17:56:51.881+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T17:56:51.885+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T17:56:51.888+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T17:56:51.890+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T17:56:51.893+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T17:56:51.896+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T17:56:51.898+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T17:56:51.900+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T17:56:51.903+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T17:56:51.905+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T17:56:51.911+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T17:56:51.917+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T17:56:51.920+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T17:56:51.924+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T17:56:51.926+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T17:56:51.929+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T17:56:51.931+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T17:56:51.934+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T17:56:51.937+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T17:56:52.196+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T17:56:52.199+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T17:56:52.246+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T17:56:52.278+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T15:30:00+00:00, execution_date=20250223T153000, start_date=20250223T175645, end_date=20250223T175652
[2025-02-23T17:56:52.378+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T17:56:52.381+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 32 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 691)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T17:56:52.431+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T17:56:52.519+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T17:56:52.530+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T17:56:56.122+0000] {processor.py:186} INFO - Started process (PID=269) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:56:56.145+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T17:56:56.243+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:56:56.241+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:57:00.643+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:57:01.003+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:57:01.003+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T17:57:01.453+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T17:57:01.456+0000] {subprocess.py:106} INFO - Traceb[2025-02-23T17:57:01.471+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.645 seconds
"ake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T17:57:01.467+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T17:57:01.471+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T17:57:01.478+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T17:57:01.488+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T17:57:01.491+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T17:57:01.495+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T17:57:01.499+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T17:57:01.505+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T17:57:01.515+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T17:57:01.518+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T17:57:01.533+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T17:57:01.537+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T17:57:01.544+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T17:57:01.549+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T17:57:01.552+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T17:57:01.555+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T17:57:01.558+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T17:57:01.562+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T17:57:01.576+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T17:57:01.581+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T17:57:01.890+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T17:57:01.893+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T17:57:01.960+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T17:57:02.008+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T17:40:00+00:00, execution_date=20250223T174000, start_date=20250223T175648, end_date=20250223T175702
[2025-02-23T17:57:02.204+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T17:57:02.213+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 33 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 701)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T17:57:02.293+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T17:57:02.423+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T17:57:02.429+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T17:57:31.801+0000] {processor.py:186} INFO - Started process (PID=271) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:57:31.808+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T17:57:31.821+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:57:31.817+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:57:35.530+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:57:35.692+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:57:35.691+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T17:57:35.829+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:57:35.828+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 17:50:00+00:00, run_after=2025-02-23 18:00:00+00:00
[2025-02-23T17:57:35.950+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.201 seconds
[2025-02-23T17:58:06.342+0000] {processor.py:186} INFO - Started process (PID=273) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:58:06.365+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T17:58:06.377+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:58:06.375+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:58:09.114+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:58:09.356+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:58:09.354+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T17:58:09.491+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:58:09.490+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 17:50:00+00:00, run_after=2025-02-23 18:00:00+00:00
[2025-02-23T17:58:09.583+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.336 seconds
[2025-02-23T17:58:39.733+0000] {processor.py:186} INFO - Started process (PID=275) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:58:39.775+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T17:58:39.784+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:58:39.780+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:58:42.094+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:58:42.232+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:58:42.231+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T17:58:42.377+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:58:42.376+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 17:50:00+00:00, run_after=2025-02-23 18:00:00+00:00
[2025-02-23T17:58:42.469+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.776 seconds
[2025-02-23T17:59:13.591+0000] {processor.py:186} INFO - Started process (PID=277) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:59:13.706+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T17:59:13.730+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:59:13.729+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:59:16.329+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:59:16.496+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:59:16.495+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T17:59:16.645+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:59:16.645+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 17:50:00+00:00, run_after=2025-02-23 18:00:00+00:00
[2025-02-23T17:59:16.814+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.257 seconds
[2025-02-23T17:59:47.459+0000] {processor.py:186} INFO - Started process (PID=279) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:59:47.469+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T17:59:47.483+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:59:47.482+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:59:49.580+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T17:59:49.714+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:59:49.711+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T17:59:49.850+0000] {logging_mixin.py:190} INFO - [2025-02-23T17:59:49.849+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 17:50:00+00:00, run_after=2025-02-23 18:00:00+00:00
[2025-02-23T17:59:49.944+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.556 seconds
[2025-02-23T18:00:07.531+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:00:07.600+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T17:50:00+00:00 [queued]>
[2025-02-23T18:00:07.643+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T17:50:00+00:00 [queued]>
[2025-02-23T18:00:07.647+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:00:07.722+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 17:50:00+00:00
[2025-02-23T18:00:07.751+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=716) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:00:07.761+0000] {standard_task_runner.py:72} INFO - Started process 717 to run task
[2025-02-23T18:00:07.761+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T17:50:00+00:00', '--job-id', '34', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpp0z5oym4']
[2025-02-23T18:00:07.775+0000] {standard_task_runner.py:105} INFO - Job 34: Subtask process_landzone
[2025-02-23T18:00:08.091+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T17:50:00+00:00 [running]> on host b06401153325
[2025-02-23T18:00:09.309+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T17:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T17:50:00+00:00'
[2025-02-23T18:00:09.317+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T18:00:09.409+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:00:09.416+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T18:00:09.471+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:00:09.501+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T18:00:21.704+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T18:00:21.752+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:21.776+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T18:00:21.792+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:21.799+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T18:00:21.804+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:21.823+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T18:00:21.830+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:21.841+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T18:00:21.844+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:21.849+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T18:00:21.855+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:21.861+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T18:00:21.872+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:21.874+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T18:00:21.876+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:21.940+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T18:00:21.943+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:21.950+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T18:00:21.956+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:21.964+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T18:00:21.977+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:21.991+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T18:00:21.998+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.parquet atualizado e enviado para MinIO!
[2025-02-23T18:00:22.272+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:00:22.276+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:00:22.486+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:00:22.488+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T17:50:00+00:00, execution_date=20250223T175000, start_date=20250223T180007, end_date=20250223T180022
[2025-02-23T18:00:23.607+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:00:23.908+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:00:23.908+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:00:24.398+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:00:24.394+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:00:24.516+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.059 seconds
[2025-02-23T18:00:29.002+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:00:29.346+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T17:50:00+00:00 [queued]>
[2025-02-23T18:00:29.481+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T17:50:00+00:00 [queued]>
[2025-02-23T18:00:29.483+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:00:29.576+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 17:50:00+00:00
[2025-02-23T18:00:29.595+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=762) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:00:29.602+0000] {standard_task_runner.py:72} INFO - Started process 763 to run task
[2025-02-23T18:00:29.605+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T17:50:00+00:00', '--job-id', '35', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpl0g64ugc']
[2025-02-23T18:00:29.610+0000] {standard_task_runner.py:105} INFO - Job 35: Subtask process_bronze
[2025-02-23T18:12:12.099+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 0.934 seconds
zone_fornecedores_db1.parquet'.
[2025-02-23T18:00:38.028+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db1.parquet' enviado para 'datalake/bronze/db1/fornecedores_db1.parquet'.
[2025-02-23T18:00:38.032+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.parquet' baixado para 'temp_landzone_fornecedores_db2.parquet'.
[2025-02-23T18:00:38.039+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db2.parquet' enviado para 'datalake/bronze/db2/fornecedores_db2.parquet'.
[2025-02-23T18:00:38.048+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.parquet' baixado para 'temp_landzone_fornecedores_db3.parquet'.
[2025-02-23T18:00:38.051+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db3.parquet' enviado para 'datalake/bronze/db3/fornecedores_db3.parquet'.
[2025-02-23T18:00:38.053+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.parquet' baixado para 'temp_landzone_produtos_db1.parquet'.
[2025-02-23T18:00:38.055+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db1.parquet' enviado para 'datalake/bronze/db1/produtos_db1.parquet'.
[2025-02-23T18:00:38.057+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.parquet' baixado para 'temp_landzone_produtos_db2.parquet'.
[2025-02-23T18:00:38.059+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db2.parquet' enviado para 'datalake/bronze/db2/produtos_db2.parquet'.
[2025-02-23T18:00:38.061+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.parquet' baixado para 'temp_landzone_produtos_db3.parquet'.
[2025-02-23T18:00:38.065+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db3.parquet' enviado para 'datalake/bronze/db3/produtos_db3.parquet'.
[2025-02-23T18:00:38.068+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.parquet' baixado para 'temp_landzone_vendas_db1.parquet'.
[2025-02-23T18:00:38.072+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db1.parquet' enviado para 'datalake/bronze/db1/vendas_db1.parquet'.
[2025-02-23T18:00:38.075+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.parquet' baixado para 'temp_landzone_vendas_db2.parquet'.
[2025-02-23T18:00:38.078+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db2.parquet' enviado para 'datalake/bronze/db2/vendas_db2.parquet'.
[2025-02-23T18:00:38.083+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.parquet' baixado para 'temp_landzone_vendas_db3.parquet'.
[2025-02-23T18:00:38.085+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db3.parquet' enviado para 'datalake/bronze/db3/vendas_db3.parquet'.
[2025-02-23T18:00:38.089+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.parquet' baixado para 'temp_landzone_vendedores_db1.parquet'.
[2025-02-23T18:00:38.093+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db1.parquet' enviado para 'datalake/bronze/db1/vendedores_db1.parquet'.
[2025-02-23T18:00:38.105+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.parquet' baixado para 'temp_landzone_vendedores_db2.parquet'.
[2025-02-23T18:00:38.109+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db2.parquet' enviado para 'datalake/bronze/db2/vendedores_db2.parquet'.
[2025-02-23T18:00:38.116+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.parquet' baixado para 'temp_landzone_vendedores_db3.parquet'.
[2025-02-23T18:00:38.120+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db3.parquet' enviado para 'datalake/bronze/db3/vendedores_db3.parquet'.
[2025-02-23T18:00:38.181+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:00:38.184+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:00:38.354+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:00:38.359+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T17:50:00+00:00, execution_date=20250223T175000, start_date=20250223T180029, end_date=20250223T180038
[2025-02-23T18:00:38.563+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T18:00:38.688+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:00:38.693+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:00:45.184+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:00:45.377+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T17:50:00+00:00 [queued]>
[2025-02-23T18:00:45.460+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T17:50:00+00:00 [queued]>
[2025-02-23T18:00:45.465+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:00:45.568+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 17:50:00+00:00
[2025-02-23T18:00:45.594+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=768) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:00:45.604+0000] {standard_task_runner.py:72} INFO - Started process 769 to run task
[2025-02-23T18:00:45.606+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T17:50:00+00:00', '--job-id', '36', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp_on9dvu1']
[2025-02-23T18:00:45.615+0000] {standard_task_runner.py:105} INFO - Job 36: Subtask process_silver
[2025-02-23T18:00:55.064+0000] {processor.py:186} INFO - Started process (PID=283) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:00:55.118+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:00:55.127+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:00:55.125+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:00:58.177+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:00:58.408+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:00:58.407+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:00:58.550+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:00:58.549+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
rt_to_silver(file_path)
"[2025-02-23T18:00:57.430+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T18:00:57.443+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T18:00:57.452+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:00:57.462+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T18:00:57.467+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T18:00:57.470+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T18:00:57.472+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T18:00:57.479+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T18:00:57.488+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:00:57.491+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T18:00:57.495+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T18:00:57.499+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T18:00:57.514+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T18:00:57.539+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T18:00:57.547+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:00:57.553+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T18:00:57.560+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T18:00:57.592+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T18:00:57.613+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T18:00:58.168+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T18:00:58.174+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T18:00:58.282+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:00:58.340+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T17:50:00+00:00, execution_date=20250223T175000, start_date=20250223T180045, end_date=20250223T180058
[2025-02-23T18:00:58.541+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:00:58.551+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 36 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 769)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:00:58.649+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_[2025-02-23T18:00:58.727+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:00:58.754+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:01:29.069+0000] {processor.py:186} INFO - Started process (PID=285) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:01:29.072+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:01:29.078+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:01:29.076+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:01:32.226+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:01:32.404+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:01:32.403+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:01:32.560+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:01:32.559+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:01:32.703+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.665 seconds
[2025-02-23T18:02:03.007+0000] {processor.py:186} INFO - Started process (PID=287) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:02:03.011+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:02:03.019+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:02:03.017+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:02:05.499+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:02:05.836+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:02:05.835+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:02:06.058+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:02:06.057+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:02:06.174+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.195 seconds
[2025-02-23T18:02:36.855+0000] {processor.py:186} INFO - Started process (PID=289) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:02:36.863+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:02:36.869+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:02:36.868+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:02:38.667+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:02:38.838+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:02:38.836+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:02:38.982+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:02:38.982+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:02:39.099+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.281 seconds
[2025-02-23T18:03:09.535+0000] {processor.py:186} INFO - Started process (PID=292) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:03:09.563+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:03:09.568+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:03:09.567+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:03:11.374+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:03:11.481+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:03:11.478+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:03:11.778+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:03:11.776+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:03:11.963+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.453 seconds
[2025-02-23T18:03:42.580+0000] {processor.py:186} INFO - Started process (PID=294) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:03:42.587+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:03:42.595+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:03:42.594+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:03:44.314+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:03:44.446+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:03:44.444+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:03:44.589+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:03:44.587+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:03:44.683+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.139 seconds
[2025-02-23T18:04:14.873+0000] {processor.py:186} INFO - Started process (PID=296) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:04:15.468+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:04:15.479+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:04:15.475+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:04:19.415+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:04:19.713+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:04:19.712+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:04:19.914+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:04:19.913+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:04:20.088+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.263 seconds
[2025-02-23T18:04:50.707+0000] {processor.py:186} INFO - Started process (PID=298) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:04:50.755+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:04:50.772+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:04:50.766+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:04:56.123+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:04:56.364+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:04:56.362+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:04:56.581+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:04:56.580+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:04:56.698+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 6.060 seconds
[2025-02-23T18:05:27.090+0000] {processor.py:186} INFO - Started process (PID=300) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:05:27.124+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:05:27.142+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:05:27.140+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:05:29.205+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:05:29.349+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:05:29.348+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:05:29.443+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:05:29.442+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:05:29.572+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.528 seconds
[2025-02-23T18:06:00.146+0000] {processor.py:186} INFO - Started process (PID=302) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:06:00.151+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:06:00.160+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:06:00.159+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:06:02.095+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:06:02.190+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:06:02.189+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:06:02.295+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:06:02.294+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:06:02.519+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.405 seconds
[2025-02-23T18:06:33.321+0000] {processor.py:186} INFO - Started process (PID=304) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:06:33.332+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:06:33.342+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:06:33.340+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:06:35.288+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:06:35.416+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:06:35.415+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:06:35.515+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:06:35.513+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:06:35.616+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.334 seconds
[2025-02-23T18:06:59.468+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:06:59.686+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:30:00+00:00 [queued]>
[2025-02-23T18:06:59.780+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T15:30:00+00:00 [queued]>
[2025-02-23T18:06:59.783+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T18:06:59.913+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 15:30:00+00:00
[2025-02-23T18:06:59.949+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=781) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:06:59.964+0000] {standard_task_runner.py:72} INFO - Started process 782 to run task
[2025-02-23T18:06:59.965+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T15:30:00+00:00', '--job-id', '37', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp8tpccnsd']
[2025-02-23T18:06:59.973+0000] {standard_task_runner.py:105} INFO - Job 37: Subtask process_silver
[2025-02-23T18:07:06.179+0000] {processor.py:186} INFO - Started process (PID=306) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:07:06.192+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:07:06.218+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:07:06.217+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:07:10.370+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:07:10.523+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:07:10.522+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:07:10.988+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:07:10.987+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:07:11.216+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.110 seconds
[2025-02-23T18:07:12.777+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:07:12.968+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T17:40:00+00:00 [queued]>
[2025-02-23T18:07:13.056+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T17:40:00+00:00 [queued]>
[2025-02-23T18:07:13.061+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T18:07:13.206+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 17:40:00+00:00
[2025-02-23T18:07:13.303+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=784) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:07:13.321+0000] {standard_task_runner.py:72} INFO - Started process 792 to run task
[2025-02-23T18:07:13.321+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T17:40:00+00:00', '--job-id', '38', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp4mu97r7s']
[2025-02-23T18:07:13.344+0000] {standard_task_runner.py:105} INFO - Job 38: Subtask process_silver
[2025-02-23T18:07:20.922+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T18:07:20.926+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T18:07:20.932+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T18:07:20.940+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T18:07:20.944+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T18:07:20.955+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T18:07:20.971+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:07:21.007+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T18:07:21.066+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T18:07:21.107+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T18:07:21.111+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T18:07:21.107+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:07:21.106+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00
[2025-02-23T18:07:21.120+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T18:07:21.154+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:07:21.169+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T18:07:21.191+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T18:07:21.203+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T18:07:21.231+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T18:07:21.253+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T18:07:21.259+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:07:21.273+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T18:07:21.291+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T18:07:21.371+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T18:07:21.400+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T18:07:21.433+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 1.345 seconds
[2025-02-23T18:07:21.855+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T18:07:21.858+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T18:07:21.982+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:07:22.016+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T15:30:00+00:00, execution_date=20250223T153000, start_date=20250223T180659, end_date=20250223T180722
[2025-02-23T18:07:22.267+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:07:22.271+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 37 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 782)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:07:22.389+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T18:07:22.574+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:07:22.600+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:07:32.310+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T18:07:32.317+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T18:07:32.321+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T18:07:32.324+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T18:07:32.330+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T18:07:32.334+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T18:07:32.340+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:07:32.345+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T18:07:32.352+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T18:07:32.356+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T18:07:32.363+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T18:07:32.371+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T18:07:32.375+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:07:32.380+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T18:07:32.384+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T18:07:32.388+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T18:07:32.391+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T18:07:32.399+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T18:07:32.414+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:07:32.422+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T18:07:32.427+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T18:07:32.433+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T18:07:32.449+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T18:07:32.967+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T18:07:32.971+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T18:07:33.155+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:07:33.251+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T17:40:00+00:00, execution_date=20250223T174000, start_date=20250223T180712, end_date=20250223T180733
[2025-02-23T18:07:33.610+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:07:33.615+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 38 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 792)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:07:33.875+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T18:07:34.198+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:07:34.228+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:07:42.014+0000] {processor.py:186} INFO - Started process (PID=308) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:07:42.040+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:07:42.051+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:07:42.050+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:07:43.857+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:07:44.193+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:07:44.189+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:07:44.682+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:07:44.681+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:07:44.835+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.968 seconds
[2025-02-23T18:08:15.886+0000] {processor.py:186} INFO - Started process (PID=310) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:08:15.890+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:08:15.896+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:08:15.895+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:08:18.236+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:08:18.351+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:08:18.350+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:08:18.438+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:08:18.437+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:00:00+00:00, run_after=2025-02-23 18:10:00+00:00
[2025-02-23T18:08:18.488+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.724 seconds
[2025-02-23T18:10:10.964+0000] {processor.py:186} INFO - Started process (PID=312) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:10:11.110+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:10:11.180+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:10:11.161+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:10:14.928+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:10:15.058+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:10:15.057+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:10:15.186+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:10:15.184+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:10:15.289+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.717 seconds
[2025-02-23T18:10:17.270+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:10:17.401+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T18:00:00+00:00 [queued]>
[2025-02-23T18:10:17.443+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T18:00:00+00:00 [queued]>
[2025-02-23T18:10:17.445+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:10:17.528+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 18:00:00+00:00
[2025-02-23T18:10:17.555+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=807) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:10:17.563+0000] {standard_task_runner.py:72} INFO - Started process 808 to run task
[2025-02-23T18:10:17.570+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T18:00:00+00:00', '--job-id', '39', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmph2_kufht']
[2025-02-23T18:10:17.578+0000] {standard_task_runner.py:105} INFO - Job 39: Subtask process_landzone
[2025-02-23T18:10:17.822+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T18:00:00+00:00 [running]> on host b06401153325
[2025-02-23T18:10:18.267+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:00:00+00:00'
[2025-02-23T18:10:18.275+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T18:10:18.375+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:10:18.379+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T18:10:18.426+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:10:18.450+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T18:11:35.586+0000] {processor.py:186} INFO - Started process (PID=314) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:11:35.596+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:11:35.611+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:11:35.609+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:11:43.469+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:11:43.832+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:11:43.830+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:11:44.191+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:11:44.190+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:11:44.963+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 10.198 seconds
[2025-02-23T18:12:10.320+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:12:10.584+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T18:00:00+00:00 [queued]>
[2025-02-23T18:12:10.936+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T18:00:00+00:00 [queued]>
[2025-02-23T18:12:10.942+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:12:12.477+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T18:12:12.481+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T18:12:12.495+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T18:12:12.500+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T18:12:12.504+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T18:12:12.511+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T18:12:12.544+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:12:12.552+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T18:12:12.559+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T18:12:12.564+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T18:12:12.577+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T18:12:12.581+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T18:12:12.599+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:12:12.607+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T18:12:12.618+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T18:12:12.624+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T18:12:12.626+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T18:12:12.629+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T18:12:12.633+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:12:12.635+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T18:12:12.639+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T18:12:12.644+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T18:12:12.647+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T18:12:12.925+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:00:00+00:00'
[2025-02-23T18:12:12.931+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T18:12:13.054+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:12:13.062+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T18:12:13.065+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T18:12:13.069+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T18:12:13.130+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:12:13.160+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:12:13.269+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T18:12:13.292+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T17:50:00+00:00, execution_date=20250223T175000, start_date=20250223T181150, end_date=20250223T181213
[2025-02-23T18:12:13.608+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:12:13.614+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 40 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 818)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/[2025-02-23T21:11:35.667+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:35.648+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00"
py:266} INFO - Task exited with return code 1
[2025-02-23T18:12:15.760+0000] {processor.py:186} INFO - Started process (PID=317) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:12:15.770+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:12:15.780+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:12:15.778+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:12:24.795+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:12:25.314+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:12:25.313+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:12:26.152+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:12:26.150+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:12:26.745+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 11.048 seconds
[2025-02-23T18:12:34.986+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.parquet' baixado para 'temp_landzone_fornecedores_db1.parquet'.
[2025-02-23T18:12:34.996+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db1.parquet' enviado para 'datalake/bronze/db1/fornecedores_db1.parquet'.
[2025-02-23T18:12:35.037+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.parquet' baixado para 'temp_landzone_fornecedores_db2.parquet'.
[2025-02-23T18:12:35.041+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db2.parquet' enviado para 'datalake/bronze/db2/fornecedores_db2.parquet'.
[2025-02-23T18:12:35.048+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.parquet' baixado para 'temp_landzone_fornecedores_db3.parquet'.
[2025-02-23T18:12:35.138+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db3.parquet' enviado para 'datalake/bronze/db3/fornecedores_db3.parquet'.
[2025-02-23T18:12:35.157+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.parquet' baixado para 'temp_landzone_produtos_db1.parquet'.
[2025-02-23T18:12:35.247+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db1.parquet' enviado para 'datalake/bronze/db1/produtos_db1.parquet'.
[2025-02-23T18:12:35.253+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.parquet' baixado para 'temp_landzone_produtos_db2.parquet'.
[2025-02-23T18:12:35.257+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db2.parquet' enviado para 'datalake/bronze/db2/produtos_db2.parquet'.
[2025-02-23T18:12:35.260+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.parquet' baixado para 'temp_landzone_produtos_db3.parquet'.
[2025-02-23T18:12:35.276+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db3.parquet' enviado para 'datalake/bronze/db3/produtos_db3.parquet'.
[2025-02-23T18:12:35.279+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.parquet' baixado para 'temp_landzone_vendas_db1.parquet'.
[2025-02-23T18:12:35.286+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db1.parquet' enviado para 'datalake/bronze/db1/vendas_db1.parquet'.
[2025-02-23T18:12:35.292+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.parquet' baixado para 'temp_landzone_vendas_db2.parquet'.
[2025-02-23T18:12:35.294+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db2.parquet' enviado para 'datalake/bronze/db2/vendas_db2.parquet'.
[2025-02-23T18:12:35.297+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.parquet' baixado para 'temp_landzone_vendas_db3.parquet'.
[2025-02-23T18:12:35.305+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db3.parquet' enviado para 'datalake/bronze/db3/vendas_db3.parquet'.
[2025-02-23T18:12:35.308+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.parquet' baixado para 'temp_landzone_vendedores_db1.parquet'.
[2025-02-23T18:12:35.312+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db1.parquet' enviado para 'datalake/bronze/db1/vendedores_db1.parquet'.
[2025-02-23T18:12:35.315+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.parquet' baixado para 'temp_landzone_vendedores_db2.parquet'.
[2025-02-23T18:12:35.390+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db2.parquet' enviado para 'datalake/bronze/db2/vendedores_db2.parquet'.
[2025-02-23T18:12:35.395+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.parquet' baixado para 'temp_landzone_vendedores_db3.parquet'.
[2025-02-23T18:12:35.399+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db3.parquet' enviado para 'datalake/bronze/db3/vendedores_db3.parquet'.
[2025-02-23T18:12:35.739+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:12:35.746+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:12:36.445+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:12:36.459+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T18:00:00+00:00, execution_date=20250223T180000, start_date=20250223T181210, end_date=20250223T181236
[2025-02-23T18:12:37.032+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T18:12:37.263+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:12:37.270+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:12:57.245+0000] {processor.py:186} INFO - Started process (PID=319) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:12:57.307+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:12:57.341+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:12:57.339+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:13:05.427+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:13:06.525+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:13:06.521+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:13:06.934+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:13:06.933+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:13:07.085+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 9.844 seconds
[2025-02-23T18:13:22.125+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T18:13:22.138+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T18:13:22.142+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T18:13:22.150+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T18:13:22.154+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T18:13:22.157+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T18:13:22.160+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:13:22.167+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T18:13:22.170+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T18:13:22.178+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T18:13:22.186+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T18:13:22.190+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T18:13:22.196+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:13:22.202+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T18:13:22.204+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T18:13:22.208+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T18:13:22.216+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T18:13:22.220+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T18:13:22.225+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:13:22.228+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T18:13:22.234+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T18:13:22.237+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T18:13:22.240+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T18:13:22.829+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T18:13:22.841+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T18:13:23.036+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:13:23.170+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T18:00:00+00:00, execution_date=20250223T180000, start_date=20250223T181253, end_date=20250223T181323
[2025-02-23T18:13:23.551+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:13:23.556+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 42 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 873)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:13:23.687+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T18:13:24.227+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:13:24.242+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:13:24.474+0000] {standard_task_runner.py:217} INFO - Process not found (most likely exited), stop collecting metrics
[2025-02-23T18:13:37.888+0000] {processor.py:186} INFO - Started process (PID=321) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:13:37.899+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:13:37.937+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:13:37.935+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:13:43.033+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:13:43.443+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:13:43.440+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:13:43.686+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:13:43.685+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:13:43.818+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.979 seconds
[2025-02-23T18:14:14.530+0000] {processor.py:186} INFO - Started process (PID=323) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:14:14.547+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:14:14.567+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:14:14.565+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:14:18.105+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:14:18.282+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:14:18.280+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:14:18.528+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:14:18.525+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:14:18.745+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.356 seconds
[2025-02-23T18:14:49.451+0000] {processor.py:186} INFO - Started process (PID=325) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:14:49.505+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:14:49.515+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:14:49.514+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:14:55.485+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:14:55.630+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:14:55.629+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:14:55.747+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:14:55.746+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:14:55.845+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 6.608 seconds
[2025-02-23T18:15:26.471+0000] {processor.py:186} INFO - Started process (PID=327) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:15:26.479+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:15:26.486+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:15:26.485+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:15:29.501+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:15:30.006+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:15:30.003+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:15:30.253+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:15:30.252+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:15:30.381+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.960 seconds
[2025-02-23T18:16:00.745+0000] {processor.py:186} INFO - Started process (PID=330) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:16:00.752+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:16:00.759+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:16:00.756+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:16:02.711+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:16:02.827+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:16:02.827+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:16:02.967+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:16:02.966+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:16:03.042+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.328 seconds
[2025-02-23T18:16:33.611+0000] {processor.py:186} INFO - Started process (PID=332) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:16:33.617+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:16:33.626+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:16:33.623+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:16:35.427+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:16:35.544+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:16:35.542+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:16:35.696+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:16:35.694+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:16:35.858+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.322 seconds
[2025-02-23T18:17:06.160+0000] {processor.py:186} INFO - Started process (PID=334) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:17:06.162+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:17:06.166+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:17:06.166+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:17:07.104+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:17:07.182+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:17:07.181+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:17:07.279+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:17:07.277+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:17:07.389+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.245 seconds
[2025-02-23T18:17:38.486+0000] {processor.py:186} INFO - Started process (PID=336) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:17:38.499+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:17:38.503+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:17:38.502+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:17:48.703+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:17:49.122+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:17:49.118+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:17:49.874+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:17:49.873+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:17:50.378+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 12.138 seconds
[2025-02-23T18:18:21.422+0000] {processor.py:186} INFO - Started process (PID=338) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:18:21.703+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:18:21.707+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:18:21.707+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:18:24.100+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:18:24.233+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:18:24.232+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:18:24.366+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:18:24.365+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:18:24.679+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.287 seconds
[2025-02-23T18:18:55.395+0000] {processor.py:186} INFO - Started process (PID=340) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:18:55.398+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:18:55.401+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:18:55.400+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:18:56.301+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:18:56.373+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:18:56.372+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:18:56.457+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:18:56.457+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:18:56.541+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.165 seconds
[2025-02-23T18:19:27.411+0000] {processor.py:186} INFO - Started process (PID=342) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:19:27.434+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:19:27.440+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:19:27.438+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:19:29.883+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:19:30.046+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:19:30.045+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:19:30.223+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:19:30.222+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:10:00+00:00, run_after=2025-02-23 18:20:00+00:00
[2025-02-23T18:19:30.426+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.051 seconds
[2025-02-23T18:20:01.536+0000] {processor.py:186} INFO - Started process (PID=344) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:20:01.591+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:20:01.598+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:20:01.597+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:20:03.327+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:20:03.396+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:20:03.395+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:20:03.468+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:20:03.467+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:20:03.559+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.119 seconds
[2025-02-23T18:20:04.590+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:20:04.674+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T18:10:00+00:00 [queued]>
[2025-02-23T18:20:04.725+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T18:10:00+00:00 [queued]>
[2025-02-23T18:20:04.730+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:20:04.893+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 18:10:00+00:00
[2025-02-23T18:20:04.928+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=885) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:20:04.935+0000] {standard_task_runner.py:72} INFO - Started process 886 to run task
[2025-02-23T18:20:04.934+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T18:10:00+00:00', '--job-id', '43', '--raw', '--subdir', 'DAGS_FOLDER/dag_sche[2025-02-23T18:46:04.140+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
ard_task_runner.py:105} INFO - Job 43: Subtask process_landzone
[2025-02-23T18:20:19.561+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:20:19.641+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T18:10:00+00:00 [queued]>
[2025-02-23T18:20:19.668+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T18:10:00+00:00 [queued]>
[2025-02-23T18:20:19.669+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:20:19.780+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 18:10:00+00:00
[2025-02-23T18:20:19.797+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=931) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:20:19.810+0000] {standard_task_runner.py:72} INFO - Started process 932 to run task
[2025-02-23T18:20:19.811+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T18:10:00+00:00', '--job-id', '44', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp7i3rhgle']
[2025-02-23T18:20:19.814+0000] {standard_task_runner.py:105} INFO - Job 44: Subtask process_bronze
[2025-02-23T18:20:20.076+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T18:10:00+00:00 [running]> on host b06401153325
[2025-02-23T18:20:20.456+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:10:00+00:00'
[2025-02-23T18:20:20.467+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T18:20:20.587+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:20:20.590+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T18:20:20.619+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:20:20.634+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T18:20:26.146+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.parquet' baixado para 'temp_landzone_fornecedores_db1.parquet'.
[2025-02-23T18:20:26.151+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db1.parquet' enviado para 'datalake/bronze/db1/fornecedores_db1.parquet'.
[2025-02-23T18:20:26.156+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.parquet' baixado para 'temp_landzone_fornecedores_db2.parquet'.
[2025-02-23T18:20:26.162+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db2.parquet' enviado para 'datalake/bronze/db2/fornecedores_db2.parquet'.
[2025-02-23T18:20:26.165+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.parquet' baixado para 'temp_landzone_fornecedores_db3.parquet'.
[2025-02-23T18:20:26.168+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_fornecedores_db3.parquet' enviado para 'datalake/bronze/db3/fornecedores_db3.parquet'.
[2025-02-23T18:20:26.170+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.parquet' baixado para 'temp_landzone_produtos_db1.parquet'.
[2025-02-23T18:20:26.173+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db1.parquet' enviado para 'datalake/bronze/db1/produtos_db1.parquet'.
[2025-02-23T18:20:26.176+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.parquet' baixado para 'temp_landzone_produtos_db2.parquet'.
[2025-02-23T18:20:26.179+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db2.parquet' enviado para 'datalake/bronze/db2/produtos_db2.parquet'.
[2025-02-23T18:20:26.182+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.parquet' baixado para 'temp_landzone_produtos_db3.parquet'.
[2025-02-23T18:20:26.185+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_produtos_db3.parquet' enviado para 'datalake/bronze/db3/produtos_db3.parquet'.
[2025-02-23T18:20:26.188+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.parquet' baixado para 'temp_landzone_vendas_db1.parquet'.
[2025-02-23T18:20:26.191+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db1.parquet' enviado para 'datalake/bronze/db1/vendas_db1.parquet'.
[2025-02-23T18:20:26.193+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.parquet' baixado para 'temp_landzone_vendas_db2.parquet'.
[2025-02-23T18:20:26.197+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db2.parquet' enviado para 'datalake/bronze/db2/vendas_db2.parquet'.
[2025-02-23T18:20:26.199+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.parquet' baixado para 'temp_landzone_vendas_db3.parquet'.
[2025-02-23T18:20:26.202+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendas_db3.parquet' enviado para 'datalake/bronze/db3/vendas_db3.parquet'.
[2025-02-23T18:20:26.204+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.parquet' baixado para 'temp_landzone_vendedores_db1.parquet'.
[2025-02-23T18:20:26.207+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db1.parquet' enviado para 'datalake/bronze/db1/vendedores_db1.parquet'.
[2025-02-23T18:20:26.210+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.parquet' baixado para 'temp_landzone_vendedores_db2.parquet'.
[2025-02-23T18:20:26.212+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db2.parquet' enviado para 'datalake/bronze/db2/vendedores_db2.parquet'.
[2025-02-23T18:20:26.215+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.parquet' baixado para 'temp_landzone_vendedores_db3.parquet'.
[2025-02-23T18:20:26.216+0000] {subprocess.py:106} INFO - Arquivo 'temp_landzone_vendedores_db3.parquet' enviado para 'datalake/bronze/db3/vendedores_db3.parquet'.
[2025-02-23T18:20:26.254+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:20:26.257+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:20:26.412+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:20:26.415+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T18:10:00+00:00, execution_date=20250223T181000, start_date=20250223T182019, end_date=20250223T182026
[2025-02-23T18:20:26.790+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T18:20:26.969+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:20:26.975+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:20:31.333+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:20:31.399+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:10:00+00:00 [queued]>
[2025-02-23T18:20:31.455+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:10:00+00:00 [queued]>
[2025-02-23T18:20:31.458+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:20:31.575+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 18:10:00+00:00
[2025-02-23T18:20:31.613+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=937) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:20:31.627+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T18:10:00+00:00', '--job-id', '45', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpbuxtxy_q']
[2025-02-23T18:20:31.638+0000] {standard_task_runner.py:105} INFO - Job 45: Subtask process_silver
[2025-02-23T18:20:31.639+0000] {standard_task_runner.py:72} INFO - Started process 938 to run task
[2025-02-23T18:20:32.054+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:10:00+00:00 [running]> on host b06401153325
[2025-02-23T18:20:32.545+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:10:00+00:00'
[2025-02-23T18:20:32.550+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T18:20:32.627+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:20:32.632+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T18:20:32.660+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:20:32.674+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T18:20:34.350+0000] {processor.py:186} INFO - Started process (PID=346) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:20:34.363+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:20:34.372+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:20:34.371+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:20:36.548+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:20:36.915+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:20:36.914+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:20:37.462+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:20:37.458+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:20:37.540+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.252 seconds
[2025-02-23T18:20:49.111+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T18:20:49.119+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T18:20:49.123+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T18:20:49.131+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T18:20:49.134+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T18:20:49.136+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T18:20:49.138+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:20:49.145+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T18:20:49.149+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T18:20:49.152+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T18:20:49.155+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T18:20:49.161+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T18:20:49.164+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:20:49.167+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T18:20:49.171+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T18:20:49.176+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T18:20:49.179+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T18:20:49.182+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T18:20:49.186+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:20:49.189+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T18:20:49.198+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T18:20:49.201+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T18:20:49.204+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T18:20:50.883+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T18:20:50.936+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T18:20:51.063+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:20:51.205+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T18:10:00+00:00, execution_date=20250223T181000, start_date=20250223T182031, end_date=20250223T182051
[2025-02-23T18:20:51.612+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:20:51.615+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 45 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 938)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:20:51.893+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T18:20:52.576+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:20:52.751+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:21:07.845+0000] {processor.py:186} INFO - Started process (PID=348) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:21:07.849+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:21:07.856+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:21:07.854+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:21:08.978+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:21:09.046+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:21:09.045+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:21:09.102+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:21:09.102+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:21:09.164+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.348 seconds
[2025-02-23T18:21:39.692+0000] {processor.py:186} INFO - Started process (PID=350) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:21:39.733+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:21:39.739+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:21:39.738+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:21:42.208+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:21:42.400+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:21:42.399+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:21:42.593+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:21:42.592+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:21:42.750+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.083 seconds
[2025-02-23T18:22:12.967+0000] {processor.py:186} INFO - Started process (PID=352) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:22:12.969+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:22:12.973+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:22:12.972+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:22:13.732+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:22:13.784+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:22:13.783+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:22:13.828+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:22:13.828+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:22:13.907+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.958 seconds
[2025-02-23T18:22:44.228+0000] {processor.py:186} INFO - Started process (PID=354) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:22:44.231+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:22:44.235+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:22:44.234+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:22:45.037+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:22:45.085+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:22:45.084+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:22:45.138+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:22:45.137+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:22:45.225+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.019 seconds
[2025-02-23T18:23:15.494+0000] {processor.py:186} INFO - Started process (PID=356) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:23:15.514+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:23:15.517+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:23:15.516+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:23:16.429+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:23:16.563+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:23:16.562+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:23:16.707+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:23:16.706+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:23:16.814+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.334 seconds
[2025-02-23T18:23:26.925+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:23:27.003+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:00:00+00:00 [queued]>
[2025-02-23T18:23:27.027+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:00:00+00:00 [queued]>
[2025-02-23T18:23:27.029+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T18:23:27.073+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 18:00:00+00:00
[2025-02-23T18:23:27.092+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=950) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:23:27.098+0000] {standard_task_runner.py:72} INFO - Started process 951 to run task
[2025-02-23T18:23:27.098+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T18:00:00+00:00', '--job-id', '46', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp4vtpei_h']
[2025-02-23T18:23:27.102+0000] {standard_task_runner.py:105} INFO - Job 46: Subtask process_silver
[2025-02-23T18:23:27.249+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:00:00+00:00 [running]> on host b06401153325
[2025-02-23T18:23:27.779+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:00:00+00:00'
[2025-02-23T18:23:27.782+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T18:23:27.822+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:23:27.825+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T18:23:27.847+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:23:27.859+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T18:23:33.239+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.parquet' baixado para 'temp_bronze_fornecedores_db1.parquet'.
[2025-02-23T18:23:33.268+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T18:23:33.271+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 107, in <module>"
[2025-02-23T18:23:33.273+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T18:23:33.275+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 59, in upsert_to_silver"
[2025-02-23T18:23:33.277+0000] {subprocess.py:106} INFO -     df = pd.read_parquet(local_bronze_file)
[2025-02-23T18:23:33.280+0000] {subprocess.py:106} INFO -          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:23:33.282+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 670, in read_parquet"
[2025-02-23T18:23:33.284+0000] {subprocess.py:106} INFO -     return impl.read(
[2025-02-23T18:23:33.286+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^
"[2025-02-23T18:23:33.289+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/io/parquet.py"", line 272, in read"
[2025-02-23T18:23:33.292+0000] {subprocess.py:106} INFO -     pa_table = self.api.parquet.read_table(
[2025-02-23T18:23:33.294+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:23:33.296+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1793, in read_table"
[2025-02-23T18:23:33.299+0000] {subprocess.py:106} INFO -     dataset = ParquetDataset(
[2025-02-23T18:23:33.313+0000] {subprocess.py:106} INFO -               ^^^^^^^^^^^^^^^
"[2025-02-23T18:23:33.402+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pyarrow/parquet/core.py"", line 1360, in __init__"
[2025-02-23T18:23:33.406+0000] {subprocess.py:106} INFO -     [fragment], schema=schema or fragment.physical_schema,
[2025-02-23T18:23:33.409+0000] {subprocess.py:106} INFO -                                  ^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:23:33.412+0000] {subprocess.py:106} INFO -   File ""pyarrow/_dataset.pyx"", line 1443, in pyarrow._dataset.Fragment.physical_schema.__get__"
"[2025-02-23T18:23:33.415+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 155, in pyarrow.lib.pyarrow_internal_check_status"
"[2025-02-23T18:23:33.420+0000] {subprocess.py:106} INFO -   File ""pyarrow/error.pxi"", line 92, in pyarrow.lib.check_status"
[2025-02-23T18:23:33.422+0000] {subprocess.py:106} INFO - pyarrow.lib.ArrowInvalid: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.
[2025-02-23T18:23:33.695+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T18:23:33.699+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T18:23:33.772+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:23:33.793+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T18:00:00+00:00, execution_date=20250223T180000, start_date=20250223T182327, end_date=20250223T182333
[2025-02-23T18:23:33.924+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:23:33.926+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 46 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 951)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           [2025-02-23T18:46:04.162+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:23:33.986+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T18:23:34.040+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:23:34.101+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:23:47.534+0000] {processor.py:186} INFO - Started process (PID=358) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:23:47.536+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:23:47.539+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:23:47.539+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:23:48.423+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:23:48.469+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:23:48.469+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:23:48.522+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:23:48.522+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:23:48.560+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.037 seconds
[2025-02-23T18:24:19.265+0000] {processor.py:186} INFO - Started process (PID=360) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:24:19.391+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:24:19.441+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:24:19.440+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:24:21.975+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:24:22.061+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:24:22.061+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:24:22.140+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:24:22.139+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:24:22.297+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.057 seconds
[2025-02-23T18:24:52.799+0000] {processor.py:186} INFO - Started process (PID=362) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:24:52.831+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:24:52.842+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:24:52.840+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:24:58.540+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:24:59.278+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:24:59.276+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:24:59.978+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:24:59.973+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:25:00.516+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 7.782 seconds
[2025-02-23T18:25:31.206+0000] {processor.py:186} INFO - Started process (PID=365) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:25:31.247+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:25:31.255+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:25:31.253+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:25:33.436+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:25:33.490+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:25:33.489+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:25:33.536+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:25:33.536+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:25:33.670+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.505 seconds
[2025-02-23T18:26:04.514+0000] {processor.py:186} INFO - Started process (PID=367) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:26:04.519+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:26:04.542+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:26:04.538+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:26:05.970+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:26:06.072+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:26:06.064+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:26:06.260+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:26:06.260+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:26:06.392+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.914 seconds
[2025-02-23T18:26:36.520+0000] {processor.py:186} INFO - Started process (PID=369) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:26:36.524+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:26:36.527+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:26:36.526+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[202[2025-02-23T18:45:21.322+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:45:21.318+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00
26+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:26:39.107+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:26:39.107+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:26:39.157+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.662 seconds
[2025-02-23T18:27:09.351+0000] {processor.py:186} INFO - Started process (PID=371) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:27:09.353+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:27:09.357+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:27:09.356+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:27:10.403+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:27:10.540+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:27:10.539+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:27:10.606+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:27:10.605+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:27:10.653+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.318 seconds
[2025-02-23T18:27:40.973+0000] {processor.py:186} INFO - Started process (PID=373) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:27:40.986+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:27:40.994+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:27:40.992+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:27:44.988+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:27:45.134+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:27:45.131+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:27:45.311+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:27:45.310+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:27:45.420+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.470 seconds
[2025-02-23T18:28:16.027+0000] {processor.py:186} INFO - Started process (PID=375) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:28:16.036+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:28:16.045+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:28:16.043+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:28:20.779+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:28:21.010+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:28:21.008+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:28:21.704+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:28:21.703+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:28:21.955+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.974 seconds
[2025-02-23T18:28:52.363+0000] {processor.py:186} INFO - Started process (PID=377) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:28:52.372+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:28:52.379+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:28:52.377+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:28:54.220+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:28:54.279+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:28:54.278+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:28:54.331+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:28:54.330+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:28:54.392+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.068 seconds
[2025-02-23T18:29:24.651+0000] {processor.py:186} INFO - Started process (PID=379) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:29:24.695+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:29:24.707+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:29:24.706+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:29:26.340+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:29:26.442+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:29:26.442+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:29:26.525+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:29:26.524+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:29:26.617+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.026 seconds
[2025-02-23T18:29:57.061+0000] {processor.py:186} INFO - Started process (PID=381) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:29:57.176+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:29:57.191+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:29:57.180+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:29:58.700+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:29:58.943+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:29:58.943+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:29:59.204+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:29:59.203+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:29:59.511+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.491 seconds
[2025-02-23T18:30:30.379+0000] {processor.py:186} INFO - Started process (PID=383) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:30:30.414+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:30:30.422+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:30:30.420+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:30:31.699+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:30:32.241+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:30:32.240+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:30:32.716+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:30:32.715+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:30:32.822+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.471 seconds
[2025-02-23T18:31:03.695+0000] {processor.py:186} INFO - Started process (PID=385) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:31:03.699+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:31:03.703+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:31:03.702+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:31:05.801+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:31:06.014+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:31:06.013+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:31:06.156+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:31:06.155+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:31:06.326+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.667 seconds
[2025-02-23T18:31:37.079+0000] {processor.py:186} INFO - Started process (PID=387) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:31:37.083+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:31:37.091+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:31:37.089+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:31:38.057+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:31:38.126+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:31:38.126+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:31:38.217+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:31:38.216+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:31:38.306+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.263 seconds
[2025-02-23T18:32:08.870+0000] {processor.py:186} INFO - Started process (PID=389) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:32:08.872+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:32:08.875+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:32:08.874+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:32:09.923+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:32:10.052+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:32:10.052+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:32:10.245+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:32:10.244+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:32:10.504+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.646 seconds
[2025-02-23T18:32:41.511+0000] {processor.py:186} INFO - Started process (PID=391) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:32:41.516+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:32:41.522+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:32:41.520+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:32:42.873+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:32:42.945+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:32:42.944+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:32:43.000+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:32:43.000+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:32:43.074+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.597 seconds
[2025-02-23T18:33:13.530+0000] {processor.py:186} INFO - Started process (PID=393) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:33:13.532+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:33:13.540+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:33:13.539+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:33:14.333+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:33:14.400+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:33:14.399+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:33:14.462+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:33:14.461+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:33:14.523+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.018 seconds
[2025-02-23T18:33:45.089+0000] {processor.py:186} INFO - Started process (PID=395) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:33:45.090+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:33:45.095+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:33:45.094+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:33:46.181+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:33:46.257+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:33:46.256+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:33:46.332+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:33:46.331+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:33:46.405+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.332 seconds
[2025-02-23T18:34:16.963+0000] {processor.py:186} INFO - Started process (PID=397) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:34:17.010+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:34:17.024+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:34:17.022+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:34:18.826+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:34:19.017+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:34:19.016+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:34:19.135+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:34:19.134+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:34:19.243+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.498 seconds
[2025-02-23T18:34:50.054+0000] {processor.py:186} INFO - Started process (PID=399) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:34:50.176+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:34:50.186+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:34:50.185+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:34:51.159+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:34:51.235+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:34:51.234+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:34:51.309+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:34:51.308+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:34:51.384+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.357 seconds
[2025-02-23T18:35:21.705+0000] {processor.py:186} INFO - Started process (PID=401) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:35:21.729+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:35:21.733+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:35:21.732+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:35:23.132+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:35:23.232+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:35:23.231+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:35:23.345+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:35:23.342+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:35:23.418+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.738 seconds
[2025-02-23T18:35:53.938+0000] {processor.py:186} INFO - Started process (PID=403) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:35:54.051+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:35:54.064+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:35:54.061+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:35:55.832+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:35:55.927+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:35:55.927+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:35:56.049+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:35:56.047+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:35:56.157+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.240 seconds
[2025-02-23T18:36:27.017+0000] {processor.py:186} INFO - Started process (PID=405) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:36:27.025+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:36:27.030+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:36:27.029+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:36:28.508+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:36:28.595+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:36:28.593+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:36:28.714+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:36:28.713+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:36:28.910+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.929 seconds
[2025-02-23T18:36:59.600+0000] {processor.py:186} INFO - Started process (PID=407) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:36:59.603+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:36:59.608+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:36:59.608+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:37:00.839+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:37:00.911+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:37:00.910+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:37:00.968+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:37:00.968+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:37:01.067+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.484 seconds
[2025-02-23T18:37:31.503+0000] {processor.py:186} INFO - Started process (PID=409) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:37:31.505+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:37:31.509+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:37:31.508+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:37:32.505+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:37:32.557+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:37:32.557+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:37:32.602+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:37:32.602+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:37:32.678+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.191 seconds
[2025-02-23T18:38:02.834+0000] {processor.py:186} INFO - Started process (PID=411) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:38:02.836+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:38:02.840+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:38:02.839+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:38:04.018+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:38:04.131+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:38:04.128+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:38:04.330+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:38:04.329+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:38:04.393+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.576 seconds
[2025-02-23T18:38:34.780+0000] {processor.py:186} INFO - Started process (PID=414) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:38:35.248+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:38:35.256+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:38:35.254+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:38:37.605+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:38:37.821+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:38:37.819+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:38:37.961+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:38:37.960+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:38:38.210+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.469 seconds
[2025-02-23T18:39:08.730+0000] {processor.py:186} INFO - Started process (PID=416) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:39:08.736+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:39:08.745+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:39:08.743+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:39:19.395+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:39:19.779+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:39:19.778+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:39:21.140+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:39:21.139+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:39:22.015+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 13.330 seconds
[2025-02-23T18:39:53.227+0000] {processor.py:186} INFO - Started process (PID=418) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:39:53.675+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:39:53.679+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:39:53.678+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:39:55.110+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:39:55.187+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:39:55.187+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:39:55.265+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:39:55.265+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:20:00+00:00, run_after=2025-02-23 18:30:00+00:00
[2025-02-23T18:39:55.349+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.143 seconds
[2025-02-23T18:40:26.486+0000] {processor.py:186} INFO - Started process (PID=420) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:40:26.603+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:40:26.613+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:40:26.612+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:40:32.031+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:40:32.209+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:40:32.208+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:40:32.460+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:40:32.458+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:30:00+00:00, run_after=2025-02-23 18:40:00+00:00
[2025-02-23T18:40:32.727+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 6.438 seconds
[2025-02-23T18:41:03.602+0000] {processor.py:186} INFO - Started process (PID=422) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:41:03.720+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:41:03.745+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:41:03.736+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:41:05.538+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:41:05.649+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:41:05.648+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:41:05.743+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:41:05.742+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:30:00+00:00, run_after=2025-02-23 18:40:00+00:00
[2025-02-23T18:41:05.864+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.299 seconds
[2025-02-23T18:41:36.050+0000] {processor.py:186} INFO - Started process (PID=424) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:41:36.054+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:41:36.063+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:41:36.062+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:41:37.767+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:41:37.851+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:41:37.850+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:41:38.162+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:41:38.161+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:30:00+00:00, run_after=2025-02-23 18:40:00+00:00
[2025-02-23T18:41:38.420+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.401 seconds
[2025-02-23T18:42:09.158+0000] {processor.py:186} INFO - Started process (PID=426) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:42:09.179+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:42:09.197+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:42:09.194+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:42:14.086+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:42:14.196+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:42:14.195+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:42:14.351+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:42:14.351+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:30:00+00:00, run_after=2025-02-23 18:40:00+00:00
[2025-02-23T18:42:14.437+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.319 seconds
[2025-02-23T18:42:45.324+0000] {processor.py:186} INFO - Started process (PID=428) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:42:45.612+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:42:45.621+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:42:45.619+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:42:47.976+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:42:48.865+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:42:48.864+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:42:49.230+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:42:49.229+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:30:00+00:00, run_after=2025-02-23 18:40:00+00:00
[2025-02-23T18:42:49.303+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.074 seconds
[2025-02-23T18:43:19.824+0000] {processor.py:186} INFO - Started process (PID=434) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:43:19.828+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:43:19.835+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:43:19.833+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:43:22.727+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:43:23.332+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:43:23.331+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:43:24.013+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:43:24.004+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:40:00+00:00, run_after=2025-02-23 18:50:00+00:00
[2025-02-23T18:43:25.369+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.571 seconds
pid=963) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:43:19.476+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T18:30:00+00:00', '--job-id', '47', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp735k_dl2']
[2025-02-23T18:43:19.479+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:10:00+00:00 [queued]>
[2025-02-23T18:43:19.481+0000] {standard_task_runner.py:72} INFO - Started process 965 to run task
[2025-02-23T18:43:19.482+0000] {standard_task_runner.py:105} INFO - Job 47: Subtask process_landzone
[2025-02-23T18:43:19.529+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:10:00+00:00 [queued]>
[2025-02-23T18:43:19.532+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T18:43:19.611+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 18:10:00+00:00
[2025-02-23T18:43:19.649+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=964) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:43:19.658+0000] {standard_task_runner.py:72} INFO - Started process 967 to run task
[2025-02-23T18:43:19.658+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T18:10:00+00:00', '--job-id', '48', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp0in3eqoz']
[2025-02-23T18:43:19.679+0000] {standard_task_runner.py:105} INFO - Job 48: Subtask process_silver
[2025-02-23T18:43:19.725+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325
[2025-02-23T18:43:19.900+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:10:00+00:00 [running]> on host b06401153325
[2025-02-23T18:43:21.042+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:30:00+00:00'
[2025-02-23T18:43:21.077+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T18:43:21.267+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:43:21.288+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T18:43:21.351+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:43:21.418+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T18:43:21.418+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:10:00+00:00'
[2025-02-23T18:43:21.426+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T18:43:21.581+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:43:21.595+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T18:43:21.704+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:43:21.757+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T18:43:58.336+0000] {processor.py:186} INFO - Started process (PID=436) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:43:58.631+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:43:58.859+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:43:58.782+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:44:04.472+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:44:06.461+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:44:06.445+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:44:07.044+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:44:07.043+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:40:00+00:00, run_after=2025-02-23 18:50:00+00:00
[2025-02-23T18:44:07.952+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 10.072 seconds
Traceback (most recent call last):
"[2025-02-23T18:44:06.560+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 91, in <module>"
[2025-02-23T18:44:06.574+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T18:44:06.578+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 75, in upsert_to_silver"
"[2025-02-23T18:44:06.602+0000] {subprocess.py:106} INFO -     silver_df = silver_df.set_index(""id"").combine_first(df.set_index(""id"")).reset_index()"
[2025-02-23T18:44:06.622+0000] {subprocess.py:106} INFO -                 ^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:44:06.626+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/core/frame.py"", line 5870, in set_index"
"[2025-02-23T18:44:06.641+0000] {subprocess.py:106} INFO -     raise KeyError(f""None of {missing} are in the columns"")"
"[2025-02-23T18:44:06.656+0000] {subprocess.py:106} INFO - KeyError: ""None of ['id'] are in the columns"""
[2025-02-23T18:44:06.817+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:44:06.813+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:44:08.265+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:44:08.222+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00
[2025-02-23T18:44:09.284+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 5.240 seconds
[2025-02-23T18:44:09.531+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T18:44:09.542+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T18:44:09.751+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:44:09.912+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T18:10:00+00:00, execution_date=20250223T181000, start_date=20250223T184319, end_date=20250223T184409
[2025-02-23T18:44:10.458+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:44:10.484+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 48 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 967)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:44:10.607+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T18:44:10.967+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:44:11.006+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:44:17.740+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T18:44:17.744+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:17.749+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T18:44:17.796+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:17.799+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T18:44:17.805+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:17.849+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T18:44:17.881+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:17.946+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T18:44:17.960+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:17.975+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T18:44:17.981+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:17.991+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T18:44:18.013+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:18.032+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T18:44:18.117+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:18.193+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T18:44:18.199+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:18.232+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T18:44:18.259+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:18.267+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T18:44:18.281+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:18.284+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T18:44:18.291+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.csv atualizado e enviado para MinIO!
[2025-02-23T18:44:20.890+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:44:21.143+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:44:22.035+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:44:22.044+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T18:30:00+00:00, execution_date=20250223T183000, start_date=20250223T184319, end_date=20250223T184422
[2025-02-23T18:44:22.339+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T18:44:22.647+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:44:22.656+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:44:40.187+0000] {processor.py:186} INFO - Started process (PID=438) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:44:40.272+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:44:40.320+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:44:40.318+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:44:47.216+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:44:47.854+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:44:47.853+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:44:48.689+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:44:48.688+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:40:00+00:00, run_after=2025-02-23 18:50:00+00:00
[2025-02-23T18:44:49.227+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 9.393 seconds
[2025-02-23T18:45:07.375+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:45:07.401+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T18:45:07.497+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:45:07.581+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T18:45:19.448+0000] {processor.py:925} INFO - DAG(s) 'scheduler_v1' retrieved from /opt/airflow/dags/dag_scheduler_v1.py
[2025-02-23T18:45:20.488+0000] {processor.py:186} INFO - Started process (PID=441) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:45:20.502+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:45:20.566+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:45:20.555+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:45:21.926+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 3.247 seconds
[2025-02-23T18:45:29.150+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:45:29.615+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:45:29.614+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:45:31.490+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:45:31.490+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:40:00+00:00, run_after=2025-02-23 18:50:00+00:00
[2025-02-23T18:45:32.496+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 12.462 seconds
[2025-02-23T18:45:33.616+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.csv' baixado para 'temp_landzone_fornecedores_db1.csv'.
[2025-02-23T18:45:33.641+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db1.csv' enviado para 'datalake/bronze/db1/fornecedores_db1.csv'.
[2025-02-23T18:45:33.657+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.csv' baixado para 'temp_landzone_fornecedores_db2.csv'.
[2025-02-23T18:45:33.665+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db2.csv' enviado para 'datalake/bronze/db2/fornecedores_db2.csv'.
[2025-02-23T18:45:33.670+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.csv' baixado para 'temp_landzone_fornecedores_db3.csv'.
[2025-02-23T18:45:33.674+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db3.csv' enviado para 'datalake/bronze/db3/fornecedores_db3.csv'.
[2025-02-23T18:45:33.682+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.csv' baixado para 'temp_landzone_produtos_db1.csv'.
[2025-02-23T18:45:33.688+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db1.csv' enviado para 'datalake/bronze/db1/produtos_db1.csv'.
[2025-02-23T18:45:33.693+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.csv' baixado para 'temp_landzone_produtos_db2.csv'.
[2025-02-23T18:45:33.701+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db2.csv' enviado para 'datalake/bronze/db2/produtos_db2.csv'.
[2025-02-23T18:45:33.707+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.csv' baixado para 'temp_landzone_produtos_db3.csv'.
[2025-02-23T18:45:33.716+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db3.csv' enviado para 'datalake/bronze/db3/produtos_db3.csv'.
[2025-02-23T18:45:33.721+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.csv' baixado para 'temp_landzone_vendas_db1.csv'.
[2025-02-23T18:45:33.725+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db1.csv' enviado para 'datalake/bronze/db1/vendas_db1.csv'.
[2025-02-23T18:45:33.733+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.csv' baixado para 'temp_landzone_vendas_db2.csv'.
[2025-02-23T18:45:33.740+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db2.csv' enviado para 'datalake/bronze/db2/vendas_db2.csv'.
[2025-02-23T18:45:33.746+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.csv' baixado para 'temp_landzone_vendas_db3.csv'.
[2025-02-23T18:45:33.749+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db3.csv' enviado para 'datalake/bronze/db3/vendas_db3.csv'.
[2025-02-23T18:45:33.754+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.csv' baixado para 'temp_landzone_vendedores_db1.csv'.
[2025-02-23T18:45:33.758+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db1.csv' enviado para 'datalake/bronze/db1/vendedores_db1.csv'.
[2025-02-23T18:45:33.770+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.csv' baixado para 'temp_landzone_vendedores_db2.csv'.
[2025-02-23T18:45:33.775+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db2.csv' enviado para 'datalake/bronze/db2/vendedores_db2.csv'.
[2025-02-23T18:45:33.785+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.csv' baixado para 'temp_landzone_vendedores_db3.csv'.
[2025-02-23T18:45:33.798+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db3.csv' enviado para 'datalake/bronze/db3/vendedores_db3.csv'.
[2025-02-23T18:45:35.979+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:45:35.992+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:45:36.462+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:45:36.470+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T18:30:00+00:00, execution_date=20250223T183000, start_date=20250223T184438, end_date=20250223T184536
[2025-02-23T18:45:36.814+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T18:45:37.310+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:45:37.353+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:45:53.571+0000] {processor.py:186} INFO - Started process (PID=442) to work on /opt/airflow/dags/dag_scheduler_v1.py
[2025-02-23T18:45:53.671+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v1.py for tasks to queue
""
[2025-02-23T18:45:54.773+0000] {processor.py:925} INFO - DAG(s) 'scheduler_v1' retrieved from /opt/airflow/dags/dag_scheduler_v1.py
[2025-02-23T18:45:55.702+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:45:55.697+0000] {dag.py:3239} INFO - Sync 1 DAGs
""
[2025-02-23T18:45:58.458+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:46:02.992+0000] {processor.py:186} INFO - Started process (PID=443) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:46:03.002+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:46:03.018+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:46:03.012+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:46:11.245+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:46:12.136+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:46:12.135+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:46:12.493+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:46:12.492+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:40:00+00:00, run_after=2025-02-23 18:50:00+00:00
[2025-02-23T18:46:12.795+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 9.696 seconds
sv'.
[2025-02-23T18:46:09.045+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db2.csv' enviado para 'datalake/bronze/db2/fornecedores_db2.csv'.
[2025-02-23T18:46:09.048+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.csv' baixado para 'temp_landzone_fornecedores_db3.csv'.
[2025-02-23T18:46:09.054+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db3.csv' enviado para 'datalake/bronze/db3/fornecedores_db3.csv'.
[2025-02-23T18:46:09.060+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.csv' baixado para 'temp_landzone_produtos_db1.csv'.
[2025-02-23T18:46:09.063+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db1.csv' enviado para 'datalake/bronze/db1/produtos_db1.csv'.
[2025-02-23T18:46:09.066+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.csv' baixado para 'temp_landzone_produtos_db2.csv'.
[2025-02-23T18:46:09.072+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db2.csv' enviado para 'datalake/bronze/db2/produtos_db2.csv'.
[2025-02-23T18:46:09.078+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.csv' baixado para 'temp_landzone_produtos_db3.csv'.
[2025-02-23T18:46:09.084+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db3.csv' enviado para 'datalake/bronze/db3/produtos_db3.csv'.
[2025-02-23T18:46:09.122+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.csv' baixado para 'temp_landzone_vendas_db1.csv'.
[2025-02-23T18:46:09.137+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db1.csv' enviado para 'datalake/bronze/db1/vendas_db1.csv'.
[2025-02-23T18:46:09.146+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.csv' baixado para 'temp_landzone_vendas_db2.csv'.
[2025-02-23T18:46:09.151+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db2.csv' enviado para 'datalake/bronze/db2/vendas_db2.csv'.
[2025-02-23T18:46:09.194+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.csv' baixado para 'temp_landzone_vendas_db3.csv'.
[2025-02-23T18:46:09.200+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db3.csv' enviado para 'datalake/bronze/db3/vendas_db3.csv'.
[2025-02-23T18:46:09.207+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.csv' baixado para 'temp_landzone_vendedores_db1.csv'.
[2025-02-23T18:46:09.217+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db1.csv' enviado para 'datalake/bronze/db1/vendedores_db1.csv'.
[2025-02-23T18:46:09.220+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.csv' baixado para 'temp_landzone_vendedores_db2.csv'.
[2025-02-23T18:46:09.226+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db2.csv' enviado para 'datalake/bronze/db2/vendedores_db2.csv'.
[2025-02-23T18:46:09.233+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.csv' baixado para 'temp_landzone_vendedores_db3.csv'.
[2025-02-23T18:46:09.242+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db3.csv' enviado para 'datalake/bronze/db3/vendedores_db3.csv'.
[2025-02-23T18:46:12.177+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:46:12.189+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:46:12.570+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:46:12.577+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=manual__2025-02-23T18:43:20.596565+00:00, execution_date=20250223T184320, start_date=20250223T184504, end_date=20250223T184612
[2025-02-23T18:46:13.392+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T18:46:13.586+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:46:31.604+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T18:46:31.612+0000] {subprocess.py:106} INFO - O arquivo 'temp_silver_fornecedores_db1.csv' no existe, criando um novo.
[2025-02-23T18:46:31.622+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T18:46:31.629+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 91, in <module>"
[2025-02-23T18:46:31.634+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T18:46:31.638+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 75, in upsert_to_silver"
"[2025-02-23T18:46:31.705+0000] {subprocess.py:106} INFO -     silver_df = silver_df.set_index(""id"").combine_first(df.set_index(""id"")).reset_index()"
[2025-02-23T18:46:31.719+0000] {subprocess.py:106} INFO -                 ^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:46:31.722+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/core/frame.py"", line 5870, in set_index"
"[2025-02-23T18:46:31.832+0000] {subprocess.py:106} INFO -     raise KeyError(f""None of {missing} are in the columns"")"
"[2025-02-23T18:46:31.835+0000] {subprocess.py:106} INFO - KeyError: ""None of ['id'] are in the columns"""
[2025-02-23T18:46:32.950+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T18:46:32.953+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T18:46:33.079+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:46:33.389+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T18:30:00+00:00, execution_date=20250223T183000, start_date=20250223T184558, end_date=20250223T184633
[2025-02-23T18:46:33.998+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:46:34.002+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 52 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 1089)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:46:34.152+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T18:46:34.415+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:46:44.072+0000] {processor.py:186} INFO - Started process (PID=445) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:46:44.112+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:46:44.125+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:46:44.122+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:46:47.205+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:46:47.432+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:46:47.431+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:46:47.681+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:46:47.680+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:40:00+00:00, run_after=2025-02-23 18:50:00+00:00
[2025-02-23T18:46:47.869+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T18:46:47.875+0000] {subprocess.py:106} INFO - O arquivo 'temp_silver_fornecedores_db1.csv' no existe, criando um novo.
[2025-02-23T18:46:47.886+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T18:46:47.891+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 91, in <module>"
[2025-02-23T18:46:47.896+0000] {subprocess.py:106} INFO -     upsert_to_silver(file_path)
"[2025-02-23T18:46:47.908+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/2_uploadSilver/2_upsertSilver.py"", line 75, in upsert_to_silver"
"[2025-02-23T18:46:47.930+0000] {subprocess.py:106} INFO -     silver_df = silver_df.set_index(""id"").combine_first(df.set_index(""id"")).reset_index()"
[2025-02-23T18:46:47.935+0000] {subprocess.py:106} INFO -                 ^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T18:46:47.939+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/pandas/core/frame.py"", line 5870, in set_index"
"[2025-02-23T18:46:47.947+0000] {subprocess.py:106} INFO -     raise KeyError(f""None of {missing} are in the columns"")"
"[2025-02-23T18:46:47.951+0000] {subprocess.py:106} INFO - KeyError: ""None of ['id'] are in the columns"""
[2025-02-23T18:46:49.297+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T18:46:49.302+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T18:46:50.201+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:46:50.367+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_silver, run_id=manual__2025-02-23T18:43:20.596565+00:00, execution_date=20250223T184320, start_date=20250223T184626, end_date=20250223T184650
[2025-02-23T18:46:50.798+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:46:50.801+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 53 for task process_silver (Bash command failed. The command returned a non-zero exit code 1.; 1099)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T18:46:50.899+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T18:46:51.019+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:46:51.039+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:47:18.461+0000] {processor.py:186} INFO - Started process (PID=447) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:47:18.468+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:47:18.483+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:47:18.481+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:47:20.565+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:47:20.861+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:47:20.860+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:47:21.217+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:47:21.216+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:40:00+00:00, run_after=2025-02-23 18:50:00+00:00
[2025-02-23T18:47:21.480+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.101 seconds
[2025-02-23T18:47:52.648+0000] {processor.py:186} INFO - Started process (PID=449) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:47:52.657+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:47:52.678+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:47:52.672+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:47:59.306+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:47:59.521+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:47:59.520+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:47:59.756+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:47:59.755+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:40:00+00:00, run_after=2025-02-23 18:50:00+00:00
[2025-02-23T18:47:59.912+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 7.565 seconds
[2025-02-23T18:48:30.235+0000] {processor.py:186} INFO - Started process (PID=451) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:48:30.239+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:48:30.248+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:48:30.246+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:48:31.623+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:48:32.539+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:48:32.537+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:48:33.073+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:48:33.072+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:40:00+00:00, run_after=2025-02-23 18:50:00+00:00
[2025-02-23T18:48:33.249+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.041 seconds
[2025-02-23T18:49:04.044+0000] {processor.py:186} INFO - Started process (PID=453) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:49:04.047+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:49:04.054+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:49:04.051+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:49:04.941+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:49:05.070+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:49:05.069+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:49:05.218+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:49:05.217+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:40:00+00:00, run_after=2025-02-23 18:50:00+00:00
[2025-02-23T18:49:05.304+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.336 seconds
[2025-02-23T18:49:35.463+0000] {processor.py:186} INFO - Started process (PID=455) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:49:35.496+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:49:35.500+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:49:35.498+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:49:36.672+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:49:37.031+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:49:37.030+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:49:38.597+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:49:38.592+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:40:00+00:00, run_after=2025-02-23 18:50:00+00:00
[2025-02-23T18:49:39.102+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.630 seconds
[2025-02-23T18:50:10.963+0000] {processor.py:186} INFO - Started process (PID=457) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:50:10.988+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:50:11.009+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:50:11.005+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:50:12.823+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:50:13.070+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:50:13.070+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:50:13.487+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:50:13.486+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:50:13.632+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.010 seconds
[2025-02-23T18:50:32.182+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:50:32.369+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T18:40:00+00:00 [queued]>
[2025-02-23T18:50:32.399+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T18:40:00+00:00 [queued]>
[2025-02-23T18:50:32.401+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:50:32.439+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 18:40:00+00:00
[2025-02-23T18:50:32.454+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1170) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:50:32.459+0000] {standard_task_runner.py:72} INFO - Started process 1171 to run task
[2025-02-23T18:50:32.460+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T18:40:00+00:00', '--job-id', '55', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpab_xcf1g']
[2025-02-23T18:50:32.464+0000] {standard_task_runner.py:105} INFO - Job 55: Subtask process_bronze
[2025-02-23T18:50:32.632+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T18:40:00+00:00 [running]> on host b06401153325
[2025-02-23T18:50:32.925+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:40:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:40:00+00:00'
[2025-02-23T18:50:32.928+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T18:50:32.988+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:50:32.991+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T18:50:33.017+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:50:33.030+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T18:50:44.195+0000] {processor.py:186} INFO - Started process (PID=460) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:50:44.246+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:50:44.256+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:50:44.254+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:50:45.131+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:50:45.116+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00
[2025-02-23T18:50:45.281+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 1.888 seconds
[2025-02-23T18:50:50.097+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:50:50.476+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:50:50.475+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:50:50.905+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:50:50.904+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:50:51.217+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 7.239 seconds
[2025-02-23T18:50:56.474+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.csv' baixado para 'temp_landzone_fornecedores_db1.csv'.
[2025-02-23T18:50:56.608+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db1.csv' enviado para 'datalake/bronze/db1/fornecedores_db1.csv'.
[2025-02-23T18:50:56.612+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.csv' baixado para 'temp_landzone_fornecedores_db2.csv'.
[2025-02-23T18:50:56.615+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db2.csv' enviado para 'datalake/bronze/db2/fornecedores_db2.csv'.
[2025-02-23T18:50:56.622+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.csv' baixado para 'temp_landzone_fornecedores_db3.csv'.
[2025-02-23T18:50:56.625+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db3.csv' enviado para 'datalake/bronze/db3/fornecedores_db3.csv'.
[2025-02-23T18:50:56.633+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.csv' baixado para 'temp_landzone_produtos_db1.csv'.
[2025-02-23T18:50:56.638+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db1.csv' enviado para 'datalake/bronze/db1/produtos_db1.csv'.
[2025-02-23T18:50:56.642+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.csv' baixado para 'temp_landzone_produtos_db2.csv'.
[2025-02-23T18:50:56.645+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db2.csv' enviado para 'datalake/bronze/db2/produtos_db2.csv'.
[2025-02-23T18:50:56.648+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.csv' baixado para 'temp_landzone_produtos_db3.csv'.
[2025-02-23T18:50:56.653+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db3.csv' enviado para 'datalake/bronze/db3/produtos_db3.csv'.
[2025-02-23T18:50:56.780+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.csv' baixado para 'temp_landzone_vendas_db1.csv'.
[2025-02-23T18:50:56.787+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db1.csv' enviado para 'datalake/bronze/db1/vendas_db1.csv'.
[2025-02-23T18:50:56.789+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.csv' baixado para 'temp_landzone_vendas_db2.csv'.
[2025-02-23T18:50:56.805+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db2.csv' enviado para 'datalake/bronze/db2/vendas_db2.csv'.
[2025-02-23T18:50:56.822+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.csv' baixado para 'temp_landzone_vendas_db3.csv'.
[2025-02-23T18:50:56.834+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db3.csv' enviado para 'datalake/bronze/db3/vendas_db3.csv'.
[2025-02-23T18:50:56.841+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.csv' baixado para 'temp_landzone_vendedores_db1.csv'.
[2025-02-23T18:50:56.848+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db1.csv' enviado para 'datalake/bronze/db1/vendedores_db1.csv'.
[2025-02-23T18:50:56.856+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.csv' baixado para 'temp_landzone_vendedores_db2.csv'.
[2025-02-23T18:50:56.866+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db2.csv' enviado para 'datalake/bronze/db2/vendedores_db2.csv'.
[2025-02-23T18:50:56.870+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.csv' baixado para 'temp_landzone_vendedores_db3.csv'.
[2025-02-23T18:50:56.873+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db3.csv' enviado para 'datalake/bronze/db3/vendedores_db3.csv'.
[2025-02-23T18:50:57.593+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:50:57.597+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:50:57.752+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:50:57.755+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T18:40:00+00:00, execution_date=20250223T184000, start_date=20250223T185032, end_date=20250223T185057
[2025-02-23T18:50:57.954+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T18:50:58.083+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:50:58.090+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:51:02.270+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:51:02.452+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:40:00+00:00 [queued]>
[2025-02-23T18:51:02.510+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:40:00+00:00 [queued]>
[2025-02-23T18:51:02.513+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:51:02.639+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 18:40:00+00:00
[2025-02-23T18:51:02.672+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1180) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:51:02.680+0000] {standard_task_runner.py:72} INFO - Started process 1181 to run task
[2025-02-23T18:51:02.683+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T18:40:00+00:00', '--job-id', '56', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmploo6bhdq']
[2025-02-23T18:51:02.688+0000] {standard_task_runner.py:105} INFO - Job 56: Subtask process_silver
[2025-02-23T18:51:16.323+0000] {processor.py:925} INFO - DAG(s) 'scheduler_v1' retrieved from /opt/airflow/dags/dag_scheduler_v1.py
[2025-02-23T18:51:16.579+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:51:16.578+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:51:17.299+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:51:17.297+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00
[2025-02-23T18:51:17.602+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 1.496 seconds
[2025-02-23T18:51:19.207+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T18:51:19.210+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:02.859+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:40:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:51:19.213+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/fornecedores_db1.csv
[2025-02-23T18:51:19.216+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/produtos_db1.csv' baixado para 'temp_bronze_produtos_db1.csv'.
[2025-02-23T18:51:19.220+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:02.859+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:40:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:51:19.223+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/produtos_db1.csv
[2025-02-23T18:51:19.226+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendas_db1.csv' baixado para 'temp_bronze_vendas_db1.csv'.
[2025-02-23T18:51:19.229+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:16.169+0000] {processor.py:186} INFO - Started process (PID=461) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:51:19.232+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendas_db1.csv
[2025-02-23T18:51:19.234+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendedores_db1.csv' baixado para 'temp_bronze_vendedores_db1.csv'.
[2025-02-23T18:51:19.236+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:16.169+0000] {processor.py:186} INFO - Started process (PID=461) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:51:19.239+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendedores_db1.csv
[2025-02-23T18:51:19.241+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/fornecedores_db2.csv' baixado para 'temp_bronze_fornecedores_db2.csv'.
[2025-02-23T18:51:19.244+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:16.169+0000] {processor.py:186} INFO - Started process (PID=461) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:51:19.247+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/fornecedores_db2.csv
[2025-02-23T18:51:19.249+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/produtos_db2.csv' baixado para 'temp_bronze_produtos_db2.csv'.
[2025-02-23T18:51:19.252+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:16.169+0000] {processor.py:186} INFO - Started process (PID=461) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:51:19.255+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/produtos_db2.csv
[2025-02-23T18:51:19.258+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendas_db2.csv' baixado para 'temp_bronze_vendas_db2.csv'.
[2025-02-23T18:51:19.262+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:16.169+0000] {processor.py:186} INFO - Started process (PID=461) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:51:19.265+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendas_db2.csv
[2025-02-23T18:51:19.268+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendedores_db2.csv' baixado para 'temp_bronze_vendedores_db2.csv'.
[2025-02-23T18:51:19.271+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:16.169+0000] {processor.py:186} INFO - Started process (PID=461) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:51:19.275+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendedores_db2.csv
[2025-02-23T18:51:19.278+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/fornecedores_db3.csv' baixado para 'temp_bronze_fornecedores_db3.csv'.
[2025-02-23T18:51:19.282+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:16.169+0000] {processor.py:186} INFO - Started process (PID=461) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:51:19.285+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/fornecedores_db3.csv
[2025-02-23T18:51:19.290+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/produtos_db3.csv' baixado para 'temp_bronze_produtos_db3.csv'.
[2025-02-23T18:51:19.293+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:16.169+0000] {processor.py:186} INFO - Started process (PID=461) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:51:19.297+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/produtos_db3.csv
[2025-02-23T18:51:19.301+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendas_db3.csv' baixado para 'temp_bronze_vendas_db3.csv'.
[2025-02-23T18:51:19.304+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:16.169+0000] {processor.py:186} INFO - Started process (PID=461) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:51:19.308+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendas_db3.csv
[2025-02-23T18:51:19.312+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendedores_db3.csv' baixado para 'temp_bronze_vendedores_db3.csv'.
[2025-02-23T18:51:19.314+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:51:16.169+0000] {processor.py:186} INFO - Started process (PID=461) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:51:19.317+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendedores_db3.csv
[2025-02-23T18:51:19.494+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:51:19.497+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:51:19.595+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:51:19.598+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T18:40:00+00:00, execution_date=20250223T184000, start_date=20250223T185102, end_date=20250223T185119
[2025-02-23T18:51:19.777+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T18:51:19.886+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:51:19.893+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:51:21.685+0000] {processor.py:186} INFO - Started process (PID=462) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:51:21.770+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:51:21.855+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:51:21.853+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:51:23.347+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:51:23.529+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:51:23.527+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:51:24.062+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:51:24.061+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:51:24.424+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.869 seconds
[2025-02-23T18:51:27.373+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:51:27.523+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T18:40:00+00:00 [queued]>
[2025-02-23T18:51:27.615+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T18:40:00+00:00 [queued]>
[2025-02-23T18:51:27.622+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:51:28.028+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_analytics> on 2025-02-23 18:40:00+00:00
[2025-02-23T18:51:28.112+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1226) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:51:28.142+0000] {standard_task_runner.py:72} INFO - Started process 1227 to run task
[2025-02-23T18:51:28.146+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_analytics', 'scheduled__2025-02-23T18:40:00+00:00', '--job-id', '57', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpd6snlik3']
[2025-02-23T18:51:28.163+0000] {standard_task_runner.py:105} INFO - Job 57: Subtask process_analytics
[2025-02-23T18:51:53.346+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:51:53.720+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T18:40:00+00:00 [queued]>
[2025-02-23T18:51:53.801+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T18:40:00+00:00 [queued]>
[2025-02-23T18:51:53.823+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:51:54.138+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_gold> on 2025-02-23 18:40:00+00:00
[2025-02-23T18:51:54.208+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_gold', 'scheduled__2025-02-23T18:40:00+00:00', '--job-id', '58', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpmqxki8hb']
[2025-02-23T18:51:54.298+0000] {standard_task_runner.py:105} INFO - Job 58: Subtask process_gold
[2025-02-23T18:51:54.313+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1242) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:51:54.343+0000] {standard_task_runner.py:72} INFO - Started process 1243 to run task
[2025-02-23T18:51:54.779+0000] {processor.py:186} INFO - Started process (PID=464) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:51:54.791+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:51:54.802+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:51:54.799+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:51:58.201+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:51:58.877+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:51:58.876+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:51:59.678+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:51:59.677+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:52:02.577+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 7.853 seconds
[2025-02-23T18:52:33.845+0000] {processor.py:186} INFO - Started process (PID=466) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:52:33.847+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:52:33.850+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:52:33.849+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:52:34.740+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:52:34.839+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:52:34.838+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:52:34.909+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:52:34.909+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:52:34.975+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.142 seconds
[2025-02-23T18:53:05.471+0000] {processor.py:186} INFO - Started process (PID=468) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:53:05.577+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:53:05.581+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:53:05.580+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:53:06.572+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:53:06.663+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:53:06.662+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:53:06.793+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:53:06.793+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:53:07.014+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.560 seconds
[2025-02-23T18:53:38.079+0000] {processor.py:186} INFO - Started process (PID=470) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:53:38.180+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:53:38.184+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:53:38.183+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:53:38.604+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:53:38.664+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:53:38.663+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:53:38.713+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:53:38.712+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:53:38.790+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.735 seconds
[2025-02-23T18:54:09.513+0000] {processor.py:186} INFO - Started process (PID=472) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:54:09.515+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:54:09.518+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:54:09.517+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:54:09.890+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:54:09.949+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:54:09.948+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:54:10.012+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:54:10.012+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:54:10.055+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.555 seconds
[2025-02-23T18:54:40.433+0000] {processor.py:186} INFO - Started process (PID=474) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:54:40.536+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:54:40.540+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:54:40.539+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:54:41.472+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:54:41.535+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:54:41.534+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:54:41.583+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:54:41.583+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:54:41.621+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.202 seconds
[2025-02-23T18:55:11.826+0000] {processor.py:186} INFO - Started process (PID=476) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:55:12.003+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:55:12.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:55:12.007+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:55:12.924+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:55:12.977+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:55:12.976+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:55:13.034+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:55:13.033+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:55:13.090+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.286 seconds
[2025-02-23T18:55:44.145+0000] {processor.py:186} INFO - Started process (PID=478) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:55:44.496+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:55:44.502+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:55:44.501+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:55:46.225+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:55:46.307+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:55:46.306+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:55:46.382+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:55:46.382+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:55:46.443+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.581 seconds
[2025-02-23T18:56:16.970+0000] {processor.py:186} INFO - Started process (PID=480) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:56:16.975+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:56:16.983+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:56:16.980+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:56:17.992+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:56:18.078+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:56:18.078+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:56:18.163+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:56:18.163+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:56:18.239+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.330 seconds
[2025-02-23T18:56:38.380+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:56:38.692+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [queued]>
[2025-02-23T18:56:38.850+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [queued]>
[2025-02-23T18:56:38.853+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T18:56:39.001+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 18:30:00+00:00
[2025-02-23T18:56:39.090+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T18:30:00+00:00', '--job-id', '59', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpky3kg8in']
[2025-02-23T18:56:39.105+0000] {standard_task_runner.py:105} INFO - Job 59: Subtask process_silver
[2025-02-23T18:56:39.081+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1264) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:56:39.154+0000] {standard_task_runner.py:72} INFO - Started process 1265 to run task
[2025-02-23T18:56:48.578+0000] {processor.py:186} INFO - Started process (PID=482) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:56:48.590+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:56:48.609+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:56:48.606+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:56:56.217+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:56:56.393+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:56:56.392+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:56:56.598+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:56:56.597+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:56:56.724+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 8.248 seconds
[2025-02-23T18:56:58.348+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T18:56:58.352+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.368+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/fornecedores_db1.csv
[2025-02-23T18:56:58.371+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/produtos_db1.csv' baixado para 'temp_bronze_produtos_db1.csv'.
[2025-02-23T18:56:58.373+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.378+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/produtos_db1.csv
[2025-02-23T18:56:58.380+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendas_db1.csv' baixado para 'temp_bronze_vendas_db1.csv'.
[2025-02-23T18:56:58.385+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.388+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendas_db1.csv
[2025-02-23T18:56:58.390+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendedores_db1.csv' baixado para 'temp_bronze_vendedores_db1.csv'.
[2025-02-23T18:56:58.392+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.395+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendedores_db1.csv
[2025-02-23T18:56:58.398+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/fornecedores_db2.csv' baixado para 'temp_bronze_fornecedores_db2.csv'.
[2025-02-23T18:56:58.401+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.404+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/fornecedores_db2.csv
[2025-02-23T18:56:58.406+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/produtos_db2.csv' baixado para 'temp_bronze_produtos_db2.csv'.
[2025-02-23T18:56:58.410+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.412+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/produtos_db2.csv
[2025-02-23T18:56:58.414+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendas_db2.csv' baixado para 'temp_bronze_vendas_db2.csv'.
[2025-02-23T18:56:58.417+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.420+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendas_db2.csv
[2025-02-23T18:56:58.423+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendedores_db2.csv' baixado para 'temp_bronze_vendedores_db2.csv'.
[2025-02-23T18:56:58.429+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.433+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendedores_db2.csv
[2025-02-23T18:56:58.436+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/fornecedores_db3.csv' baixado para 'temp_bronze_fornecedores_db3.csv'.
[2025-02-23T18:56:58.440+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.448+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/fornecedores_db3.csv
[2025-02-23T18:56:58.505+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/produtos_db3.csv' baixado para 'temp_bronze_produtos_db3.csv'.
[2025-02-23T18:56:58.508+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.511+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/produtos_db3.csv
[2025-02-23T18:56:58.514+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendas_db3.csv' baixado para 'temp_bronze_vendas_db3.csv'.
[2025-02-23T18:56:58.518+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.527+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendas_db3.csv
[2025-02-23T18:56:58.531+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendedores_db3.csv' baixado para 'temp_bronze_vendedores_db3.csv'.
[2025-02-23T18:56:58.533+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:39.888+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325']
[2025-02-23T18:56:58.537+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendedores_db3.csv
[2025-02-23T18:56:58.946+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:56:58.950+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:56:59.090+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:56:59.094+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T18:30:00+00:00, execution_date=20250223T183000, start_date=20250223T185638, end_date=20250223T185659
[2025-02-23T18:56:59.402+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T18:57:01.964+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:57:02.037+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver manual__2025-02-23T18:43:20.596565+00:00 [queued]>
[2025-02-23T18:57:02.069+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver manual__2025-02-23T18:43:20.596565+00:00 [queued]>
[2025-02-23T18:57:02.071+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T18:57:02.120+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 18:43:20.596565+00:00
[2025-02-23T18:57:02.140+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1289) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:57:02.148+0000] {standard_task_runner.py:72} INFO - Started process 1312 to run task
[2025-02-23T18:57:02.147+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'manual__2025-02-23T18:43:20.596565+00:00', '--job-id', '60', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmphbdb90s4']
[2025-02-23T18:57:02.152+0000] {standard_task_runner.py:105} INFO - Job 60: Subtask process_silver
[2025-02-23T18:57:02.379+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver manual__2025-02-23T18:43:20.596565+00:00 [running]> on host b06401153325
[2025-02-23T18:57:02.818+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:43:20.596565+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-02-23T18:43:20.596565+00:00'
[2025-02-23T18:57:02.823+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T18:57:02.909+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:57:02.914+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T18:57:02.964+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:57:02.989+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T18:57:03.907+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:57:04.046+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T18:30:00+00:00 [queued]>
[2025-02-23T18:57:04.092+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T18:30:00+00:00 [queued]>
[2025-02-23T18:57:04.095+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:57:04.170+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_analytics> on 2025-02-23 18:30:00+00:00
[2025-02-23T18:57:04.191+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1311) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:57:04.200+0000] {standard_task_runner.py:72} INFO - Started process 1320 to run task
[2025-02-23T18:57:04.203+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_analytics', 'scheduled__2025-02-23T18:30:00+00:00', '--job-id', '61', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpga5jszzj']
[2025-02-23T18:57:04.210+0000] {standard_task_runner.py:105} INFO - Job 61: Subtask process_analytics
[2025-02-23T18:57:04.406+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T18:30:00+00:00 [running]> on host b06401153325
[2025-02-23T18:57:04.844+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_analytics' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:30:00+00:00'
[2025-02-23T18:57:05.069+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T18:57:05.173+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T18:57:05.178+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_3.sh']
[2025-02-23T18:57:05.225+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T18:57:05.454+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T18:57:15.994+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T18:57:16.030+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.060+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/fornecedores_db1.csv
[2025-02-23T18:57:16.065+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/produtos_db1.csv' baixado para 'temp_bronze_produtos_db1.csv'.
[2025-02-23T18:57:16.068+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.071+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/produtos_db1.csv
[2025-02-23T18:57:16.075+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendas_db1.csv' baixado para 'temp_bronze_vendas_db1.csv'.
[2025-02-23T18:57:16.078+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.082+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendas_db1.csv
[2025-02-23T18:57:16.086+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendedores_db1.csv' baixado para 'temp_bronze_vendedores_db1.csv'.
[2025-02-23T18:57:16.089+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.092+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendedores_db1.csv
[2025-02-23T18:57:16.095+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/fornecedores_db2.csv' baixado para 'temp_bronze_fornecedores_db2.csv'.
[2025-02-23T18:57:16.098+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.102+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/fornecedores_db2.csv
[2025-02-23T18:57:16.112+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/produtos_db2.csv' baixado para 'temp_bronze_produtos_db2.csv'.
[2025-02-23T18:57:16.118+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.123+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/produtos_db2.csv
[2025-02-23T18:57:16.126+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendas_db2.csv' baixado para 'temp_bronze_vendas_db2.csv'.
[2025-02-23T18:57:16.131+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.137+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendas_db2.csv
[2025-02-23T18:57:16.141+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendedores_db2.csv' baixado para 'temp_bronze_vendedores_db2.csv'.
[2025-02-23T18:57:16.145+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.148+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendedores_db2.csv
[2025-02-23T18:57:16.152+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/fornecedores_db3.csv' baixado para 'temp_bronze_fornecedores_db3.csv'.
[2025-02-23T18:57:16.158+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.161+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/fornecedores_db3.csv
[2025-02-23T18:57:16.166+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/produtos_db3.csv' baixado para 'temp_bronze_produtos_db3.csv'.
[2025-02-23T18:57:16.170+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.174+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/produtos_db3.csv
[2025-02-23T18:57:16.179+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendas_db3.csv' baixado para 'temp_bronze_vendas_db3.csv'.
[2025-02-23T18:57:16.182+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.186+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendas_db3.csv
[2025-02-23T18:57:16.190+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendedores_db3.csv' baixado para 'temp_bronze_vendedores_db3.csv'.
[2025-02-23T18:57:16.194+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T18:56:59.510+0000] {processor.py:186} INFO - Started process (PID=483) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T18:57:16.198+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendedores_db3.csv
[2025-02-23T18:57:17.319+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:57:17.329+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:57:19.047+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:57:19.057+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_silver, run_id=manual__2025-02-23T18:43:20.596565+00:00, execution_date=20250223T184320, start_date=20250223T185702, end_date=20250223T185719
[2025-02-23T18:57:19.528+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T18:57:19.945+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:57:19.954+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:57:20.748+0000] {subprocess.py:106} INFO -  Bucket 'datalake' j existe.
[2025-02-23T18:57:20.754+0000] {subprocess.py:106} INFO -  'silveranalitics/' inicializado no bucket 'datalake'.
[2025-02-23T18:57:20.758+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db1.csv' carregado de silver/db1.
[2025-02-23T18:57:20.762+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db2.csv' carregado de silver/db2.
[2025-02-23T18:57:20.766+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db3.csv' carregado de silver/db3.
[2025-02-23T18:57:20.773+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T18:57:20.790+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db1.csv' carregado de silver/db1.
[2025-02-23T18:57:20.793+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db2.csv' carregado de silver/db2.
[2025-02-23T18:57:20.800+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db3.csv' carregado de silver/db3.
[2025-02-23T18:57:20.805+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T18:57:20.815+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db1.csv' carregado de silver/db1.
[2025-02-23T18:57:20.819+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db2.csv' carregado de silver/db2.
[2025-02-23T18:57:20.824+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db3.csv' carregado de silver/db3.
[2025-02-23T18:57:20.828+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T18:57:20.831+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db1.csv' carregado de silver/db1.
[2025-02-23T18:57:20.835+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db2.csv' carregado de silver/db2.
[2025-02-23T18:57:20.838+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db3.csv' carregado de silver/db3.
[2025-02-23T18:57:20.842+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T18:57:21.911+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T18:57:21.915+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T18:57:22.083+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T18:57:22.087+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_analytics, run_id=scheduled__2025-02-23T18:30:00+00:00, execution_date=20250223T183000, start_date=20250223T185704, end_date=20250223T185722
[2025-02-23T18:57:22.234+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T18:57:22.305+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T18:57:22.314+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T18:57:28.309+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:57:28.530+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics manual__2025-02-23T18:43:20.596565+00:00 [queued]>
[2025-02-23T18:57:28.612+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps t[2025-02-23T18:57:30.312+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:57:30.485+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:57:30.485+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:57:30.779+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:57:30.778+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:57:30.881+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.988 seconds
 = os.fork()
[2025-02-23T18:57:28.749+0000] {standard_task_runner.py:72} INFO - Started process 1380 to run task
[2025-02-23T18:57:28.750+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_analytics', 'manual__2025-02-23T18:43:20.596565+00:00', '--job-id', '62', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmppc7lyp5n']
[2025-02-23T18:57:28.758+0000] {standard_task_runner.py:105} INFO - Job 62: Subtask process_analytics
[2025-02-23T18:57:31.546+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:57:31.545+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00
[2025-02-23T18:57:31.662+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 0.634 seconds
[2025-02-23T18:57:31.748+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:57:31.906+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T18:30:00+00:00 [queued]>
[2025-02-23T18:57:32.000+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T18:30:00+00:00 [queued]>
[2025-02-23T18:57:32.004+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:57:32.119+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_gold> on 2025-02-23 18:30:00+00:00
[2025-02-23T18:57:32.446+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1379) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:57:32.523+0000] {standard_task_runner.py:72} INFO - Started process 1388 to run task
[2025-02-23T18:57:32.528+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_gold', 'scheduled__2025-02-23T18:30:00+00:00', '--job-id', '63', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpm3h1jvbt']
[2025-02-23T18:57:32.549+0000] {standard_task_runner.py:105} INFO - Job 63: Subtask process_gold
[2025-02-23T18:57:54.180+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T18:57:54.278+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold manual__2025-02-23T18:43:20.596565+00:00 [queued]>
[2025-02-23T18:57:54.323+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold manual__2025-02-23T18:43:20.596565+00:00 [queued]>
[2025-02-23T18:57:54.325+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T18:57:54.375+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_gold> on 2025-02-23 18:43:20.596565+00:00
[2025-02-23T18:57:54.397+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1413) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T18:57:54.404+0000] {standard_task_runner.py:72} INFO - Started process 1423 to run task
[2025-02-23T18:57:54.405+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_gold', 'manual__2025-02-23T18:43:20.596565+00:00', '--job-id', '64', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpevbs_69n']
[2025-02-23T18:57:54.412+0000] {standard_task_runner.py:105} INFO - Job 64: Subtask process_gold
[2025-02-23T18:58:01.105+0000] {processor.py:186} INFO - Started process (PID=486) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:58:01.135+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:58:01.144+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:58:01.142+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:58:04.400+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:58:04.629+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:58:04.627+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:58:05.051+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:58:05.050+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:58:05.208+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.185 seconds
[2025-02-23T18:58:35.793+0000] {processor.py:186} INFO - Started process (PID=489) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:58:35.940+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:58:36.038+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:58:36.033+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:58:37.470+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:58:37.576+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:58:37.575+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:58:37.667+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:58:37.665+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:58:37.738+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.182 seconds
[2025-02-23T18:59:08.211+0000] {processor.py:186} INFO - Started process (PID=491) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:59:08.217+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:59:08.232+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:59:08.230+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:59:12.512+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:59:12.847+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:59:12.846+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:59:13.017+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:59:13.016+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:59:13.210+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.051 seconds
[2025-02-23T18:59:43.724+0000] {processor.py:186} INFO - Started process (PID=493) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:59:44.074+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T18:59:44.084+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:59:44.083+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:59:44.746+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T18:59:44.802+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:59:44.802+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T18:59:44.880+0000] {logging_mixin.py:190} INFO - [2025-02-23T18:59:44.879+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 18:50:00+00:00, run_after=2025-02-23 19:00:00+00:00
[2025-02-23T18:59:44.939+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.234 seconds
[2025-02-23T19:00:04.169+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T19:00:04.325+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T18:50:00+00:00 [queued]>
[2025-02-23T19:00:04.355+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T18:50:00+00:00 [queued]>
[2025-02-23T19:00:04.356+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T19:00:04.411+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 18:50:00+00:00
[2025-02-23T19:00:04.433+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1444) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T19:00:04.441+0000] {standard_task_runner.py:72} INFO - Started process 1445 to run task
[2025-02-23T19:00:04.441+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T18:50:00+00:00', '--job-id', '65', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpdj5o0fzw']
[2025-02-23T19:00:04.450+0000] {standard_task_runner.py:105} INFO - Job 65: Subtask process_landzone
[2025-02-23T19:00:04.785+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T18:50:00+00:00 [running]> on host b06401153325
[2025-02-23T19:00:05.338+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:50:00+00:00'
[2025-02-23T19:00:05.351+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T19:00:05.548+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T19:00:05.680+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T19:00:05.736+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T19:00:05.763+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T19:00:15.337+0000] {processor.py:186} INFO - Started process (PID=495) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:00:15.426+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:00:15.433+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:00:15.431+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:00:16.607+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:00:16.778+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:00:16.778+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:00:16.951+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:00:16.950+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:00:17.120+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.831 seconds
[2025-02-23T19:00:24.619+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T19:00:24.754+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T18:50:00+00:00 [queued]>
[2025-02-23T19:00:24.804+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T18:50:00+00:00 [queued]>
[2025-02-23T19:00:24.807+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T19:00:24.896+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 18:50:00+00:00
[2025-02-23T19:00:24.915+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1490) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T19:00:24.923+0000] {standard_task_runner.py:72} INFO - Started process 1491 to run task
[2025-02-23T19:00:24.925+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T18:50:00+00:00', '--job-id', '66', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpdo2ybgp1']
[2025-02-23T19:00:24.930+0000] {standard_task_runner.py:105} INFO - Job 66: Subtask process_bronze
[2025-02-23T19:00:25.099+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T18:50:00+00:00 [running]> on host b06401153325
[2025-02-23T19:00:25.408+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:50:00+00:00'
[2025-02-23T19:00:25.412+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T19:00:25.511+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T19:00:25.518+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T19:00:25.574+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T19:00:25.605+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T19:00:43.238+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:00:43.237+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:00:43.382+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:00:43.381+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00
[2025-02-23T19:00:43.481+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 0.811 seconds
[2025-02-23T19:00:44.051+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.csv' baixado para 'temp_landzone_fornecedores_db1.csv'.
[2025-02-23T19:00:44.063+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db1.csv' enviado para 'datalake/bronze/db1/fornecedores_db1.csv'.
[2025-02-23T19:00:44.070+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.csv' baixado para 'temp_landzone_fornecedores_db2.csv'.
[2025-02-23T19:00:44.078+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db2.csv' enviado para 'datalake/bronze/db2/fornecedores_db2.csv'.
[2025-02-23T19:00:44.082+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.csv' baixado para 'temp_landzone_fornecedores_db3.csv'.
[2025-02-23T19:00:44.085+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db3.csv' enviado para 'datalake/bronze/db3/fornecedores_db3.csv'.
[2025-02-23T19:00:44.089+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.csv' baixado para 'temp_landzone_produtos_db1.csv'.
[2025-02-23T19:00:44.091+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db1.csv' enviado para 'datalake/bronze/db1/produtos_db1.csv'.
[2025-02-23T19:00:44.094+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.csv' baixado para 'temp_landzone_produtos_db2.csv'.
[2025-02-23T19:00:44.098+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db2.csv' enviado para 'datalake/bronze/db2/produtos_db2.csv'.
[2025-02-23T19:00:44.100+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.csv' baixado para 'temp_landzone_produtos_db3.csv'.
[2025-02-23T19:00:44.103+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db3.csv' enviado para 'datalake/bronze/db3/produtos_db3.csv'.
[2025-02-23T19:00:44.106+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.csv' baixado para 'temp_landzone_vendas_db1.csv'.
[2025-02-23T19:00:44.109+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db1.csv' enviado para 'datalake/bronze/db1/vendas_db1.csv'.
[2025-02-23T19:00:44.112+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.csv' baixado para 'temp_landzone_vendas_db2.csv'.
[2025-02-23T19:00:44.114+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db2.csv' enviado para 'datalake/bronze/db2/vendas_db2.csv'.
[2025-02-23T19:00:44.117+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.csv' baixado para 'temp_landzone_vendas_db3.csv'.
[2025-02-23T19:00:44.119+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db3.csv' enviado para 'datalake/bronze/db3/vendas_db3.csv'.
[2025-02-23T19:00:44.121+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.csv' baixado para 'temp_landzone_vendedores_db1.csv'.
[2025-02-23T19:00:44.126+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db1.csv' enviado para 'datalake/bronze/db1/vendedores_db1.csv'.
[2025-02-23T19:00:44.128+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.csv' baixado para 'temp_landzone_vendedores_db2.csv'.
[2025-02-23T19:00:44.131+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db2.csv' enviado para 'datalake/bronze/db2/vendedores_db2.csv'.
[2025-02-23T19:00:44.133+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.csv' baixado para 'temp_landzone_vendedores_db3.csv'.
[2025-02-23T19:00:44.136+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db3.csv' enviado para 'datalake/bronze/db3/vendedores_db3.csv'.
[2025-02-23T19:00:44.671+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T19:00:44.676+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T19:00:45.376+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T19:00:45.397+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T18:50:00+00:00, execution_date=20250223T185000, start_date=20250223T190024, end_date=20250223T190045
[2025-02-23T19:00:45.810+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T19:00:46.011+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T19:00:46.029+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T19:00:47.528+0000] {processor.py:186} INFO - Started process (PID=497) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:00:47.535+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:00:47.547+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:00:47.545+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:00:48.384+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:00:48.469+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:00:48.468+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:00:48.546+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:00:48.546+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:00:48.613+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.143 seconds
[2025-02-23T19:00:49.552+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T19:00:49.618+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:50:00+00:00 [queued]>
[2025-02-23T19:00:49.646+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:50:00+00:00 [queued]>
[2025-02-23T19:00:49.647+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T19:00:49.838+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 18:50:00+00:00
[2025-02-23T19:00:49.865+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1500) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T19:00:49.875+0000] {standard_task_runner.py:72} INFO - Started process 1501 to run task
[2025-02-23T19:00:49.875+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T18:50:00+00:00', '--job-id', '67', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpgdz2z8xi']
[2025-02-23T19:00:49.882+0000] {standard_task_runner.py:105} INFO - Job 67: Subtask process_silver
[2025-02-23T19:00:50.060+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T18:50:00+00:00 [running]> on host b06401153325
[2025-02-23T19:00:50.807+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:50:00+00:00'
[2025-02-23T19:00:50.818+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T19:00:50.933+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T19:00:50.940+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T19:00:51.002+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T19:00:51.036+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T19:00:59.005+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T19:00:59.009+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.060+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/fornecedores_db1.csv
[2025-02-23T19:00:59.068+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/produtos_db1.csv' baixado para 'temp_bronze_produtos_db1.csv'.
[2025-02-23T19:00:59.070+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.076+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/produtos_db1.csv
[2025-02-23T19:00:59.080+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendas_db1.csv' baixado para 'temp_bronze_vendas_db1.csv'.
[2025-02-23T19:00:59.093+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.097+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendas_db1.csv
[2025-02-23T19:00:59.102+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendedores_db1.csv' baixado para 'temp_bronze_vendedores_db1.csv'.
[2025-02-23T19:00:59.106+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.112+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendedores_db1.csv
[2025-02-23T19:00:59.116+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/fornecedores_db2.csv' baixado para 'temp_bronze_fornecedores_db2.csv'.
[2025-02-23T19:00:59.119+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.126+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/fornecedores_db2.csv
[2025-02-23T19:00:59.129+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/produtos_db2.csv' baixado para 'temp_bronze_produtos_db2.csv'.
[2025-02-23T19:00:59.132+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.139+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/produtos_db2.csv
[2025-02-23T19:00:59.146+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendas_db2.csv' baixado para 'temp_bronze_vendas_db2.csv'.
[2025-02-23T19:00:59.150+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.154+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendas_db2.csv
[2025-02-23T19:00:59.159+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendedores_db2.csv' baixado para 'temp_bronze_vendedores_db2.csv'.
[2025-02-23T19:00:59.163+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.166+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendedores_db2.csv
[2025-02-23T19:00:59.169+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/fornecedores_db3.csv' baixado para 'temp_bronze_fornecedores_db3.csv'.
[2025-02-23T19:00:59.172+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.186+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/fornecedores_db3.csv
[2025-02-23T19:00:59.193+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/produtos_db3.csv' baixado para 'temp_bronze_produtos_db3.csv'.
[2025-02-23T19:00:59.200+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.204+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/produtos_db3.csv
[2025-02-23T19:00:59.210+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendas_db3.csv' baixado para 'temp_bronze_vendas_db3.csv'.
[2025-02-23T19:00:59.216+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.226+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendas_db3.csv
[2025-02-23T19:00:59.231+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendedores_db3.csv' baixado para 'temp_bronze_vendedores_db3.csv'.
[2025-02-23T19:00:59.234+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:00:42.732+0000] {processor.py:186} INFO - Started process (PID=496) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:00:59.237+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendedores_db3.csv
[2025-02-23T19:00:59.764+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T19:00:59.768+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T19:00:59.926+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T19:00:59.930+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T18:50:00+00:00, execution_date=20250223T185000, start_date=20250223T190049, end_date=20250223T190059
[2025-02-23T19:01:00.191+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T19:01:00.320+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T19:01:00.333+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T19:01:04.648+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T19:01:04.741+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T18:50:00+00:00 [queued]>
[2025-02-23T19:01:04.777+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T18:50:00+00:00 [queued]>
[2025-02-23T19:01:04.779+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T19:01:04.828+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_analytics> on 2025-02-23 18:50:00+00:00
[2025-02-23T19:01:04.848+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1546) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T19:01:04.854+0000] {standard_task_runner.py:72} INFO - Started process 1547 to run task
[2025-02-23T19:01:04.855+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_analytics', 'scheduled__2025-02-23T18:50:00+00:00', '--job-id', '68', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp1y_b04ot']
[2025-02-23T19:01:04.859+0000] {standard_task_runner.py:105} INFO - Job 68: Subtask process_analytics
[2025-02-23T19:01:05.034+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T18:50:00+00:00 [running]> on host b06401153325
[2025-02-23T19:01:05.387+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_analytics' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T18:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T18:50:00+00:00'
[2025-02-23T19:01:05.390+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T19:01:05.440+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T19:01:05.445+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_3.sh']
[2025-02-23T19:01:05.480+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T19:01:05.499+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T19:01:11.850+0000] {subprocess.py:106} INFO -  Bucket 'datalake' j existe.
[2025-02-23T19:01:11.855+0000] {subprocess.py:106} INFO -  'silveranalitics/' inicializado no bucket 'datalake'.
[2025-02-23T19:01:11.858+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db1.csv' carregado de silver/db1.
[2025-02-23T19:01:11.861+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db2.csv' carregado de silver/db2.
[2025-02-23T19:01:11.865+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db3.csv' carregado de silver/db3.
[2025-02-23T19:01:11.867+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:01:11.870+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db1.csv' carregado de silver/db1.
[2025-02-23T19:01:11.873+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db2.csv' carregado de silver/db2.
[2025-02-23T19:01:11.876+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db3.csv' carregado de silver/db3.
[2025-02-23T19:01:11.880+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:01:11.883+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db1.csv' carregado de silver/db1.
[2025-02-23T19:01:11.886+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db2.csv' carregado de silver/db2.
[2025-02-23T19:01:11.888+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db3.csv' carregado de silver/db3.
[2025-02-23T19:01:11.891+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:01:11.893+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db1.csv' carregado de silver/db1.
[2025-02-23T19:01:11.896+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db2.csv' carregado de silver/db2.
[2025-02-23T19:01:11.899+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db3.csv' carregado de silver/db3.
[2025-02-23T19:01:11.902+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:01:12.094+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T19:01:12.098+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T19:01:12.243+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T19:01:12.245+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_analytics, run_id=scheduled__2025-02-23T18:50:00+00:00, execution_date=20250223T185000, start_date=20250223T190104, end_date=20250223T190112
[2025-02-23T19:01:12.371+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T19:01:12.436+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T19:01:12.450+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T19:01:16.379+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T19:01:16.496+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T18:50:00+00:00 [queued]>
[2025-02-23T19:01:16.536+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T18:50:00+00:00 [queued]>
[2025-02-23T19:01:16.539+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T19:01:16.599+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_gold> on 2025-02-23 18:50:00+00:00
[2025-02-23T19:01:16.622+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1568) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T19:01:16.636+0000] {standard_task_runner.py:72} INFO - Started process 1569 to run task
[2025-02-23T19:01:16.637+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_gold', 'scheduled__2025-02-23T18:50:00+00:00', '--job-id', '69', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp30l4lovk']
[2025-02-23T19:01:16.644+0000] {standard_task_runner.py:105} INFO - Job 69: Subtask process_gold
[2025-02-23T19:01:18.749+0000] {processor.py:186} INFO - Started process (PID=499) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:01:18.752+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:01:18.758+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:01:18.757+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:01:19.404+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:01:19.488+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:01:19.487+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:01:19.623+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:01:19.622+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:01:19.716+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.987 seconds
[2025-02-23T19:01:50.224+0000] {processor.py:186} INFO - Started process (PID=501) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:01:50.226+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:01:50.230+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:01:50.229+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:01:50.617+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:01:50.671+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:01:50.671+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:01:50.740+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:01:50.740+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:01:50.795+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.584 seconds
[2025-02-23T19:02:21.003+0000] {processor.py:186} INFO - Started process (PID=503) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:02:21.005+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:02:21.008+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:02:21.007+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:02:21.423+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:02:21.503+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:02:21.503+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:02:21.563+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:02:21.562+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:02:21.602+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.612 seconds
[2025-02-23T19:02:52.355+0000] {processor.py:186} INFO - Started process (PID=505) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:02:52.357+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:02:52.361+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:02:52.359+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:02:52.724+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:02:52.774+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:02:52.773+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:02:52.823+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:02:52.823+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:02:52.864+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.522 seconds
[2025-02-23T19:03:22.922+0000] {processor.py:186} INFO - Started process (PID=507) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:03:22.924+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:03:22.927+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:03:22.926+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:03:23.274+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:03:23.326+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:03:23.325+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:03:23.374+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:03:23.374+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:03:23.424+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.515 seconds
[2025-02-23T19:03:53.639+0000] {processor.py:186} INFO - Started process (PID=509) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:03:53.642+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:03:53.647+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:03:53.646+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:03:54.067+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:03:54.134+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:03:54.134+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:03:54.194+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:03:54.193+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:03:54.242+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.625 seconds
[2025-02-23T19:04:24.305+0000] {processor.py:186} INFO - Started process (PID=511) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:04:24.307+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:04:24.310+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:04:24.309+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:04:24.649+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:04:24.701+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:04:24.701+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:04:24.749+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:04:24.749+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:04:24.790+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.496 seconds
[2025-02-23T19:04:55.044+0000] {processor.py:186} INFO - Started process (PID=513) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:04:55.046+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:04:55.050+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:04:55.049+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:04:55.410+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:04:55.464+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:04:55.463+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:04:55.511+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:04:55.511+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:04:55.554+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.524 seconds
[2025-02-23T19:05:25.663+0000] {processor.py:186} INFO - Started process (PID=515) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:05:25.665+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:05:25.668+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:05:25.667+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:05:26.018+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:05:26.071+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:05:26.071+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:05:26.126+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:05:26.125+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:05:26.172+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.523 seconds
[2025-02-23T19:05:56.249+0000] {processor.py:186} INFO - Started process (PID=517) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:05:56.251+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:05:56.254+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:05:56.253+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:05:56.678+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:05:56.746+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:05:56.745+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:05:56.807+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:05:56.807+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:05:56.850+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.613 seconds
[2025-02-23T19:06:27.597+0000] {processor.py:186} INFO - Started process (PID=519) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:06:27.599+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:06:27.602+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:06:27.601+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:06:27.944+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:06:27.995+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:06:27.995+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:06:28.043+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:06:28.042+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:06:28.087+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.503 seconds
[2025-02-23T19:06:58.393+0000] {processor.py:186} INFO - Started process (PID=521) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:06:58.396+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:06:58.399+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:06:58.398+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:06:58.819+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:06:58.871+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:06:58.871+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:06:58.919+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:06:58.918+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:06:58.960+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.581 seconds
[2025-02-23T19:07:29.737+0000] {processor.py:186} INFO - Started process (PID=523) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:07:29.740+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:07:29.742+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:07:29.742+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:07:30.087+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:07:30.184+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:07:30.183+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:07:30.249+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:07:30.248+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:07:30.305+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.580 seconds
[2025-02-23T19:08:00.523+0000] {processor.py:186} INFO - Started process (PID=525) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:08:00.525+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:08:00.528+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:08:00.528+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:08:00.869+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:08:00.931+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:08:00.930+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:08:00.995+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:08:00.995+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:08:01.039+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.528 seconds
[2025-02-23T19:08:31.131+0000] {processor.py:186} INFO - Started process (PID=527) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:08:31.133+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:08:31.138+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:08:31.137+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:08:31.483+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:08:31.539+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:08:31.538+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:08:31.587+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:08:31.587+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:08:31.634+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.518 seconds
[2025-02-23T19:09:02.475+0000] {processor.py:186} INFO - Started process (PID=529) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:09:02.477+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:09:02.480+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:09:02.480+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:09:02.931+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:09:02.982+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:09:02.982+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:09:03.029+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:09:03.028+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:09:03.069+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.607 seconds
[2025-02-23T19:09:33.308+0000] {processor.py:186} INFO - Started process (PID=531) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:09:33.311+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:09:33.315+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:09:33.314+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:09:33.812+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:09:33.866+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:09:33.865+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:09:33.914+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:09:33.914+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:00:00+00:00, run_after=2025-02-23 19:10:00+00:00
[2025-02-23T19:09:33.958+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.668 seconds
[2025-02-23T19:10:02.417+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T19:10:02.479+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T19:00:00+00:00 [queued]>
[2025-02-23T19:10:02.497+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T19:00:00+00:00 [queued]>
[2025-02-23T19:10:02.498+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T19:10:02.528+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 19:00:00+00:00
[2025-02-23T19:10:02.539+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1590) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T19:10:02.542+0000] {standard_task_runner.py:72} INFO - Started process 1591 to run task
[2025-02-23T19:10:02.543+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T19:00:00+00:00', '--job-id', '70', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmps4oiz8gf']
[2025-02-23T19:10:02.547+0000] {standard_task_runner.py:105} INFO - Job 70: Subtask process_landzone
[2025-02-23T19:10:02.718+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T19:00:00+00:00 [running]> on host b06401153325
[2025-02-23T19:10:03.002+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T19:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T19:00:00+00:00'
[2025-02-23T19:10:03.006+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T19:10:03.046+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T19:10:03.048+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T19:10:03.067+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T19:10:03.077+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T19:10:07.561+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T19:10:07.563+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.565+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T19:10:07.567+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.569+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T19:10:07.571+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.574+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T19:10:07.576+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.579+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T19:10:07.582+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.585+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T19:10:07.587+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.589+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T19:10:07.592+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.594+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T19:10:07.596+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.598+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T19:10:07.600+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.601+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T19:10:07.603+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.605+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T19:10:07.607+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.609+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T19:10:07.611+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.csv atualizado e enviado para MinIO!
[2025-02-23T19:10:07.773+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T19:10:07.774+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T19:10:07.832+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T19:10:07.833+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T19:00:00+00:00, execution_date=20250223T190000, start_date=20250223T191002, end_date=20250223T191007
[2025-02-23T19:10:07.905+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T19:10:07.996+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T19:10:08.005+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T19:10:10.681+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T19:10:10.726+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T19:00:00+00:00 [queued]>
[2025-02-23T19:10:10.747+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T19:00:00+00:00 [queued]>
[2025-02-23T19:10:10.748+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T19:10:10.777+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 19:00:00+00:00
[2025-02-23T19:10:10.788+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1636) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T19:10:10.792+0000] {standard_task_runner.py:72} INFO - Started process 1637 to run task
[2025-02-23T19:10:10.793+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T19:00:00+00:00', '--job-id', '71', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpkvunrf_6']
[2025-02-23T19:10:10.795+0000] {standard_task_runner.py:105} INFO - Job 71: Subtask process_bronze
[2025-02-23T19:10:10.890+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T19:00:00+00:00 [running]> on host b06401153325
[2025-02-23T19:10:11.092+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T19:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T19:00:00+00:00'
[2025-02-23T19:10:11.094+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T19:10:11.129+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T19:10:11.132+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T19:10:11.152+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T19:10:11.161+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T19:10:18.123+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.csv' baixado para 'temp_landzone_fornecedores_db1.csv'.
[2025-02-23T19:10:18.125+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db1.csv' enviado para 'datalake/bronze/db1/fornecedores_db1.csv'.
[2025-02-23T19:10:18.127+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.csv' baixado para 'temp_landzone_fornecedores_db2.csv'.
[2025-02-23T19:10:18.129+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db2.csv' enviado para 'datalake/bronze/db2/fornecedores_db2.csv'.
[2025-02-23T19:10:18.132+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.csv' baixado para 'temp_landzone_fornecedores_db3.csv'.
[2025-02-23T19:10:18.135+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db3.csv' enviado para 'datalake/bronze/db3/fornecedores_db3.csv'.
[2025-02-23T19:10:18.138+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.csv' baixado para 'temp_landzone_produtos_db1.csv'.
[2025-02-23T19:10:18.142+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db1.csv' enviado para 'datalake/bronze/db1/produtos_db1.csv'.
[2025-02-23T19:10:18.144+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.csv' baixado para 'temp_landzone_produtos_db2.csv'.
[2025-02-23T19:10:18.146+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db2.csv' enviado para 'datalake/bronze/db2/produtos_db2.csv'.
[2025-02-23T19:10:18.149+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.csv' baixado para 'temp_landzone_produtos_db3.csv'.
[2025-02-23T19:10:18.152+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db3.csv' enviado para 'datalake/bronze/db3/produtos_db3.csv'.
[2025-02-23T19:10:18.155+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.csv' baixado para 'temp_landzone_vendas_db1.csv'.
[2025-02-23T19:10:18.157+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db1.csv' enviado para 'datalake/bronze/db1/vendas_db1.csv'.
[2025-02-23T19:10:18.159+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.csv' baixado para 'temp_landzone_vendas_db2.csv'.
[2025-02-23T19:10:18.161+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db2.csv' enviado para 'datalake/bronze/db2/vendas_db2.csv'.
[2025-02-23T19:10:18.163+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.csv' baixado para 'temp_landzone_vendas_db3.csv'.
[2025-02-23T19:10:18.165+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db3.csv' enviado para 'datalake/bronze/db3/vendas_db3.csv'.
[2025-02-23T19:10:18.167+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.csv' baixado para 'temp_landzone_vendedores_db1.csv'.
[2025-02-23T19:10:18.174+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db1.csv' enviado para 'datalake/bronze/db1/vendedores_db1.csv'.
[2025-02-23T19:10:18.176+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.csv' baixado para 'temp_landzone_vendedores_db2.csv'.
[2025-02-23T19:10:18.179+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db2.csv' enviado para 'datalake/bronze/db2/vendedores_db2.csv'.
[2025-02-23T19:10:18.185+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.csv' baixado para 'temp_landzone_vendedores_db3.csv'.
[2025-02-23T19:10:18.189+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db3.csv' enviado para 'datalake/bronze/db3/vendedores_db3.csv'.
[2025-02-23T19:10:18.364+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T19:10:18.366+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T19:10:18.427+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T19:10:18.429+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T19:00:00+00:00, execution_date=20250223T190000, start_date=20250223T191010, end_date=20250223T191018
[2025-02-23T19:10:18.617+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T19:10:18.726+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T19:10:18.734+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T19:10:21.685+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T19:10:21.737+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T19:00:00+00:00 [queued]>
[2025-02-23T19:10:21.766+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T19:00:00+00:00 [queued]>
[2025-02-23T19:10:21.768+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T19:10:21.807+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 19:00:00+00:00
[2025-02-23T19:10:21.827+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1646) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T19:10:21.834+0000] {standard_task_runner.py:72} INFO - Started process 1647 to run task
[2025-02-23T19:10:21.835+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T19:00:00+00:00', '--job-id', '72', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp_ig8l8t7']
[2025-02-23T19:10:21.840+0000] {standard_task_runner.py:105} INFO - Job 72: Subtask process_silver
[2025-02-23T19:10:21.954+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T19:00:00+00:00 [running]> on host b06401153325
[2025-02-23T19:10:22.255+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T19:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T19:00:00+00:00'
[2025-02-23T19:10:22.258+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T19:10:22.293+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T19:10:22.295+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T19:10:22.315+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T19:10:22.325+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T19:10:26.954+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:10:26.952+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:10:27.133+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:10:27.132+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00
[2025-02-23T19:10:27.234+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 0.633 seconds
[2025-02-23T19:10:27.844+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T19:10:27.850+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:09:55.389+0000] {processor.py:186} INFO - Started process (PID=532) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:27.856+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/fornecedores_db1.csv
[2025-02-23T19:10:27.886+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/produtos_db1.csv' baixado para 'temp_bronze_produtos_db1.csv'.
[2025-02-23T19:10:27.889+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:09:55.389+0000] {processor.py:186} INFO - Started process (PID=532) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:27.895+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/produtos_db1.csv
[2025-02-23T19:10:27.898+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendas_db1.csv' baixado para 'temp_bronze_vendas_db1.csv'.
[2025-02-23T19:10:27.901+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:09:55.389+0000] {processor.py:186} INFO - Started process (PID=532) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:27.907+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendas_db1.csv
[2025-02-23T19:10:27.913+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendedores_db1.csv' baixado para 'temp_bronze_vendedores_db1.csv'.
[2025-02-23T19:10:27.919+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:09:55.389+0000] {processor.py:186} INFO - Started process (PID=532) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:27.925+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendedores_db1.csv
[2025-02-23T19:10:27.931+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/fornecedores_db2.csv' baixado para 'temp_bronze_fornecedores_db2.csv'.
[2025-02-23T19:10:27.935+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:09:55.389+0000] {processor.py:186} INFO - Started process (PID=532) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:27.937+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/fornecedores_db2.csv
[2025-02-23T19:10:27.940+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/produtos_db2.csv' baixado para 'temp_bronze_produtos_db2.csv'.
[2025-02-23T19:10:27.948+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:10:26.631+0000] {processor.py:186} INFO - Started process (PID=534) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:27.951+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/produtos_db2.csv
[2025-02-23T19:10:27.953+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendas_db2.csv' baixado para 'temp_bronze_vendas_db2.csv'.
[2025-02-23T19:10:27.956+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:10:26.631+0000] {processor.py:186} INFO - Started process (PID=534) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:27.959+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendas_db2.csv
[2025-02-23T19:10:27.962+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendedores_db2.csv' baixado para 'temp_bronze_vendedores_db2.csv'.
[2025-02-23T19:10:27.968+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:10:26.631+0000] {processor.py:186} INFO - Started process (PID=534) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:27.972+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendedores_db2.csv
[2025-02-23T19:10:27.975+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/fornecedores_db3.csv' baixado para 'temp_bronze_fornecedores_db3.csv'.
[2025-02-23T19:10:27.978+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:10:26.631+0000] {processor.py:186} INFO - Started process (PID=534) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:27.981+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/fornecedores_db3.csv
[2025-02-23T19:10:27.984+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/produtos_db3.csv' baixado para 'temp_bronze_produtos_db3.csv'.
[2025-02-23T19:10:27.987+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:10:26.631+0000] {processor.py:186} INFO - Started process (PID=534) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:27.991+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/produtos_db3.csv
[2025-02-23T19:10:27.998+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendas_db3.csv' baixado para 'temp_bronze_vendas_db3.csv'.
[2025-02-23T19:10:28.002+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:10:26.631+0000] {processor.py:186} INFO - Started process (PID=534) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:28.012+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendas_db3.csv
[2025-02-23T19:10:28.029+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendedores_db3.csv' baixado para 'temp_bronze_vendedores_db3.csv'.
[2025-02-23T19:10:28.034+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T19:10:26.631+0000] {processor.py:186} INFO - Started process (PID=534) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T19:10:28.036+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendedores_db3.csv
[2025-02-23T19:10:28.356+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T19:10:28.358+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T19:10:28.423+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T19:10:28.424+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T19:00:00+00:00, execution_date=20250223T190000, start_date=20250223T191021, end_date=20250223T191028
[2025-02-23T19:10:28.512+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T19:10:28.573+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T19:10:28.582+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T19:10:31.154+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T19:10:31.232+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T19:00:00+00:00 [queued]>
[2025-02-23T19:10:31.261+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T19:00:00+00:00 [queued]>
[2025-02-23T19:10:31.262+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T19:10:31.303+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_analytics> on 2025-02-23 19:00:00+00:00
[2025-02-23T19:10:31.318+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1692) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T19:10:31.322+0000] {standard_task_runner.py:72} INFO - Started process 1693 to run task
[2025-02-23T19:10:31.324+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_analytics', 'scheduled__2025-02-23T19:00:00+00:00', '--job-id', '73', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpce51s570']
[2025-02-23T19:10:31.329+0000] {standard_task_runner.py:105} INFO - Job 73: Subtask process_analytics
[2025-02-23T19:10:31.435+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T19:00:00+00:00 [running]> on host b06401153325
[2025-02-23T19:10:31.645+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_analytics' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T19:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T19:00:00+00:00'
[2025-02-23T19:10:31.647+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T19:10:31.696+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T19:10:31.700+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_3.sh']
[2025-02-23T19:10:31.729+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T19:10:31.743+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T19:10:36.039+0000] {subprocess.py:106} INFO -  Bucket 'datalake' j existe.
[2025-02-23T19:10:36.042+0000] {subprocess.py:106} INFO -  'silveranalitics/' inicializado no bucket 'datalake'.
[2025-02-23T19:10:36.045+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db1.csv' carregado de silver/db1.
[2025-02-23T19:10:36.047+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db2.csv' carregado de silver/db2.
[2025-02-23T19:10:36.049+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db3.csv' carregado de silver/db3.
[2025-02-23T19:10:36.052+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:10:36.055+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db1.csv' carregado de silver/db1.
[2025-02-23T19:10:36.058+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db2.csv' carregado de silver/db2.
[2025-02-23T19:10:36.060+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db3.csv' carregado de silver/db3.
[2025-02-23T19:10:36.063+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:10:36.065+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db1.csv' carregado de silver/db1.
[2025-02-23T19:10:36.067+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db2.csv' carregado de silver/db2.
[2025-02-23T19:10:36.070+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db3.csv' carregado de silver/db3.
[2025-02-23T19:10:36.073+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:10:36.075+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db1.csv' carregado de silver/db1.
[2025-02-23T19:10:36.078+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db2.csv' carregado de silver/db2.
[2025-02-23T19:10:36.080+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db3.csv' carregado de silver/db3.
[2025-02-23T19:10:36.083+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:10:36.265+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T19:10:36.267+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T19:10:36.325+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T19:10:36.327+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_analytics, run_id=scheduled__2025-02-23T19:00:00+00:00, execution_date=20250223T190000, start_date=20250223T191031, end_date=20250223T191036
[2025-02-23T19:10:36.557+0000] {processor.py:186} INFO - Started process (PID=535) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:10:36.561+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:10:36.570+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:10:36.568+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:10:37.973+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:10:38.049+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:10:38.049+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:10:38.128+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:10:38.128+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:10:38.192+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.663 seconds
[2025-02-23T19:10:39.165+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T19:10:39.222+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T19:00:00+00:00 [queued]>
[2025-02-23T19:10:39.248+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T19:00:00+00:00 [queued]>
[2025-02-23T19:10:39.250+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T19:10:39.289+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_gold> on 2025-02-23 19:00:00+00:00
[2025-02-23T19:10:39.303+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1714) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T19:10:39.307+0000] {standard_task_runner.py:72} INFO - Started process 1715 to run task
[2025-02-23T19:10:39.308+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_gold', 'scheduled__2025-02-23T19:00:00+00:00', '--job-id', '74', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpfdoto8cl']
[2025-02-23T19:10:39.311+0000] {standard_task_runner.py:105} INFO - Job 74: Subtask process_gold
[2025-02-23T19:10:39.445+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T19:00:00+00:00 [running]> on host b06401153325
[2025-02-23T19:10:39.779+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_gold' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T19:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T19:00:00+00:00'
[2025-02-23T19:10:39.784+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T19:10:39.841+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T19:10:39.844+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_4.sh']
[2025-02-23T19:10:39.878+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T19:10:39.893+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T19:10:44.875+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsFornecedor.csv' carregado.
[2025-02-23T19:10:44.878+0000] {subprocess.py:106} INFO -  'gold/goldFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:10:44.881+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsProdutos.csv' carregado.
[2025-02-23T19:10:44.884+0000] {subprocess.py:106} INFO -  'gold/goldProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:10:44.888+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsVendas.csv' carregado.
[2025-02-23T19:10:44.890+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' carregado para *join*.
[2025-02-23T19:10:44.893+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' carregado para *join*.
[2025-02-23T19:10:44.895+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' carregado para *join*.
[2025-02-23T19:10:44.899+0000] {subprocess.py:106} INFO -  'gold/goldVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:10:44.902+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsVendedor.csv' carregado.
[2025-02-23T19:10:44.905+0000] {subprocess.py:106} INFO -  'gold/goldVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T19:10:45.218+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T19:10:45.220+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T19:10:45.292+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T19:10:45.294+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_gold, run_id=scheduled__2025-02-23T19:00:00+00:00, execution_date=20250223T190000, start_date=20250223T191039, end_date=20250223T191045
[2025-02-23T19:10:45.415+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T19:10:45.449+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T19:10:45.456+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T19:11:08.871+0000] {processor.py:186} INFO - Started process (PID=537) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:11:08.874+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:11:08.877+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:11:08.877+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:11:09.257+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:11:09.315+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:11:09.314+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:11:09.366+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:11:09.365+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:11:09.444+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.607 seconds
[2025-02-23T19:11:39.598+0000] {processor.py:186} INFO - Started process (PID=539) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:11:39.600+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:11:39.604+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:11:39.603+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:11:39.958+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:11:40.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:11:40.011+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:11:40.059+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:11:40.058+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:11:40.109+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.525 seconds
[2025-02-23T19:12:10.179+0000] {processor.py:186} INFO - Started process (PID=541) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:12:10.182+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:12:10.187+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:12:10.186+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:12:10.586+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:12:10.639+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:12:10.638+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:12:10.687+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:12:10.686+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:12:10.728+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.570 seconds
[2025-02-23T19:12:41.567+0000] {processor.py:186} INFO - Started process (PID=543) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:12:41.569+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:12:41.573+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:12:41.572+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:12:41.936+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:12:41.987+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:12:41.986+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:12:42.042+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:12:42.041+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:12:42.106+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.553 seconds
[2025-02-23T19:13:12.408+0000] {processor.py:186} INFO - Started process (PID=545) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:13:12.411+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:13:12.415+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:13:12.414+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:13:12.816+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:13:12.871+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:13:12.871+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:13:12.922+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:13:12.922+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:13:12.965+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.575 seconds
[2025-02-23T19:13:43.765+0000] {processor.py:186} INFO - Started process (PID=547) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:13:43.767+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:13:43.770+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:13:43.769+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:13:44.150+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:13:44.362+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:13:44.361+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:13:44.420+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:13:44.419+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:13:44.466+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.714 seconds
[2025-02-23T19:14:14.623+0000] {processor.py:186} INFO - Started process (PID=549) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:14:14.626+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:14:14.630+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:14:14.629+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:14:15.047+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:14:15.100+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:14:15.100+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:14:15.155+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:14:15.154+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:14:15.201+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.594 seconds
[2025-02-23T19:14:45.345+0000] {processor.py:186} INFO - Started process (PID=551) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:14:45.348+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:14:45.352+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:14:45.351+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:14:45.713+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:14:45.770+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:14:45.770+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:14:45.818+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:14:45.818+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:14:45.859+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.529 seconds
[2025-02-23T19:15:15.907+0000] {processor.py:186} INFO - Started process (PID=553) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:15:15.909+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:15:15.912+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:15:15.911+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:15:16.255+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:15:16.308+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:15:16.307+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:15:16.356+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:15:16.356+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:15:16.396+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.502 seconds
[2025-02-23T19:15:46.637+0000] {processor.py:186} INFO - Started process (PID=555) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:15:46.640+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:15:46.643+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:15:46.642+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:15:47.008+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:15:47.085+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:15:47.085+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:15:47.151+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:15:47.151+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:15:47.208+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.586 seconds
[2025-02-23T19:16:17.477+0000] {processor.py:186} INFO - Started process (PID=557) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:16:17.480+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:16:17.487+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:16:17.485+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:16:17.989+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:16:18.093+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:16:18.092+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:16:18.194+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:16:18.193+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:16:18.245+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.787 seconds
[2025-02-23T19:18:25.674+0000] {processor.py:186} INFO - Started process (PID=558) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:18:25.686+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T19:18:25.694+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:18:25.692+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:18:28.580+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T19:18:28.898+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:18:28.897+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T19:18:29.522+0000] {logging_mixin.py:190} INFO - [2025-02-23T19:18:29.521+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 19:10:00+00:00, run_after=2025-02-23 19:20:00+00:00
[2025-02-23T19:18:29.689+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.312 seconds
[2025-02-23T21:00:19.814+0000] {processor.py:186} INFO - Started process (PID=560) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:00:20.020+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:00:20.026+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:00:20.026+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:00:20.830+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:00:20.926+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:00:20.926+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:00:21.024+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:00:21.023+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:00:00+00:00, run_after=2025-02-23 21:10:00+00:00
[2025-02-23T21:00:21.090+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.300 seconds
[2025-02-23T21:00:22.337+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:00:22.360+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:00:22.392+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T20:50:00+00:00 [queued]>
[2025-02-23T21:00:22.415+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:00:22.426+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T20:50:00+00:00 [queued]>
[2025-02-23T21:00:22.428+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:00:22.445+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:00:22.447+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:00:22.476+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 20:50:00+00:00
[2025-02-23T21:00:22.495+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 19:10:00+00:00
[2025-02-23T21:00:22.493+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1737) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:00:22.500+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T20:50:00+00:00', '--job-id', '75', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp3smpgonx']
[2025-02-23T21:00:22.503+0000] {standard_task_runner.py:72} INFO - Started process 1738 to run task
[2025-02-23T21:00:22.504+0000] {standard_task_runner.py:105} INFO - Job 75: Subtask process_landzone
[2025-02-23T21:00:22.514+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1736) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:00:22.520+0000] {standard_task_runner.py:72} INFO - Started process 1740 to run task
[2025-02-23T21:00:22.529+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T19:10:00+00:00', '--job-id', '76', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp1ov8vvyc']
[2025-02-23T21:00:22.534+0000] {standard_task_runner.py:105} INFO - Job 76: Subtask process_landzone
[2025-02-23T21:00:22.694+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T20:50:00+00:00 [running]> on host b06401153325
[2025-02-23T21:00:22.725+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T19:10:00+00:00 [running]> on host b06401153325
[2025-02-23T21:00:23.050+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T20:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T20:50:00+00:00'
[2025-02-23T21:00:23.053+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:00:23.077+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T19:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T19:10:00+00:00'
[2025-02-23T21:00:23.080+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:00:23.102+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:00:23.105+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T21:00:23.127+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:00:23.130+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T21:00:23.132+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:00:23.147+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:00:23.161+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:00:23.175+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:00:28.621+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T21:00:28.624+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:28.628+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T21:00:28.632+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 93, in <module>"
[2025-02-23T21:00:28.635+0000] {subprocess.py:106} INFO -     df_new = read_duckdb_table(db_file, table)
[2025-02-23T21:00:28.639+0000] {subprocess.py:106} INFO -              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T21:00:28.642+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 37, in read_duckdb_table"
[2025-02-23T21:00:28.646+0000] {subprocess.py:106} INFO -     conn = duckdb.connect(db_file)
[2025-02-23T21:00:28.649+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T21:00:28.658+0000] {subprocess.py:106} INFO - duckdb.duckdb.IOException: IO Error: Could not set lock on file ""/Volumes/MACBACKUP/workspaceDlake/dlake/db1.duckdb"": Conflicting lock is held in /usr/local/bin/python3.12 (PID 1745). See also https://duckdb.org/docs/connect/concurrency"
[2025-02-23T21:00:28.934+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T21:00:28.936+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T21:00:28.969+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T21:00:28.992+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T19:10:00+00:00, execution_date=20250223T191000, start_date=20250223T210022, end_date=20250223T210028
[2025-02-23T21:00:29.133+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:00:29.135+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 76 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 1740)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T21:00:29.183+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T21:00:29.245+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:00:29.249+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:00:30.240+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T21:00:30.243+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.245+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T21:00:30.249+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.254+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T21:00:30.256+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.259+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T21:00:30.262+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.265+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T21:00:30.267+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.268+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T21:00:30.270+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.272+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T21:00:30.274+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.275+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T21:00:30.277+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.279+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T21:00:30.280+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.282+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T21:00:30.284+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.286+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T21:00:30.289+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.291+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T21:00:30.293+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:00:30.617+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:00:30.626+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:00:30.843+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:00:30.846+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T20:50:00+00:00, execution_date=20250223T205000, start_date=20250223T210022, end_date=20250223T210030
[2025-02-23T21:00:30.970+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:00:31.091+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:00:31.098+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:00:33.606+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:00:33.655+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T20:50:00+00:00 [queued]>
[2025-02-23T21:00:33.677+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T20:50:00+00:00 [queued]>
[2025-02-23T21:00:33.678+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:00:33.709+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 20:50:00+00:00
[2025-02-23T21:00:33.720+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1795) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:00:33.724+0000] {standard_task_runner.py:72} INFO - Started process 1796 to run task
[2025-02-23T21:00:33.724+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T20:50:00+00:00', '--job-id', '77', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp1u0ndjxs']
[2025-02-23T21:00:33.726+0000] {standard_task_runner.py:105} INFO - Job 77: Subtask process_bronze
[2025-02-23T21:00:33.822+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T20:50:00+00:00 [running]> on host b06401153325
[2025-02-23T21:00:34.019+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T20:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T20:50:00+00:00'
[2025-02-23T21:00:34.022+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:00:34.056+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:00:34.058+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T21:00:34.080+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:00:34.092+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:00:41.019+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.csv' baixado para 'temp_landzone_fornecedores_db1.csv'.
[2025-02-23T21:00:41.021+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db1.csv' enviado para 'datalake/bronze/db1/fornecedores_db1.csv'.
[2025-02-23T21:00:41.024+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.csv' baixado para 'temp_landzone_fornecedores_db2.csv'.
[2025-02-23T21:00:41.029+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db2.csv' enviado para 'datalake/bronze/db2/fornecedores_db2.csv'.
[2025-02-23T21:00:41.033+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.csv' baixado para 'temp_landzone_fornecedores_db3.csv'.
[2025-02-23T21:00:41.036+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db3.csv' enviado para 'datalake/bronze/db3/fornecedores_db3.csv'.
[2025-02-23T21:00:41.039+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.csv' baixado para 'temp_landzone_produtos_db1.csv'.
[2025-02-23T21:00:41.042+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db1.csv' enviado para 'datalake/bronze/db1/produtos_db1.csv'.
[2025-02-23T21:00:41.044+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.csv' baixado para 'temp_landzone_produtos_db2.csv'.
[2025-02-23T21:00:41.046+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db2.csv' enviado para 'datalake/bronze/db2/produtos_db2.csv'.
[2025-02-23T21:00:41.048+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.csv' baixado para 'temp_landzone_produtos_db3.csv'.
[2025-02-23T21:00:41.051+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db3.csv' enviado para 'datalake/bronze/db3/produtos_db3.csv'.
[2025-02-23T21:00:41.054+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.csv' baixado para 'temp_landzone_vendas_db1.csv'.
[2025-02-23T21:00:41.056+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db1.csv' enviado para 'datalake/bronze/db1/vendas_db1.csv'.
[2025-02-23T21:00:41.058+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.csv' baixado para 'temp_landzone_vendas_db2.csv'.
[2025-02-23T21:00:41.060+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db2.csv' enviado para 'datalake/bronze/db2/vendas_db2.csv'.
[2025-02-23T21:00:41.062+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.csv' baixado para 'temp_landzone_vendas_db3.csv'.
[2025-02-23T21:00:41.064+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db3.csv' enviado para 'datalake/bronze/db3/vendas_db3.csv'.
[2025-02-23T21:00:41.066+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.csv' baixado para 'temp_landzone_vendedores_db1.csv'.
[2025-02-23T21:00:41.069+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db1.csv' enviado para 'datalake/bronze/db1/vendedores_db1.csv'.
[2025-02-23T21:00:41.071+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.csv' baixado para 'temp_landzone_vendedores_db2.csv'.
[2025-02-23T21:00:41.073+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db2.csv' enviado para 'datalake/bronze/db2/vendedores_db2.csv'.
[2025-02-23T21:00:41.075+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.csv' baixado para 'temp_landzone_vendedores_db3.csv'.
[2025-02-23T21:00:41.077+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db3.csv' enviado para 'datalake/bronze/db3/vendedores_db3.csv'.
[2025-02-23T21:00:41.228+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:00:41.230+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:00:41.285+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:00:41.287+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T20:50:00+00:00, execution_date=20250223T205000, start_date=20250223T210033, end_date=20250223T210041
[2025-02-23T21:00:41.373+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:00:41.429+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:00:41.435+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:00:43.972+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:00:44.054+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T20:50:00+00:00 [queued]>
[2025-02-23T21:00:44.087+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T20:50:00+00:00 [queued]>
[2025-02-23T21:00:44.088+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:00:44.125+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 20:50:00+00:00
[2025-02-23T21:00:44.138+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1805) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:00:44.143+0000] {standard_task_runner.py:72} INFO - Started process 1806 to run task
[2025-02-23T21:00:44.144+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T20:50:00+00:00', '--job-id', '78', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp2wvven7d']
[2025-02-23T21:00:44.147+0000] {standard_task_runner.py:105} INFO - Job 78: Subtask process_silver
[2025-02-23T21:00:44.256+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T20:50:00+00:00 [running]> on host b06401153325
[2025-02-23T21:00:44.473+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T20:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T20:50:00+00:00'
[2025-02-23T21:00:44.475+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:00:44.512+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:00:44.515+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T21:00:44.537+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:00:44.549+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T21:00:49.305+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T21:00:49.309+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.312+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/fornecedores_db1.csv
[2025-02-23T21:00:49.315+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/produtos_db1.csv' baixado para 'temp_bronze_produtos_db1.csv'.
[2025-02-23T21:00:49.317+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.318+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/produtos_db1.csv
[2025-02-23T21:00:49.320+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendas_db1.csv' baixado para 'temp_bronze_vendas_db1.csv'.
[2025-02-23T21:00:49.322+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.325+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendas_db1.csv
[2025-02-23T21:00:49.327+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendedores_db1.csv' baixado para 'temp_bronze_vendedores_db1.csv'.
[2025-02-23T21:00:49.329+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.331+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendedores_db1.csv
[2025-02-23T21:00:49.334+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/fornecedores_db2.csv' baixado para 'temp_bronze_fornecedores_db2.csv'.
[2025-02-23T21:00:49.336+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.338+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/fornecedores_db2.csv
[2025-02-23T21:00:49.340+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/produtos_db2.csv' baixado para 'temp_bronze_produtos_db2.csv'.
[2025-02-23T21:00:49.342+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.344+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/produtos_db2.csv
[2025-02-23T21:00:49.346+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendas_db2.csv' baixado para 'temp_bronze_vendas_db2.csv'.
[2025-02-23T21:00:49.348+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.350+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendas_db2.csv
[2025-02-23T21:00:49.351+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendedores_db2.csv' baixado para 'temp_bronze_vendedores_db2.csv'.
[2025-02-23T21:00:49.353+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.355+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendedores_db2.csv
[2025-02-23T21:00:49.357+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/fornecedores_db3.csv' baixado para 'temp_bronze_fornecedores_db3.csv'.
[2025-02-23T21:00:49.360+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.362+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/fornecedores_db3.csv
[2025-02-23T21:00:49.364+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/produtos_db3.csv' baixado para 'temp_bronze_produtos_db3.csv'.
[2025-02-23T21:00:49.366+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.368+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/produtos_db3.csv
[2025-02-23T21:00:49.369+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendas_db3.csv' baixado para 'temp_bronze_vendas_db3.csv'.
[2025-02-23T21:00:49.371+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.374+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendas_db3.csv
[2025-02-23T21:00:49.376+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendedores_db3.csv' baixado para 'temp_bronze_vendedores_db3.csv'.
[2025-02-23T21:00:49.378+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:00:20.055+0000] {processor.py:186} INFO - Started process (PID=561) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:00:49.380+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendedores_db3.csv
[2025-02-23T21:00:49.535+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:00:49.537+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:00:49.617+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:00:49.619+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T20:50:00+00:00, execution_date=20250223T205000, start_date=20250223T210044, end_date=20250223T210049
[2025-02-23T21:00:49.740+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:00:49.834+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:00:49.839+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:00:51.781+0000] {processor.py:186} INFO - Started process (PID=563) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:00:51.783+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:00:51.786+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:00:51.785+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:00:52.346+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /op[2025-02-23T21:00:52.390+0000] {tas[2025-02-23T21:00:52.404+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:00:52.403+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:00:52.468+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:00:52.468+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:00:00+00:00, run_after=2025-02-23 21:10:00+00:00
[2025-02-23T21:00:52.516+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.749 seconds
attempt 1 of 2
[2025-02-23T21:00:52.454+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_analytics> on 2025-02-23 20:50:00+00:00
[2025-02-23T21:00:52.468+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1851) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:00:52.474+0000] {standard_task_runner.py:72} INFO - Started process 1852 to run task
[2025-02-23T21:00:52.476+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_analytics', 'scheduled__2025-02-23T20:50:00+00:00', '--job-id', '79', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp8qp6azrg']
[2025-02-23T21:00:52.480+0000] {standard_task_runner.py:105} INFO - Job 79: Subtask process_analytics
[2025-02-23T21:00:52.594+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T20:50:00+00:00 [running]> on host b06401153325
[2025-02-23T21:00:52.835+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_analytics' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T20:50:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T20:50:00+00:00'
[2025-02-23T21:00:52.839+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:00:52.898+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:00:52.902+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_3.sh']
[2025-02-23T21:00:52.940+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:00:52.960+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:01:12.066+0000] {subprocess.py:106} INFO -  Bucket 'datalake' j existe.
[2025-02-23T21:01:12.068+0000] {subprocess.py:106} INFO -  'silveranalitics/' inicializado no bucket 'datalake'.
[2025-02-23T21:01:12.069+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db1.csv' carregado de silver/db1.
[2025-02-23T21:01:12.071+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db2.csv' carregado de silver/db2.
[2025-02-23T21:01:12.072+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db3.csv' carregado de silver/db3.
[2025-02-23T21:01:12.074+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:01:12.076+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db1.csv' carregado de silver/db1.
[2025-02-23T21:01:12.077+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db2.csv' carregado de silver/db2.
[2025-02-23T21:01:12.080+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db3.csv' carregado de silver/db3.
[2025-02-23T21:01:12.082+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:01:12.083+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db1.csv' carregado de silver/db1.
[2025-02-23T21:01:12.085+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db2.csv' carregado de silver/db2.
[2025-02-23T21:01:12.086+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db3.csv' carregado de silver/db3.
[2025-02-23T21:01:12.088+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:01:12.089+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db1.csv' carregado de silver/db1.
[2025-02-23T21:01:12.091+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db2.csv' carregado de silver/db2.
[2025-02-23T21:01:12.092+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db3.csv' carregado de silver/db3.
[2025-02-23T21:01:12.094+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:01:12.232+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:01:12.233+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:01:12.286+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:01:12.288+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_analytics, run_id=scheduled__2025-02-23T20:50:00+00:00, execution_date=20250223T205000, start_date=20250223T210052, end_date=20250223T210112
[2025-02-23T21:01:12.390+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:01:12.432+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:01:12.439+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:06:18.428+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:06:19.200+0000] {processor.py:186} INFO - Started process (PID=564) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:06:19.207+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:06:19.211+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:06:19.210+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:06:24.572+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:06:24.903+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:06:24.902+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:06:25.082+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:06:25.081+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:00:00+00:00, run_after=2025-02-23 21:10:00+00:00
[2025-02-23T21:06:25.181+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 6.176 seconds
[2025-02-23T21:06:55.724+0000] {processor.py:186} INFO - Started process (PID=567) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:06:56.056+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:06:56.064+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:06:56.062+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:06:57.234+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:06:57.475+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:06:57.474+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:06:57.739+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:06:57.733+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:00:00+00:00, run_after=2025-02-23 21:10:00+00:00
[2025-02-23T21:06:57.842+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.150 seconds
[2025-02-23T21:07:28.462+0000] {processor.py:186} INFO - Started process (PID=569) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:07:28.572+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:07:28.575+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:07:28.574+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:07:29.643+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:07:29.726+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:07:29.725+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:07:29.810+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:07:29.809+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:00:00+00:00, run_after=2025-02-23 21:10:00+00:00
[2025-02-23T21:07:29.873+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.428 seconds
[2025-02-23T21:08:01.050+0000] {processor.py:186} INFO - Started process (PID=571) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:08:01.069+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:08:01.081+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:08:01.076+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:08:02.434+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:08:02.561+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:08:02.560+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:08:02.668+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:08:02.667+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:00:00+00:00, run_after=2025-02-23 21:10:00+00:00
[2025-02-23T21:08:02.765+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.046 seconds
[2025-02-23T21:08:33.451+0000] {processor.py:186} INFO - Started process (PID=573) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:08:33.454+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:08:33.459+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:08:33.457+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:08:34.008+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:08:34.095+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:08:34.094+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:08:34.179+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:08:34.179+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:00:00+00:00, run_after=2025-02-23 21:10:00+00:00
[2025-02-23T21:08:34.464+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.031 seconds
[2025-02-23T21:09:04.694+0000] {processor.py:186} INFO - Started process (PID=575) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:09:04.843+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:09:04.854+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:09:04.853+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:09:06.018+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:09:06.099+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:09:06.099+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:09:06.175+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:09:06.174+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:00:00+00:00, run_after=2025-02-23 21:10:00+00:00
[2025-02-23T21:09:06.243+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.581 seconds
[2025-02-23T21:09:37.224+0000] {processor.py:186} INFO - Started process (PID=577) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:09:37.229+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:09:37.238+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:09:37.236+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:09:38.690+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:09:39.004+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:09:39.003+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:09:39.437+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:09:39.431+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:00:00+00:00, run_after=2025-02-23 21:10:00+00:00
[2025-02-23T21:09:39.602+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.437 seconds
[2025-02-23T21:10:04.956+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:10:05.114+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:00:00+00:00 [queued]>
[2025-02-23T21:10:05.141+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:00:00+00:00 [queued]>
[2025-02-23T21:10:05.143+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:10:05.191+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 21:00:00+00:00
[2025-02-23T21:10:05.209+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1895) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:10:05.217+0000] {standard_task_runner.py:72} INFO - Started process 1896 to run task
[2025-02-23T21:10:05.217+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T21:00:00+00:00', '--job-id', '81', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpjcg8rq_6']
[2025-02-23T21:10:05.221+0000] {standard_task_runner.py:105} INFO - Job 81: Subtask process_landzone
[2025-02-23T21:10:05.388+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:00:00+00:00 [running]> on host b06401153325
[2025-02-23T21:10:05.758+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:00:00+00:00'
[2025-02-23T21:10:05.760+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:10:05.807+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:10:05.809+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T21:10:05.835+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:10:05.849+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:10:18.154+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T21:10:18.158+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:18.169+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T21:10:18.172+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:18.175+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T21:10:18.189+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:18.199+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T21:10:18.202+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:18.205+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T21:10:18.208+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:18.217+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T21:10:18.220+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:18.225+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T21:10:18.227+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:18.234+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T21:10:18.237+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:18.240+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T21:10:18.244+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:18.251+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T21:10:18.254+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:18.263+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T21:10:18.266+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:18.270+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T21:10:18.272+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:10:19.453+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:10:19.456+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:10:19.626+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:10:19.631+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T21:00:00+00:00, execution_date=20250223T210000, start_date=20250223T211005, end_date=20250223T211019
[2025-02-23T21:10:19.872+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:10:20.098+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:10:20.103+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:10:25.368+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:10:25.444+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T21:00:00+00:00 [queued]>
[2025-02-23T21:10:25.476+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T21:00:00+00:00 [queued]>
[2025-02-23T21:10:25.478+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:10:25.552+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 21:00:00+00:00
[2025-02-23T21:10:25.578+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1941) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:10:25.588+0000] {standard_task_runner.py:72} INFO - Started process 1942 to run task
[2025-02-23T21:10:25.591+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T21:00:00+00:00', '--job-id', '82', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpks_nbkgr']
[2025-02-23T21:10:25.601+0000] {standard_task_runner.py:105} INFO - Job 82: Subtask process_bronze
[2025-02-23T21:10:25.810+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T21:00:00+00:00 [running]> on host b06401153325
[2025-02-23T21:10:26.249+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:00:00+00:00'
[2025-02-23T21:10:26.251+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:10:26.295+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:10:26.300+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T21:10:26.328+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:10:26.344+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:10:36.899+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:10:37.062+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:10:37.152+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:10:37.159+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T21:10:37.507+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 19:10:00+00:00
[2025-02-23T21:10:37.645+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1947) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:10:37.679+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T19:10:00+00:00', '--job-id', '83', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmptumaawki']
[2025-02-23T21:10:37.690+0000] {standard_task_runner.py:72} INFO - Started process 1952 to run task
[2025-02-23T21:10:37.705+0000] {standard_task_runner.py:105} INFO - Job 83: Subtask process_landzone
[2025-02-23T21:10:40.737+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T19:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T19:10:00+00:00'
[2025-02-23T21:10:40.753+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:10:40.978+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:10:40.988+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T21:10:41.079+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:10:41.128+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:10:43.294+0000] {processor.py:186} INFO - Started process (PID=581) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:10:43.351+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:10:43.381+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:10:43.376+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:10:46.486+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:10:47.021+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:10:47.020+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:10:48.187+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:10:48.138+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:10:48.444+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 5.310 seconds
[2025-02-23T21:10:51.208+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.csv' baixado para 'temp_landzone_fornecedores_db1.csv'.
[2025-02-23T21:10:51.219+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db1.csv' enviado para 'datalake/bronze/db1/fornecedores_db1.csv'.
[2025-02-23T21:10:51.236+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.csv' baixado para 'temp_landzone_fornecedores_db2.csv'.
[2025-02-23T21:10:51.241+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db2.csv' enviado para 'datalake/bronze/db2/fornecedores_db2.csv'.
[2025-02-23T21:10:51.292+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.csv' baixado para 'temp_landzone_fornecedores_db3.csv'.
[2025-02-23T21:10:51.307+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db3.csv' enviado para 'datalake/bronze/db3/fornecedores_db3.csv'.
[2025-02-23T21:10:51.310+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.csv' baixado para 'temp_landzone_produtos_db1.csv'.
[2025-02-23T21:10:51.318+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db1.csv' enviado para 'datalake/bronze/db1/produtos_db1.csv'.
[2025-02-23T21:10:51.322+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.csv' baixado para 'temp_landzone_produtos_db2.csv'.
[2025-02-23T21:10:51.324+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db2.csv' enviado para 'datalake/bronze/db2/produtos_db2.csv'.
[2025-02-23T21:10:51.327+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.csv' baixado para 'temp_landzone_produtos_db3.csv'.
[2025-02-23T21:10:51.334+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db3.csv' enviado para 'datalake/bronze/db3/produtos_db3.csv'.
[2025-02-23T21:10:51.337+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.csv' baixado para 'temp_landzone_vendas_db1.csv'.
[2025-02-23T21:10:51.339+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db1.csv' enviado para 'datalake/bronze/db1/vendas_db1.csv'.
[2025-02-23T21:10:51.342+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.csv' baixado para 'temp_landzone_vendas_db2.csv'.
[2025-02-23T21:10:51.345+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db2.csv' enviado para 'datalake/bronze/db2/vendas_db2.csv'.
[2025-02-23T21:10:51.357+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.csv' baixado para 'temp_landzone_vendas_db3.csv'.
[2025-02-23T21:10:51.362+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db3.csv' enviado para 'datalake/bronze/db3/vendas_db3.csv'.
[2025-02-23T21:10:51.369+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.csv' baixado para 'temp_landzone_vendedores_db1.csv'.
[2025-02-23T21:10:51.375+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db1.csv' enviado para 'datalake/bronze/db1/vendedores_db1.csv'.
[2025-02-23T21:10:51.377+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.csv' baixado para 'temp_landzone_vendedores_db2.csv'.
[2025-02-23T21:10:51.393+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db2.csv' enviado para 'datalake/bronze/db2/vendedores_db2.csv'.
[2025-02-23T21:10:51.404+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.csv' baixado para 'temp_landzone_vendedores_db3.csv'.
[2025-02-23T21:10:51.408+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db3.csv' enviado para 'datalake/bronze/db3/vendedores_db3.csv'.
[2025-02-23T21:10:52.769+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:10:52.773+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:10:53.190+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:10:53.194+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T21:00:00+00:00, execution_date=20250223T210000, start_date=20250223T211025, end_date=20250223T211053
[2025-02-23T21:10:53.744+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:10:53.844+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:11:00.454+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:11:00.700+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T21:00:00+00:00 [queued]>
[2025-02-23T21:11:00.825+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T21:00:00+00:00 [queued]>
[2025-02-23T21:11:00.833+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:11:01.224+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 21:00:00+00:00
[2025-02-23T21:11:01.273+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=1961) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:11:01.357+0000] {standard_task_runner.py:72} INFO - Started process 1974 to run task
[2025-02-23T21:11:01.422+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T21:00:00+00:00', '--job-id', '84', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpanfg2vti']
[2025-02-23T21:11:01.433+0000] {standard_task_runner.py:105} INFO - Job 84: Subtask process_silver
[2025-02-23T21:11:16.396+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:11:16.570+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:11:16.633+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:11:16.636+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:11:16.777+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 19:10:00+00:00
[2025-02-23T21:11:16.811+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2007) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:11:16.824+0000] {standard_task_runner.py:72} INFO - Started process 2032 to run task
[2025-02-23T21:11:16.825+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T19:10:00+00:00', '--job-id', '85', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpmoteb091']
[2025-02-23T21:11:16.837+0000] {standard_task_runner.py:105} INFO - Job 85: Subtask process_bronze
[2025-02-23T21:11:18.599+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T21:11:18.602+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:11:03.894+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:03.894+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00']
[2025-02-23T21:11:18.616+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/fornecedores_db1.csv
[2025-02-23T21:11:18.619+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/produtos_db1.csv' baixado para 'temp_bronze_produtos_db1.csv'.
[2025-02-23T21:11:18.624+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:11:03.894+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:03.894+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00']
[2025-02-23T21:11:18.741+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/produtos_db1.csv
[2025-02-23T21:11:18.776+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T19:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T19:10:00+00:00'
[2025-02-23T21:11:18.820+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:11:18.817+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendas_db1.csv' baixado para 'temp_bronze_vendas_db1.csv'.
[2025-02-23T21:11:18.829+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:11:03.894+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:03.894+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00']
[2025-02-23T21:11:18.873+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendas_db1.csv
[2025-02-23T21:11:18.887+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendedores_db1.csv' baixado para 'temp_bronze_vendedores_db1.csv'.
[2025-02-23T21:11:18.891+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:11:03.894+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:03.894+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00']
[2025-02-23T21:11:18.895+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendedores_db1.csv
[2025-02-23T21:11:18.900+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/fornecedores_db2.csv' baixado para 'temp_bronze_fornecedores_db2.csv'.
[2025-02-23T21:11:18.906+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:11:03.894+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:03.894+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00']
[2025-02-23T21:11:18.936+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/fornecedores_db2.csv
[2025-02-23T21:11:18.994+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/produtos_db2.csv' baixado para 'temp_bronze_produtos_db2.csv'.
[2025-02-23T21:11:19.023+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:11:03.894+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:03.894+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00']
[2025-02-23T21:11:19.036+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/produtos_db2.csv
[2025-02-23T21:11:19.076+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendas_db2.csv' baixado para 'temp_bronze_vendas_db2.csv'.
[2025-02-23T21:11:19.069+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:11:19.078+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:11:03.894+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:03.894+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00']
[2025-02-23T21:11:19.100+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendas_db2.csv
[2025-02-23T21:11:19.095+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T21:11:19.201+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendedores_db2.csv' baixado para 'temp_bronze_vendedores_db2.csv'.
[2025-02-23T21:11:19.203+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:11:03.894+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:03.894+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00']
[2025-02-23T21:11:19.228+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendedores_db2.csv
[2025-02-23T21:11:19.243+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/fornecedores_db3.csv' baixado para 'temp_bronze_fornecedores_db3.csv'.
[2025-02-23T21:11:19.250+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:11:03.894+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:03.894+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00']
[2025-02-23T21:11:19.273+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/fornecedores_db3.csv
[2025-02-23T21:11:19.511+0000] {processor.py:186} INFO - Started process (PID=583) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:11:19.520+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:11:19.534+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:19.526+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
00] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/produtos_db3.csv
[2025-02-23T21:11:19.426+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendas_db3.csv' baixado para 'temp_bronze_vendas_db3.csv'.
[2025-02-23T21:11:19.429+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:11:17.338+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T19:10:00+00:00 [running]> on host b06401153325']
[2025-02-23T21:11:19.433+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendas_db3.csv
[2025-02-23T21:11:19.435+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendedores_db3.csv' baixado para 'temp_bronze_vendedores_db3.csv'.
[2025-02-23T21:11:19.448+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:11:17.338+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T19:10:00+00:00 [running]> on host b06401153325']
[2025-02-23T21:11:19.452+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendedores_db3.csv
[2025-02-23T21:11:19.474+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:11:19.505+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:11:19.985+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:11:19.995+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:11:23.037+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:11:23.519+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:23.515+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:11:23.840+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:23.838+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:11:24.031+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.595 seconds
[2025-02-23T21:11:28.354+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:11:28.658+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T21:00:00+00:00 [queued]>
[2025-02-23T21:11:28.759+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T21:00:00+00:00 [queued]>
[2025-02-23T21:11:28.761+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:11:28.904+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_analytics> on 2025-02-23 21:00:00+00:00
[2025-02-23T21:11:28.931+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2049) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:11:28.939+0000] {standard_task_runner.py:72} INFO - Started process 2054 to run task
[2025-02-23T21:11:28.942+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_analytics', 'scheduled__2025-02-23T21:00:00+00:00', '--job-id', '86', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpz0x6paun']
[2025-02-23T21:11:28.949+0000] {standard_task_runner.py:105} INFO - Job 86: Subtask process_analytics
[2025-02-23T21:11:36.276+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v1.py took 1.859 seconds
[2025-02-23T21:11:48.808+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.csv' baixado para 'temp_landzone_fornecedores_db1.csv'.
[2025-02-23T21:11:48.814+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db1.csv' enviado para 'datalake/bronze/db1/fornecedores_db1.csv'.
[2025-02-23T21:11:48.837+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.csv' baixado para 'temp_landzone_fornecedores_db2.csv'.
[2025-02-23T21:11:48.847+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db2.csv' enviado para 'datalake/bronze/db2/fornecedores_db2.csv'.
[2025-02-23T21:11:48.851+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.csv' baixado para 'temp_landzone_fornecedores_db3.csv'.
[2025-02-23T21:11:48.860+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db3.csv' enviado para 'datalake/bronze/db3/fornecedores_db3.csv'.
[2025-02-23T21:11:48.867+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.csv' baixado para 'temp_landzone_produtos_db1.csv'.
[2025-02-23T21:11:48.870+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db1.csv' enviado para 'datalake/bronze/db1/produtos_db1.csv'.
[2025-02-23T21:11:48.882+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.csv' baixado para 'temp_landzone_produtos_db2.csv'.
[2025-02-23T21:11:48.891+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db2.csv' enviado para 'datalake/bronze/db2/produtos_db2.csv'.
[2025-02-23T21:11:48.897+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.csv' baixado para 'temp_landzone_produtos_db3.csv'.
[2025-02-23T21:11:48.900+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db3.csv' enviado para 'datalake/bronze/db3/produtos_db3.csv'.
[2025-02-23T21:11:48.906+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.csv' baixado para 'temp_landzone_vendas_db1.csv'.
[2025-02-23T21:11:48.908+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db1.csv' enviado para 'datalake/bronze/db1/vendas_db1.csv'.
[2025-02-23T21:11:48.914+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.csv' baixado para 'temp_landzone_vendas_db2.csv'.
[2025-02-23T21:11:48.918+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db2.csv' enviado para 'datalake/bronze/db2/vendas_db2.csv'.
[2025-02-23T21:11:48.922+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.csv' baixado para 'temp_landzone_vendas_db3.csv'.
[2025-02-23T21:11:48.925+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db3.csv' enviado para 'datalake/bronze/db3/vendas_db3.csv'.
[2025-02-23T21:11:48.931+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.csv' baixado para 'temp_landzone_vendedores_db1.csv'.
[2025-02-23T21:11:48.936+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db1.csv' enviado para 'datalake/bronze/db1/vendedores_db1.csv'.
[2025-02-23T21:11:48.939+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.csv' baixado para 'temp_landzone_vendedores_db2.csv'.
[2025-02-23T21:11:48.946+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db2.csv' enviado para 'datalake/bronze/db2/vendedores_db2.csv'.
[2025-02-23T21:11:48.951+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.csv' baixado para 'temp_landzone_vendedores_db3.csv'.
[2025-02-23T21:11:48.967+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db3.csv' enviado para 'datalake/bronze/db3/vendedores_db3.csv'.
[2025-02-23T21:11:49.590+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:11:49.595+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:11:49.790+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:11:49.803+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T19:10:00+00:00, execution_date=20250223T191000, start_date=20250223T211116, end_date=20250223T211149
[2025-02-23T21:11:50.252+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:11:50.608+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:11:50.635+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:11:53.860+0000] {subprocess.py:106} INFO -  Bucket 'datalake' j existe.
[2025-02-23T21:11:53.872+0000] {subprocess.py:106} INFO -  'silveranalitics/' inicializado no bucket 'datalake'.
[2025-02-23T21:11:53.884+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db1.csv' carregado de silver/db1.
[2025-02-23T21:11:53.887+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db2.csv' carregado de silver/db2.
[2025-02-23T21:11:53.900+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db3.csv' carregado de silver/db3.
[2025-02-23T21:11:53.904+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:11:53.910+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db1.csv' carregado de silver/db1.
[2025-02-23T21:11:53.914+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db2.csv' carregado de silver/db2.
[2025-02-23T21:11:53.921+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db3.csv' carregado de silver/db3.
[2025-02-23T21:11:53.933+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:11:53.944+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db1.csv' carregado de silver/db1.
[2025-02-23T21:11:53.951+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db2.csv' carregado de silver/db2.
[2025-02-23T21:11:53.961+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db3.csv' carregado de silver/db3.
[2025-02-23T21:11:53.970+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:11:53.976+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db1.csv' carregado de silver/db1.
[2025-02-23T21:11:53.980+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db2.csv' carregado de silver/db2.
[2025-02-23T21:11:53.985+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db3.csv' carregado de silver/db3.
[2025-02-23T21:11:53.992+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:11:54.480+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:11:54.485+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:11:54.651+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:11:54.657+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_analytics, run_id=scheduled__2025-02-23T21:00:00+00:00, execution_date=20250223T210000, start_date=20250223T211128, end_date=20250223T211154
[2025-02-23T21:11:54.876+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:11:55.108+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:11:55.420+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:11:57.359+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:11:57.834+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:57.833+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:11:58.028+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:11:58.026+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:11:58.155+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.747 seconds
[2025-02-23T21:12:00.250+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:12:00.438+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:12:00.506+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:12:00.511+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:12:00.617+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 19:10:00+00:00
[2025-02-23T21:12:00.649+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2069) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:12:00.662+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T19:10:00+00:00', '--job-id', '87', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp9ijxwela']
[2025-02-23T21:12:00.665+0000] {standard_task_runner.py:72} INFO - Started process 2077 to run task
[2025-02-23T21:12:00.669+0000] {standard_task_runner.py:105} INFO - Job 87: Subtask process_silver
[2025-02-23T21:12:04.518+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:12:04.671+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T21:00:00+00:00 [queued]>
[2025-02-23T21:12:04.765+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T21:00:00+00:00 [queued]>
[2025-02-23T21:12:04.768+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:12:04.964+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_gold> on 2025-02-23 21:00:00+00:00
[2025-02-23T21:12:04.996+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2076) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:12:05.012+0000] {standard_task_runner.py:72} INFO - Started process 2085 to run task
[2025-02-23T21:12:05.012+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_gold', 'scheduled__2025-02-23T21:00:00+00:00', '--job-id', '88', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpeo7fttdr']
[2025-02-23T21:12:05.027+0000] {standard_task_runner.py:105} INFO - Job 88: Subtask process_gold
[2025-02-23T21:12:22.624+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T21:12:22.632+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.636+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/fornecedores_db1.csv
[2025-02-23T21:12:22.638+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/produtos_db1.csv' baixado para 'temp_bronze_produtos_db1.csv'.
[2025-02-23T21:12:22.641+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.645+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/produtos_db1.csv
[2025-02-23T21:12:22.651+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendas_db1.csv' baixado para 'temp_bronze_vendas_db1.csv'.
[2025-02-23T21:12:22.654+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.658+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendas_db1.csv
[2025-02-23T21:12:22.667+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendedores_db1.csv' baixado para 'temp_bronze_vendedores_db1.csv'.
[2025-02-23T21:12:22.670+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.673+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendedores_db1.csv
[2025-02-23T21:12:22.679+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/fornecedores_db2.csv' baixado para 'temp_bronze_fornecedores_db2.csv'.
[2025-02-23T21:12:22.688+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.705+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/fornecedores_db2.csv
[2025-02-23T21:12:22.738+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/produtos_db2.csv' baixado para 'temp_bronze_produtos_db2.csv'.
[2025-02-23T21:12:22.753+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.755+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/produtos_db2.csv
[2025-02-23T21:12:22.770+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendas_db2.csv' baixado para 'temp_bronze_vendas_db2.csv'.
[2025-02-23T21:12:22.778+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.786+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendas_db2.csv
[2025-02-23T21:12:22.790+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendedores_db2.csv' baixado para 'temp_bronze_vendedores_db2.csv'.
[2025-02-23T21:12:22.798+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.802+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendedores_db2.csv
[2025-02-23T21:12:22.805+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/fornecedores_db3.csv' baixado para 'temp_bronze_fornecedores_db3.csv'.
[2025-02-23T21:12:22.807+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.809+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/fornecedores_db3.csv
[2025-02-23T21:12:22.819+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/produtos_db3.csv' baixado para 'temp_bronze_produtos_db3.csv'.
[2025-02-23T21:12:22.824+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.830+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/produtos_db3.csv
[2025-02-23T21:12:22.835+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendas_db3.csv' baixado para 'temp_bronze_vendas_db3.csv'.
[2025-02-23T21:12:22.838+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.851+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendas_db3.csv
[2025-02-23T21:12:22.854+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendedores_db3.csv' baixado para 'temp_bronze_vendedores_db3.csv'.
[2025-02-23T21:12:22.905+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:12:07.114+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...']
[2025-02-23T21:12:22.914+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendedores_db3.csv
[2025-02-23T21:12:23.425+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:12:23.436+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:12:23.759+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:12:23.772+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T19:10:00+00:00, execution_date=20250223T191000, start_date=20250223T211200, end_date=20250223T211223
[2025-02-23T21:12:24.506+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:12:24.623+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:12:26.208+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsFornecedor.csv' carregado.
[2025-02-23T21:12:26.244+0000] {subprocess.py:106} INFO -  'gold/goldFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:12:26.266+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsProdutos.csv' carregado.
[2025-02-23T21:12:26.278+0000] {subprocess.py:106} INFO -  'gold/goldProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:12:26.282+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsVendas.csv' carregado.
[2025-02-23T21:12:26.294+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' carregado para *join*.
[2025-02-23T21:12:26.301+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' carregado para *join*.
[2025-02-23T21:12:26.315+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' carregado para *join*.
[2025-02-23T21:12:26.333+0000] {subprocess.py:106} INFO -  'gold/goldVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:12:26.356+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsVendedor.csv' carregado.
[2025-02-23T21:12:26.371+0000] {subprocess.py:106} INFO -  'gold/goldVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:12:27.237+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:12:27.240+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:12:27.420+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:12:27.429+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_gold, run_id=scheduled__2025-02-23T21:00:00+00:00, execution_date=20250223T210000, start_date=20250223T211204, end_date=20250223T211227
[2025-02-23T21:12:27.729+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:12:27.933+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:12:27.952+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:12:29.027+0000] {processor.py:186} INFO - Started process (PID=587) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:12:29.054+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:12:29.119+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:12:29.068+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:12:30.991+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:12:31.130+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:12:31.129+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:12:31.616+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:12:31.615+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:12:31.909+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 3.173 seconds
[2025-02-23T21:12:33.011+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:12:33.153+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:12:33.229+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:12:33.232+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:12:33.323+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_analytics> on 2025-02-23 19:10:00+00:00
[2025-02-23T21:12:33.358+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2140) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:12:33.372+0000] {standard_task_runner.py:72} INFO - Started process 2144 to run task
[2025-02-23T21:12:33.373+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_analytics', 'scheduled__2025-02-23T19:10:00+00:00', '--job-id', '89', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp8fz_4mmc']
[2025-02-23T21:12:33.384+0000] {standard_task_runner.py:105} INFO - Job 89: Subtask process_analytics
[2025-02-23T21:12:52.146+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:12:52.395+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:12:52.515+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T19:10:00+00:00 [queued]>
[2025-02-23T21:12:52.518+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:12:52.622+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_gold> on 2025-02-23 19:10:00+00:00
[2025-02-23T21:12:52.716+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2165) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:12:52.728+0000] {standard_task_runner.py:72} INFO - Started process 2166 to run task
[2025-02-23T21:12:52.729+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_gold', 'scheduled__2025-02-23T19:10:00+00:00', '--job-id', '90', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp8kt8ln3l']
[2025-02-23T21:12:52.733+0000] {standard_task_runner.py:105} INFO - Job 90: Subtask process_gold
[2025-02-23T21:13:02.930+0000] {processor.py:186} INFO - Started process (PID=589) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:13:02.940+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:13:02.951+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:13:02.948+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:13:04.145+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:13:04.318+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:13:04.317+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:13:04.457+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:13:04.453+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:13:04.566+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.734 seconds
[2025-02-23T21:13:34.691+0000] {processor.py:186} INFO - Started process (PID=591) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:13:34.693+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:13:34.696+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:13:34.695+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:13:35.115+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:13:35.188+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:13:35.188+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:13:35.252+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:13:35.251+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:13:35.295+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.622 seconds
[2025-02-23T21:14:05.704+0000] {processor.py:186} INFO - Started process (PID=593) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:14:05.706+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:14:05.710+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:14:05.709+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:14:06.068+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:14:06.127+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:14:06.127+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:14:06.183+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:14:06.182+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:14:06.220+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.529 seconds
[2025-02-23T21:14:36.297+0000] {processor.py:186} INFO - Started process (PID=595) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:14:36.299+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:14:36.303+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:14:36.302+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:14:37.047+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:14:37.117+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:14:37.116+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:14:37.198+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:14:37.198+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:14:37.258+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.977 seconds
[2025-02-23T21:15:08.023+0000] {processor.py:186} INFO - Started process (PID=597) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:15:08.379+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:15:08.384+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:15:08.383+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:15:08.740+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:15:08.804+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:15:08.803+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:15:08.904+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:15:08.902+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:15:08.962+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.952 seconds
[2025-02-23T21:15:39.418+0000] {processor.py:186} INFO - Started process (PID=599) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:15:39.420+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:15:39.424+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:15:39.423+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:15:39.756+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:15:39.807+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:15:39.806+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:15:39.855+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:15:39.855+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:15:39.899+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.494 seconds
[2025-02-23T21:16:09.991+0000] {processor.py:186} INFO - Started process (PID=601) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:16:09.994+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:16:09.999+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:16:09.998+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:16:10.522+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:16:10.593+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:16:10.593+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:16:10.651+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:16:10.650+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:16:10.694+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.724 seconds
[2025-02-23T21:16:40.752+0000] {processor.py:186} INFO - Started process (PID=603) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:16:40.754+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:16:40.757+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:16:40.756+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:16:41.190+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:16:41.286+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:16:41.286+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:16:41.353+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:16:41.352+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:16:41.397+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.659 seconds
[2025-02-23T21:17:11.831+0000] {processor.py:186} INFO - Started process (PID=605) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:17:11.833+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:17:11.836+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:17:11.835+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:17:12.178+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:17:12.230+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:17:12.229+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:17:12.277+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:17:12.276+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:17:12.317+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.499 seconds
[2025-02-23T21:17:42.517+0000] {processor.py:186} INFO - Started process (PID=607) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:17:42.524+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:17:42.531+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:17:42.530+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:17:42.913+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:17:42.969+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:17:42.968+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:17:43.020+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:17:43.019+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:17:43.061+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.562 seconds
[2025-02-23T21:18:13.849+0000] {processor.py:186} INFO - Started process (PID=609) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:18:13.852+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:18:13.855+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:18:13.854+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:18:14.196+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:18:14.247+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:18:14.247+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:18:14.296+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:18:14.295+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:18:14.339+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.503 seconds
[2025-02-23T21:18:44.467+0000] {processor.py:186} INFO - Started process (PID=611) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:18:44.469+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:18:44.472+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:18:44.471+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:18:44.823+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:18:44.880+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:18:44.879+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:18:44.927+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:18:44.927+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:18:44.967+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.513 seconds
[2025-02-23T21:19:15.055+0000] {processor.py:186} INFO - Started process (PID=613) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:19:15.058+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:19:15.060+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:19:15.060+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:19:15.423+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:19:15.508+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:19:15.508+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:19:15.565+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:19:15.565+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:19:15.605+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.562 seconds
[2025-02-23T21:19:46.050+0000] {processor.py:186} INFO - Started process (PID=615) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:19:46.170+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:19:46.174+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:19:46.173+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:19:46.620+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:19:46.673+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:19:46.673+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:19:46.720+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:19:46.720+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:10:00+00:00, run_after=2025-02-23 21:20:00+00:00
[2025-02-23T21:19:46.761+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.723 seconds
[2025-02-23T21:20:02.483+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:20:02.531+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:10:00+00:00 [queued]>
[2025-02-23T21:20:02.552+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:10:00+00:00 [queued]>
[2025-02-23T21:20:02.554+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:20:02.588+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 21:10:00+00:00
[2025-02-23T21:20:02.599+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2187) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:20:02.603+0000] {standard_task_runner.py:72} INFO - Started process 2188 to run task
[2025-02-23T21:20:02.603+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T21:10:00+00:00', '--job-id', '91', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpb8v6g5wc']
[2025-02-23T21:20:02.606+0000] {standard_task_runner.py:105} INFO - Job 91: Subtask process_landzone
[2025-02-23T21:20:02.700+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:10:00+00:00 [running]> on host b06401153325
[2025-02-23T21:20:02.999+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:10:00+00:00'
[2025-02-23T21:20:03.001+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:20:03.032+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:20:03.034+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T21:20:03.054+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:20:03.064+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:20:08.962+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T21:20:08.965+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:08.966+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T21:20:08.968+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:08.969+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T21:20:08.971+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:08.973+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T21:20:08.975+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:08.976+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T21:20:08.978+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:08.979+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T21:20:08.981+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:08.983+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T21:20:08.986+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:08.988+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T21:20:08.990+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:08.992+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T21:20:08.993+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:08.995+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T21:20:08.996+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:08.998+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T21:20:09.000+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:09.003+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T21:20:09.007+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:20:09.146+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:20:09.147+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:20:09.222+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:20:09.223+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T21:10:00+00:00, execution_date=20250223T211000, start_date=20250223T212002, end_date=20250223T212009
[2025-02-23T21:20:09.330+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:20:09.395+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:20:09.402+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:20:11.521+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:20:11.579+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T21:10:00+00:00 [queued]>
[2025-02-23T21:20:11.616+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T21:10:00+00:00 [queued]>
[2025-02-23T21:20:11.617+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:20:11.651+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 21:10:00+00:00
[2025-02-23T21:20:11.662+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2233) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:20:11.666+0000] {standard_task_runner.py:72} INFO - Started process 2234 to run task
[2025-02-23T21:20:11.667+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T21:10:00+00:00', '--job-id', '92', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpe2v0hl3m']
[2025-02-23T21:20:11.670+0000] {standard_task_runner.py:105} INFO - Job 92: Subtask process_bronze
[2025-02-23T21:20:11.791+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T21:10:00+00:00 [running]> on host b06401153325
[2025-02-23T21:20:12.106+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:10:00+00:00'
[2025-02-23T21:20:12.108+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:20:12.146+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:20:12.148+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T21:20:12.168+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:20:12.179+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:20:16.986+0000] {processor.py:186} INFO - Started process (PID=617) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:20:16.990+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:20:16.999+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:16.998+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:20:17.849+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:20:17.918+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:17.917+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:20:17.992+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:17.991+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:20:18.084+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.145 seconds
_v1.py
[2025-02-23T21:20:18.219+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.218+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:20:18.306+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.305+0000] {dag.py:4180} INFO - Setting next_dagrun for scheduler_v1 to 2025-02-22 00:00:00+00:00, run_after=2025-02-23 00:00:00+00:00
""
[2025-02-23T23:13:29.363+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
""
[2025-02-23T21:20:20.780+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.csv' baixado para 'temp_landzone_fornecedores_db1.csv'.
[2025-02-23T21:20:20.782+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db1.csv' enviado para 'datalake/bronze/db1/fornecedores_db1.csv'.
[2025-02-23T21:20:20.785+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.csv' baixado para 'temp_landzone_fornecedores_db2.csv'.
[2025-02-23T21:20:20.791+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db2.csv' enviado para 'datalake/bronze/db2/fornecedores_db2.csv'.
[2025-02-23T21:20:20.802+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.csv' baixado para 'temp_landzone_fornecedores_db3.csv'.
[2025-02-23T21:20:20.806+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db3.csv' enviado para 'datalake/bronze/db3/fornecedores_db3.csv'.
[2025-02-23T21:20:20.816+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.csv' baixado para 'temp_landzone_produtos_db1.csv'.
[2025-02-23T21:20:20.819+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db1.csv' enviado para 'datalake/bronze/db1/produtos_db1.csv'.
[2025-02-23T21:20:20.822+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.csv' baixado para 'temp_landzone_produtos_db2.csv'.
[2025-02-23T21:20:20.835+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db2.csv' enviado para 'datalake/bronze/db2/produtos_db2.csv'.
[2025-02-23T21:20:20.851+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.csv' baixado para 'temp_landzone_produtos_db3.csv'.
[2025-02-23T21:20:20.854+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db3.csv' enviado para 'datalake/bronze/db3/produtos_db3.csv'.
[2025-02-23T21:20:20.858+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.csv' baixado para 'temp_landzone_vendas_db1.csv'.
[2025-02-23T21:20:20.868+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db1.csv' enviado para 'datalake/bronze/db1/vendas_db1.csv'.
[2025-02-23T21:20:20.871+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.csv' baixado para 'temp_landzone_vendas_db2.csv'.
[2025-02-23T21:20:20.874+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db2.csv' enviado para 'datalake/bronze/db2/vendas_db2.csv'.
[2025-02-23T21:20:20.882+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.csv' baixado para 'temp_landzone_vendas_db3.csv'.
[2025-02-23T21:20:20.886+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db3.csv' enviado para 'datalake/bronze/db3/vendas_db3.csv'.
[2025-02-23T21:20:20.889+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.csv' baixado para 'temp_landzone_vendedores_db1.csv'.
[2025-02-23T21:20:20.895+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db1.csv' enviado para 'datalake/bronze/db1/vendedores_db1.csv'.
[2025-02-23T21:20:20.899+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.csv' baixado para 'temp_landzone_vendedores_db2.csv'.
[2025-02-23T21:20:20.901+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db2.csv' enviado para 'datalake/bronze/db2/vendedores_db2.csv'.
[2025-02-23T21:20:20.904+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.csv' baixado para 'temp_landzone_vendedores_db3.csv'.
[2025-02-23T21:20:20.907+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db3.csv' enviado para 'datalake/bronze/db3/vendedores_db3.csv'.
[2025-02-23T21:20:21.264+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:20:21.266+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:20:21.403+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:20:21.413+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T21:10:00+00:00, execution_date=20250223T211000, start_date=20250223T212011, end_date=20250223T212021
[2025-02-23T21:20:21.716+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:20:21.790+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:20:21.800+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:20:24.064+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:20:24.128+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T21:10:00+00:00 [queued]>
[2025-02-23T21:20:24.155+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T21:10:00+00:00 [queued]>
[2025-02-23T21:20:24.156+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:20:24.192+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 21:10:00+00:00
[2025-02-23T21:20:24.204+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2243) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:20:24.208+0000] {standard_task_runner.py:72} INFO - Started process 2244 to run task
[2025-02-23T21:20:24.209+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T21:10:00+00:00', '--job-id', '93', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpdyx1vx6y']
[2025-02-23T21:20:24.212+0000] {standard_task_runner.py:105} INFO - Job 93: Subtask process_silver
[2025-02-23T21:20:24.321+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T21:10:00+00:00 [running]> on host b06401153325
[2025-02-23T21:20:24.540+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:10:00+00:00'
[2025-02-23T21:20:24.542+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:20:24.579+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:20:24.582+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T21:20:24.607+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:20:24.617+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T21:20:29.349+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T21:20:29.352+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.354+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/fornecedores_db1.csv
[2025-02-23T21:20:29.356+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/produtos_db1.csv' baixado para 'temp_bronze_produtos_db1.csv'.
[2025-02-23T21:20:29.359+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.379+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/produtos_db1.csv
[2025-02-23T21:20:29.381+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendas_db1.csv' baixado para 'temp_bronze_vendas_db1.csv'.
[2025-02-23T21:20:29.383+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.386+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendas_db1.csv
[2025-02-23T21:20:29.390+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendedores_db1.csv' baixado para 'temp_bronze_vendedores_db1.csv'.
[2025-02-23T21:20:29.392+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.394+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendedores_db1.csv
[2025-02-23T21:20:29.398+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/fornecedores_db2.csv' baixado para 'temp_bronze_fornecedores_db2.csv'.
[2025-02-23T21:20:29.400+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.403+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/fornecedores_db2.csv
[2025-02-23T21:20:29.406+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/produtos_db2.csv' baixado para 'temp_bronze_produtos_db2.csv'.
[2025-02-23T21:20:29.409+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.416+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/produtos_db2.csv
[2025-02-23T21:20:29.419+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendas_db2.csv' baixado para 'temp_bronze_vendas_db2.csv'.
[2025-02-23T21:20:29.421+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.423+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendas_db2.csv
[2025-02-23T21:20:29.426+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendedores_db2.csv' baixado para 'temp_bronze_vendedores_db2.csv'.
[2025-02-23T21:20:29.428+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.430+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendedores_db2.csv
[2025-02-23T21:20:29.433+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/fornecedores_db3.csv' baixado para 'temp_bronze_fornecedores_db3.csv'.
[2025-02-23T21:20:29.436+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.439+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/fornecedores_db3.csv
[2025-02-23T21:20:29.447+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/produtos_db3.csv' baixado para 'temp_bronze_produtos_db3.csv'.
[2025-02-23T21:20:29.449+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.451+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/produtos_db3.csv
[2025-02-23T21:20:29.455+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendas_db3.csv' baixado para 'temp_bronze_vendas_db3.csv'.
[2025-02-23T21:20:29.457+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.459+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendas_db3.csv
[2025-02-23T21:20:29.461+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendedores_db3.csv' baixado para 'temp_bronze_vendedores_db3.csv'.
[2025-02-23T21:20:29.463+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:20:18.011+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:18.009+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:20:29.465+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendedores_db3.csv
[2025-02-23T21:20:29.719+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:20:29.721+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:20:29.792+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:20:29.794+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T21:10:00+00:00, execution_date=20250223T211000, start_date=20250223T212024, end_date=20250223T212029
[2025-02-23T21:20:29.904+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:20:29.995+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:20:30.007+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:20:32.292+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:20:32.412+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T21:10:00+00:00 [queued]>
[2025-02-23T21:20:32.459+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T21:10:00+00:00 [queued]>
[2025-02-23T21:20:32.463+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:20:32.555+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_analytics> on 2025-02-23 21:10:00+00:00
[2025-02-23T21:20:32.577+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2289) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:20:32.582+0000] {standard_task_runner.py:72} INFO - Started process 2290 to run task
[2025-02-23T21:20:32.584+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_analytics', 'scheduled__2025-02-23T21:10:00+00:00', '--job-id', '94', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp06fz83ce']
[2025-02-23T21:20:32.589+0000] {standard_task_runner.py:105} INFO - Job 94: Subtask process_analytics
[2025-02-23T21:20:32.752+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T21:10:00+00:00 [running]> on host b06401153325
[2025-02-23T21:20:33.132+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_analytics' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:10:00+00:00'
[2025-02-23T21:20:33.139+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:20:33.210+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:20:33.214+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_3.sh']
[2025-02-23T21:20:33.248+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:20:33.263+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:20:38.580+0000] {subprocess.py:106} INFO -  Bucket 'datalake' j existe.
[2025-02-23T21:20:38.586+0000] {subprocess.py:106} INFO -  'silveranalitics/' inicializado no bucket 'datalake'.
[2025-02-23T21:20:38.591+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db1.csv' carregado de silver/db1.
[2025-02-23T21:20:38.596+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db2.csv' carregado de silver/db2.
[2025-02-23T21:20:38.600+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db3.csv' carregado de silver/db3.
[2025-02-23T21:20:38.605+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:20:38.609+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db1.csv' carregado de silver/db1.
[2025-02-23T21:20:38.613+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db2.csv' carregado de silver/db2.
[2025-02-23T21:20:38.616+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db3.csv' carregado de silver/db3.
[2025-02-23T21:20:38.620+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:20:38.623+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db1.csv' carregado de silver/db1.
[2025-02-23T21:20:38.627+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db2.csv' carregado de silver/db2.
[2025-02-23T21:20:38.632+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db3.csv' carregado de silver/db3.
[2025-02-23T21:20:38.636+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:20:38.640+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db1.csv' carregado de silver/db1.
[2025-02-23T21:20:38.643+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db2.csv' carregado de silver/db2.
[2025-02-23T21:20:38.647+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db3.csv' carregado de silver/db3.
[2025-02-23T21:20:38.651+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:20:38.824+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:20:38.826+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:20:38.885+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:20:38.886+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_analytics, run_id=scheduled__2025-02-23T21:10:00+00:00, execution_date=20250223T211000, start_date=20250223T212032, end_date=20250223T212038
[2025-02-23T21:20:38.996+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:20:39.044+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:20:39.053+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:20:41.721+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:20:41.793+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T21:10:00+00:00 [queued]>
[2025-02-23T21:20:41.823+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T21:10:00+00:00 [queued]>
[2025-02-23T21:20:41.826+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:20:41.890+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_gold> on 2025-02-23 21:10:00+00:00
[2025-02-23T21:20:41.903+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2311) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:20:41.907+0000] {standard_task_runner.py:72} INFO - Started process 2312 to run task
[2025-02-23T21:20:41.907+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_gold', 'scheduled__2025-02-23T21:10:00+00:00', '--job-id', '95', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp368kw1qi']
[2025-02-23T21:20:41.910+0000] {standard_task_runner.py:105} INFO - Job 95: Subtask process_gold
[2025-02-23T21:20:42.052+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T21:10:00+00:00 [running]> on host b06401153325
[2025-02-23T21:20:42.455+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_gold' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:10:00+00:00'
[2025-02-23T21:20:42.457+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:20:42.509+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:20:42.513+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_4.sh']
[2025-02-23T21:20:42.539+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:20:42.552+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:20:46.974+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsFornecedor.csv' carregado.
[2025-02-23T21:20:46.977+0000] {subprocess.py:106} INFO -  'gold/goldFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:20:46.980+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsProdutos.csv' carregado.
[2025-02-23T21:20:46.982+0000] {subprocess.py:106} INFO -  'gold/goldProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:20:46.985+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsVendas.csv' carregado.
[2025-02-23T21:20:46.987+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' carregado para *join*.
[2025-02-23T21:20:46.990+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' carregado para *join*.
[2025-02-23T21:20:46.993+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' carregado para *join*.
[2025-02-23T21:20:46.998+0000] {subprocess.py:106} INFO -  'gold/goldVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:20:47.000+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsVendedor.csv' carregado.
[2025-02-23T21:20:47.003+0000] {subprocess.py:106} INFO -  'gold/goldVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:20:47.160+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:20:47.161+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:20:47.214+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:20:47.216+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_gold, run_id=scheduled__2025-02-23T21:10:00+00:00, execution_date=20250223T211000, start_date=20250223T212041, end_date=20250223T212047
[2025-02-23T21:20:47.318+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:20:47.358+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:20:47.366+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:20:49.073+0000] {processor.py:186} INFO - Started process (PID=619) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:20:49.078+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:20:49.086+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:49.083+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:20:49.705+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:20:49.765+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:49.764+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:20:49.817+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:20:49.817+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:20:49.858+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.818 seconds
[2025-02-23T21:21:20.245+0000] {processor.py:186} INFO - Started process (PID=621) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:21:20.255+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:21:20.260+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:21:20.259+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:21:20.812+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:21:20.877+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:21:20.876+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:21:20.955+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:21:20.955+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:21:21.007+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.782 seconds
[2025-02-23T21:21:51.430+0000] {processor.py:186} INFO - Started process (PID=624) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:21:51.433+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:21:51.439+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:21:51.438+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:21:51.917+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:21:51.969+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:21:51.969+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:21:52.016+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:21:52.016+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:21:52.058+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.673 seconds
[2025-02-23T21:22:22.319+0000] {processor.py:186} INFO - Started process (PID=625) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:22:22.325+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:22:22.342+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:22:22.341+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:22:23.968+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:22:24.471+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:22:24.469+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:22:24.627+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:22:24.626+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:22:24.788+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.549 seconds
[2025-02-23T21:22:55.208+0000] {processor.py:186} INFO - Started process (PID=628) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:22:55.211+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:22:55.214+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:22:55.213+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:22:56.142+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:22:56.280+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:22:56.280+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:22:56.389+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:22:56.388+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:22:56.466+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.274 seconds
[2025-02-23T21:23:26.862+0000] {processor.py:186} INFO - Started process (PID=630) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:23:26.864+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:23:26.867+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:23:26.866+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:23:27.226+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:23:27.278+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:23:27.277+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:23:27.328+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:23:27.327+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:23:27.405+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.560 seconds
[2025-02-23T21:23:57.538+0000] {processor.py:186} INFO - Started process (PID=632) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:23:57.541+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:23:57.546+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:23:57.544+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:23:58.059+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:23:58.114+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:23:58.113+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:23:58.173+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:23:58.172+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:23:58.218+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.703 seconds
[2025-02-23T21:24:28.610+0000] {processor.py:186} INFO - Started process (PID=634) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:24:28.612+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:24:28.616+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:24:28.615+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:24:29.073+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:24:29.277+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:24:29.276+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:24:29.325+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:24:29.324+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:24:29.367+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.771 seconds
[2025-02-23T21:25:00.235+0000] {processor.py:186} INFO - Started process (PID=636) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:25:00.350+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:25:00.358+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:25:00.354+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:25:00.704+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:25:00.760+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:25:00.759+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:25:00.808+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:25:00.808+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:25:00.849+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.630 seconds
[2025-02-23T21:25:31.065+0000] {processor.py:186} INFO - Started process (PID=638) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:25:31.067+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:25:31.070+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:25:31.069+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:25:31.479+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:25:31.534+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:25:31.534+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:25:31.583+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:25:31.583+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:25:31.627+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.575 seconds
[2025-02-23T21:26:01.694+0000] {processor.py:186} INFO - Started process (PID=640) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:26:01.696+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:26:01.699+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:26:01.698+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:26:02.157+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:26:02.213+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:26:02.212+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:26:02.265+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:26:02.265+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:26:02.309+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.630 seconds
[2025-02-23T21:26:33.073+0000] {processor.py:186} INFO - Started process (PID=642) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:26:33.075+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:26:33.077+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:26:33.077+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:26:33.555+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:26:33.607+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:26:33.607+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:26:33.660+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:26:33.660+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:26:33.701+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.642 seconds
[2025-02-23T21:27:03.859+0000] {processor.py:186} INFO - Started process (PID=644) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:27:03.861+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:27:03.864+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:27:03.863+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:27:04.238+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:27:04.291+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:27:04.291+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:27:04.342+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:27:04.341+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:27:04.382+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.536 seconds
[2025-02-23T21:27:34.579+0000] {processor.py:186} INFO - Started process (PID=646) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:27:34.580+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:27:34.584+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:27:34.582+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:27:35.083+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:27:35.138+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:27:35.137+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:27:35.187+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:27:35.186+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:27:35.229+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.664 seconds
[2025-02-23T21:28:05.619+0000] {processor.py:186} INFO - Started process (PID=648) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:28:05.621+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:28:05.624+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:28:05.624+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:28:06.005+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:28:06.060+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:28:06.060+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:28:06.109+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:28:06.109+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:28:06.152+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.546 seconds
[2025-02-23T21:28:36.428+0000] {processor.py:186} INFO - Started process (PID=650) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:28:36.430+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:28:36.433+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:28:36.432+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:28:36.780+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:28:36.836+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:28:36.835+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:28:36.886+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:28:36.886+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:28:36.928+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.516 seconds
[2025-02-23T21:29:07.012+0000] {processor.py:186} INFO - Started process (PID=652) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:29:07.014+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:29:07.017+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:29:07.016+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:29:07.555+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:29:07.611+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:29:07.610+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:29:07.661+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:29:07.661+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:29:07.703+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.707 seconds
[2025-02-23T21:29:38.110+0000] {processor.py:186} INFO - Started process (PID=654) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:29:38.112+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:29:38.115+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:29:38.114+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:29:38.600+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:29:38.722+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:29:38.719+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:29:38.794+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:29:38.794+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:20:00+00:00, run_after=2025-02-23 21:30:00+00:00
[2025-02-23T21:29:38.937+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.841 seconds
[2025-02-23T21:30:02.461+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:30:02.612+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:20:00+00:00 [queued]>
[2025-02-23T21:30:02.631+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:20:00+00:00 [queued]>
[2025-02-23T21:30:02.632+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:30:02.662+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 21:20:00+00:00
[2025-02-23T21:30:02.673+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2333) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:30:02.677+0000] {standard_task_runner.py:72} INFO - Started process 2334 to run task
[2025-02-23T21:30:02.678+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T21:20:00+00:00', '--job-id', '96', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmptx9qi2w8']
[2025-02-23T21:30:02.681+0000] {standard_task_runner.py:105} INFO - Job 96: Subtask process_landzone
[2025-02-23T21:30:02.784+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:20:00+00:00 [running]> on host b06401153325
[2025-02-23T21:30:02.981+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:20:00+00:00'
[2025-02-23T21:30:02.983+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:30:03.012+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:30:03.015+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T21:30:03.035+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:30:03.044+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:30:14.051+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T21:30:14.057+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.061+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T21:30:14.067+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.073+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T21:30:14.079+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.085+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T21:30:14.089+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.093+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T21:30:14.096+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.101+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T21:30:14.104+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.107+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T21:30:14.110+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.113+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T21:30:14.117+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.120+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T21:30:14.123+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.126+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T21:30:14.129+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.133+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T21:30:14.136+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.139+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T21:30:14.142+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.csv atualizado e enviado para MinIO!
[2025-02-23T21:30:14.341+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:30:14.343+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:30:14.426+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:30:14.428+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T21:20:00+00:00, execution_date=20250223T212000, start_date=20250223T213002, end_date=20250223T213014
[2025-02-23T21:30:14.545+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:30:14.614+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:30:14.622+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:30:16.746+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:30:16.796+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T21:20:00+00:00 [queued]>
[2025-02-23T21:30:16.819+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T21:20:00+00:00 [queued]>
[2025-02-23T21:30:16.820+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:30:16.854+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 21:20:00+00:00
[2025-02-23T21:30:16.867+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2379) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:30:16.870+0000] {standard_task_runner.py:72} INFO - Started process 2380 to run task
[2025-02-23T21:30:16.871+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T21:20:00+00:00', '--job-id', '97', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpeyxsg9zl']
[2025-02-23T21:30:16.874+0000] {standard_task_runner.py:105} INFO - Job 97: Subtask process_bronze
[2025-02-23T21:30:16.992+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T21:20:00+00:00 [running]> on host b06401153325
[2025-02-23T21:30:17.244+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:20:00+00:00'
[2025-02-23T21:30:17.248+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:30:17.319+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:30:17.324+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T21:30:17.372+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:30:17.405+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:30:30.007+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.csv' baixado para 'temp_landzone_fornecedores_db1.csv'.
[2025-02-23T21:30:30.009+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db1.csv' enviado para 'datalake/bronze/db1/fornecedores_db1.csv'.
[2025-02-23T21:30:30.012+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.csv' baixado para 'temp_landzone_fornecedores_db2.csv'.
[2025-02-23T21:30:30.015+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db2.csv' enviado para 'datalake/bronze/db2/fornecedores_db2.csv'.
[2025-02-23T21:30:30.019+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.csv' baixado para 'temp_landzone_fornecedores_db3.csv'.
[2025-02-23T21:30:30.023+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db3.csv' enviado para 'datalake/bronze/db3/fornecedores_db3.csv'.
[2025-02-23T21:30:30.026+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.csv' baixado para 'temp_landzone_produtos_db1.csv'.
[2025-02-23T21:30:30.030+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db1.csv' enviado para 'datalake/bronze/db1/produtos_db1.csv'.
[2025-02-23T21:30:30.032+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.csv' baixado para 'temp_landzone_produtos_db2.csv'.
[2025-02-23T21:30:30.035+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db2.csv' enviado para 'datalake/bronze/db2/produtos_db2.csv'.
[2025-02-23T21:30:30.037+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.csv' baixado para 'temp_landzone_produtos_db3.csv'.
[2025-02-23T21:30:30.038+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db3.csv' enviado para 'datalake/bronze/db3/produtos_db3.csv'.
[2025-02-23T21:30:30.040+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.csv' baixado para 'temp_landzone_vendas_db1.csv'.
[2025-02-23T21:30:30.044+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db1.csv' enviado para 'datalake/bronze/db1/vendas_db1.csv'.
[2025-02-23T21:30:30.046+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.csv' baixado para 'temp_landzone_vendas_db2.csv'.
[2025-02-23T21:30:30.048+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db2.csv' enviado para 'datalake/bronze/db2/vendas_db2.csv'.
[2025-02-23T21:30:30.050+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.csv' baixado para 'temp_landzone_vendas_db3.csv'.
[2025-02-23T21:30:30.052+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db3.csv' enviado para 'datalake/bronze/db3/vendas_db3.csv'.
[2025-02-23T21:30:30.054+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.csv' baixado para 'temp_landzone_vendedores_db1.csv'.
[2025-02-23T21:30:30.055+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db1.csv' enviado para 'datalake/bronze/db1/vendedores_db1.csv'.
[2025-02-23T21:30:30.057+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.csv' baixado para 'temp_landzone_vendedores_db2.csv'.
[2025-02-23T21:30:30.059+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db2.csv' enviado para 'datalake/bronze/db2/vendedores_db2.csv'.
[2025-02-23T21:30:30.060+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.csv' baixado para 'temp_landzone_vendedores_db3.csv'.
[2025-02-23T21:30:30.062+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db3.csv' enviado para 'datalake/bronze/db3/vendedores_db3.csv'.
[2025-02-23T21:30:30.282+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:30:30.287+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:30:30.388+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:30:30.389+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T21:20:00+00:00, execution_date=20250223T212000, start_date=20250223T213016, end_date=20250223T213030
[2025-02-23T21:30:30.521+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:30:30.584+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:30:30.591+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:30:33.276+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:30:33.330+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T21:20:00+00:00 [queued]>
[2025-02-23T21:30:33.352+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T21:20:00+00:00 [queued]>
[2025-02-23T21:30:33.353+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:30:33.418+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 21:20:00+00:00
[2025-02-23T21:30:33.439+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2389) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:30:33.445+0000] {standard_task_runner.py:72} INFO - Started process 2390 to run task
[2025-02-23T21:30:33.448+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T21:20:00+00:00', '--job-id', '98', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpuzzss9bb']
[2025-02-23T21:30:33.453+0000] {standard_task_runner.py:105} INFO - Job 98: Subtask process_silver
[2025-02-23T21:30:33.597+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T21:20:00+00:00 [running]> on host b06401153325
[2025-02-23T21:30:33.849+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:20:00+00:00'
[2025-02-23T21:30:33.852+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:30:34.044+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:30:34.050+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T21:30:34.098+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:30:34.116+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T21:30:39.610+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T21:30:39.613+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.616+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/fornecedores_db1.csv
[2025-02-23T21:30:39.618+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/produtos_db1.csv' baixado para 'temp_bronze_produtos_db1.csv'.
[2025-02-23T21:30:39.620+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.622+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/produtos_db1.csv
[2025-02-23T21:30:39.624+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendas_db1.csv' baixado para 'temp_bronze_vendas_db1.csv'.
[2025-02-23T21:30:39.626+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.627+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendas_db1.csv
[2025-02-23T21:30:39.630+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendedores_db1.csv' baixado para 'temp_bronze_vendedores_db1.csv'.
[2025-02-23T21:30:39.631+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.633+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendedores_db1.csv
[2025-02-23T21:30:39.635+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/fornecedores_db2.csv' baixado para 'temp_bronze_fornecedores_db2.csv'.
[2025-02-23T21:30:39.636+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.638+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/fornecedores_db2.csv
[2025-02-23T21:30:39.640+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/produtos_db2.csv' baixado para 'temp_bronze_produtos_db2.csv'.
[2025-02-23T21:30:39.641+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.644+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/produtos_db2.csv
[2025-02-23T21:30:39.646+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendas_db2.csv' baixado para 'temp_bronze_vendas_db2.csv'.
[2025-02-23T21:30:39.648+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.649+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendas_db2.csv
[2025-02-23T21:30:39.651+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendedores_db2.csv' baixado para 'temp_bronze_vendedores_db2.csv'.
[2025-02-23T21:30:39.653+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.655+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendedores_db2.csv
[2025-02-23T21:30:39.657+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/fornecedores_db3.csv' baixado para 'temp_bronze_fornecedores_db3.csv'.
[2025-02-23T21:30:39.659+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.661+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/fornecedores_db3.csv
[2025-02-23T21:30:39.663+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/produtos_db3.csv' baixado para 'temp_bronze_produtos_db3.csv'.
[2025-02-23T21:30:39.665+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.667+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/produtos_db3.csv
[2025-02-23T21:30:39.669+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendas_db3.csv' baixado para 'temp_bronze_vendas_db3.csv'.
[2025-02-23T21:30:39.671+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.673+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendas_db3.csv
[2025-02-23T21:30:39.675+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendedores_db3.csv' baixado para 'temp_bronze_vendedores_db3.csv'.
[2025-02-23T21:30:39.677+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T21:30:30.818+0000] {processor.py:186} INFO - Started process (PID=657) to work on /opt/***/dags/dag_scheduler_v1.py']
[2025-02-23T21:30:39.679+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendedores_db3.csv
[2025-02-23T21:30:39.842+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:30:39.845+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:30:40.000+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:30:40.003+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T21:20:00+00:00, execution_date=20250223T212000, start_date=20250223T213033, end_date=20250223T213040
[2025-02-23T21:30:40.503+0000] {processor.py:186} INFO - Started process (PID=658) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:30:40.511+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:30:40.519+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:30:40.518+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:30:41.634+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:30:41.731+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:30:41.731+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:30:41.835+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:30:41.834+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:30:41.914+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.443 seconds
[2025-02-23T21:30:42.877+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:30:42.933+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T21:20:00+00:00 [queued]>
[2025-02-23T21:30:42.974+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T21:20:00+00:00 [queued]>
[2025-02-23T21:30:42.976+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:30:43.054+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_analytics> on 2025-02-23 21:20:00+00:00
[2025-02-23T21:30:43.087+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2435) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:30:43.101+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_analytics', 'scheduled__2025-02-23T21:20:00+00:00', '--job-id', '99', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpkrxqqabg']
[2025-02-23T21:30:43.111+0000] {standard_task_runner.py:105} INFO - Job 99: Subtask process_analytics
[2025-02-23T21:30:43.107+0000] {standard_task_runner.py:72} INFO - Started process 2436 to run task
[2025-02-23T21:30:43.408+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T21:20:00+00:00 [running]> on host b06401153325
[2025-02-23T21:30:44.549+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_analytics' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:20:00+00:00'
[2025-02-23T21:30:44.553+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:30:44.638+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:30:44.643+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_3.sh']
[2025-02-23T21:30:44.694+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:30:44.722+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:30:49.402+0000] {subprocess.py:106} INFO -  Bucket 'datalake' j existe.
[2025-02-23T21:30:49.406+0000] {subprocess.py:106} INFO -  'silveranalitics/' inicializado no bucket 'datalake'.
[2025-02-23T21:30:49.408+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db1.csv' carregado de silver/db1.
[2025-02-23T21:30:49.411+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db2.csv' carregado de silver/db2.
[2025-02-23T21:30:49.415+0000] {subprocess.py:106} INFO -  Arquivo 'fornecedores_db3.csv' carregado de silver/db3.
[2025-02-23T21:30:49.417+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:30:49.421+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db1.csv' carregado de silver/db1.
[2025-02-23T21:30:49.423+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db2.csv' carregado de silver/db2.
[2025-02-23T21:30:49.426+0000] {subprocess.py:106} INFO -  Arquivo 'produtos_db3.csv' carregado de silver/db3.
[2025-02-23T21:30:49.428+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:30:49.432+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db1.csv' carregado de silver/db1.
[2025-02-23T21:30:49.435+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db2.csv' carregado de silver/db2.
[2025-02-23T21:30:49.438+0000] {subprocess.py:106} INFO -  Arquivo 'vendas_db3.csv' carregado de silver/db3.
[2025-02-23T21:30:49.440+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:30:49.443+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db1.csv' carregado de silver/db1.
[2025-02-23T21:30:49.445+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db2.csv' carregado de silver/db2.
[2025-02-23T21:30:49.448+0000] {subprocess.py:106} INFO -  Arquivo 'vendedores_db3.csv' carregado de silver/db3.
[2025-02-23T21:30:49.451+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:30:49.605+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:30:49.606+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:30:49.667+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:30:49.669+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_analytics, run_id=scheduled__2025-02-23T21:20:00+00:00, execution_date=20250223T212000, start_date=20250223T213042, end_date=20250223T213049
[2025-02-23T21:30:49.791+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:30:49.842+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:30:49.850+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:30:52.191+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T21:30:52.244+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T21:20:00+00:00 [queued]>
[2025-02-23T21:30:52.269+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T21:20:00+00:00 [queued]>
[2025-02-23T21:30:52.271+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T21:30:52.314+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_gold> on 2025-02-23 21:20:00+00:00
[2025-02-23T21:30:52.351+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2457) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T21:30:52.364+0000] {standard_task_runner.py:72} INFO - Started process 2458 to run task
[2025-02-23T21:30:52.365+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_gold', 'scheduled__2025-02-23T21:20:00+00:00', '--job-id', '100', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpxsaupq8y']
[2025-02-23T21:30:52.371+0000] {standard_task_runner.py:105} INFO - Job 100: Subtask process_gold
[2025-02-23T21:30:52.544+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T21:20:00+00:00 [running]> on host b06401153325
[2025-02-23T21:30:53.009+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_gold' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:20:00+00:00'
[2025-02-23T21:30:53.015+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T21:30:53.078+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T21:30:53.081+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_4.sh']
[2025-02-23T21:30:53.106+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T21:30:53.119+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T21:30:57.371+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsFornecedor.csv' carregado.
[2025-02-23T21:30:57.374+0000] {subprocess.py:106} INFO -  'gold/goldFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:30:57.377+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsProdutos.csv' carregado.
[2025-02-23T21:30:57.379+0000] {subprocess.py:106} INFO -  'gold/goldProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:30:57.382+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsVendas.csv' carregado.
[2025-02-23T21:30:57.385+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' carregado para *join*.
[2025-02-23T21:30:57.387+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' carregado para *join*.
[2025-02-23T21:30:57.389+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' carregado para *join*.
[2025-02-23T21:30:57.391+0000] {subprocess.py:106} INFO -  'gold/goldVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:30:57.393+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsVendedor.csv' carregado.
[2025-02-23T21:30:57.395+0000] {subprocess.py:106} INFO -  'gold/goldVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T21:30:57.670+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T21:30:57.672+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T21:30:57.835+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T21:30:57.842+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_gold, run_id=scheduled__2025-02-23T21:20:00+00:00, execution_date=20250223T212000, start_date=20250223T213052, end_date=20250223T213057
[2025-02-23T21:30:58.010+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T21:30:58.068+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T21:30:58.075+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T21:31:12.610+0000] {processor.py:186} INFO - Started process (PID=660) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:31:12.613+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:31:12.617+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:31:12.616+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:31:13.345+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:31:13.418+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:31:13.417+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:31:13.474+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:31:13.474+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:31:13.514+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.920 seconds
[2025-02-23T21:31:43.897+0000] {processor.py:186} INFO - Started process (PID=662) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:31:43.899+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:31:43.902+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:31:43.901+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:31:44.286+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:31:44.347+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:31:44.347+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:31:44.402+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:31:44.402+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:31:44.446+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.563 seconds
[2025-02-23T21:32:14.720+0000] {processor.py:186} INFO - Started process (PID=664) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:32:14.722+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:32:14.725+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:32:14.725+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:32:15.113+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:32:15.195+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:32:15.195+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:32:15.253+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:32:15.253+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:32:15.295+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.591 seconds
[2025-02-23T21:32:46.061+0000] {processor.py:186} INFO - Started process (PID=666) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:32:46.063+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:32:46.066+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:32:46.065+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:32:46.422+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:32:46.474+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:32:46.474+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:32:46.524+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:32:46.523+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:32:46.567+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.520 seconds
[2025-02-23T21:33:16.643+0000] {processor.py:186} INFO - Started process (PID=668) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:33:16.647+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:33:16.651+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:33:16.650+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:33:17.032+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:33:17.086+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:33:17.085+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:33:17.134+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:33:17.133+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:33:17.177+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.550 seconds
[2025-02-23T21:33:47.250+0000] {processor.py:186} INFO - Started process (PID=670) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:33:47.252+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:33:47.255+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:33:47.254+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:33:47.624+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:33:47.679+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:33:47.679+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:33:47.735+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:33:47.734+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:33:47.778+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.541 seconds
[2025-02-23T21:34:17.831+0000] {processor.py:186} INFO - Started process (PID=672) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:34:17.833+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:34:17.836+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:34:17.836+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:34:18.203+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:34:18.269+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:34:18.269+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:34:18.361+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:34:18.360+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:34:18.450+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.633 seconds
[2025-02-23T21:34:49.194+0000] {processor.py:186} INFO - Started process (PID=674) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:34:49.620+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:34:49.624+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:34:49.623+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:34:50.145+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:34:50.200+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:34:50.200+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:34:50.249+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:34:50.248+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:34:50.292+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.111 seconds
[2025-02-23T21:35:20.741+0000] {processor.py:186} INFO - Started process (PID=676) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:35:20.743+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:35:20.746+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:35:20.746+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:35:21.153+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:35:21.503+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:35:21.503+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:35:21.554+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:35:21.553+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:35:21.596+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.868 seconds
[2025-02-23T21:35:51.744+0000] {processor.py:186} INFO - Started process (PID=678) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:35:51.746+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:35:51.749+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:35:51.749+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:35:52.219+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:35:52.295+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:35:52.295+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:35:52.355+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:35:52.354+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:35:52.400+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.676 seconds
[2025-02-23T21:36:22.825+0000] {processor.py:186} INFO - Started process (PID=680) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:36:22.827+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:36:22.833+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:36:22.832+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:36:23.238+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:36:23.290+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:36:23.289+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:36:23.341+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:36:23.341+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:36:23.387+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.581 seconds
[2025-02-23T21:36:53.730+0000] {processor.py:186} INFO - Started process (PID=682) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:36:53.734+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:36:53.742+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:36:53.740+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:36:54.140+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:36:54.195+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:36:54.195+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:36:54.249+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:36:54.249+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:36:54.288+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.580 seconds
[2025-02-23T21:39:24.921+0000] {processor.py:186} INFO - Started process (PID=684) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:39:25.324+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T21:39:25.336+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:39:25.335+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:39:26.352+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T21:39:26.481+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:39:26.481+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T21:39:26.609+0000] {logging_mixin.py:190} INFO - [2025-02-23T21:39:26.609+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 21:30:00+00:00, run_after=2025-02-23 21:40:00+00:00
[2025-02-23T21:39:26.685+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.803 seconds
[2025-02-23T22:35:50.735+0000] {processor.py:186} INFO - Started process (PID=686) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T22:35:50.984+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T22:35:50.990+0000] {logging_mixin.py:190} INFO - [2025-02-23T22:35:50.989+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T22:35:51.646+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T22:35:51.736+0000] {logging_mixin.py:190} INFO - [2025-02-23T22:35:51.735+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T22:35:51.835+0000] {logging_mixin.py:190} INFO - [2025-02-23T22:35:51.834+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 22:30:00+00:00, run_after=2025-02-23 22:40:00+00:00
[2025-02-23T22:35:51.938+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 1.220 seconds
[2025-02-23T22:35:53.115+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T22:35:53.163+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:30:00+00:00 [queued]>
[2025-02-23T22:35:53.182+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:30:00+00:00 [queued]>
[2025-02-23T22:35:53.183+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T22:35:53.251+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 21:30:00+00:00
[2025-02-23T22:35:53.270+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2479) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T22:35:53.274+0000] {standard_task_runner.py:72} INFO - Started process 2481 to run task
[2025-02-23T22:35:53.274+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T21:30:00+00:00', '--job-id', '101', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpwvt9230r']
[2025-02-23T22:35:53.277+0000] {standard_task_runner.py:105} INFO - Job 101: Subtask process_landzone
[2025-02-23T22:35:53.317+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T22:35:53.358+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T22:20:00+00:00 [queued]>
[2025-02-23T22:35:53.377+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T22:20:00+00:00 [queued]>
[2025-02-23T22:35:53.378+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T22:35:53.383+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:30:00+00:00 [running]> on host b06401153325
[2025-02-23T22:35:53.438+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 22:20:00+00:00
[2025-02-23T22:35:53.449+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2480) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T22:35:53.454+0000] {standard_task_runner.py:72} INFO - Started process 2483 to run task
[2025-02-23T22:35:53.456+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T22:20:00+00:00', '--job-id', '102', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpa4d8tyd9']
[2025-02-23T22:35:53.458+0000] {standard_task_runner.py:105} INFO - Job 102: Subtask process_landzone
[2025-02-23T22:35:53.566+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T22:20:00+00:00 [running]> on host b06401153325
[2025-02-23T22:35:53.625+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T21:30:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T21:30:00+00:00'
[2025-02-23T22:35:53.629+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T22:35:53.664+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T22:35:53.667+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T22:35:53.687+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T22:35:53.699+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T22:35:53.794+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T22:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T22:20:00+00:00'
[2025-02-23T22:35:53.796+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T22:35:53.832+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T22:35:53.835+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T22:35:53.856+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T22:35:53.865+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T22:35:58.725+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T22:35:58.729+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T22:35:58.741+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T22:35:58.743+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 93, in <module>"
[2025-02-23T22:35:58.748+0000] {subprocess.py:106} INFO -     df_new = read_duckdb_table(db_file, table)
[2025-02-23T22:35:58.751+0000] {subprocess.py:106} INFO -              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T22:35:58.807+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 37, in read_duckdb_table"
[2025-02-23T22:35:58.818+0000] {subprocess.py:106} INFO -     conn = duckdb.connect(db_file)
[2025-02-23T22:35:58.837+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-23T22:35:58.839+0000] {subprocess.py:106} INFO - duckdb.duckdb.IOException: IO Error: Could not set lock on file ""/Volumes/MACBACKUP/workspaceDlake/dlake/db1.duckdb"": Conflicting lock is held in /usr/local/bin/python3.12 (PID 2490). See also https://duckdb.org/docs/connect/concurrency"
[2025-02-23T22:35:59.241+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T22:35:59.243+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T22:35:59.269+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T22:35:59.287+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T21:30:00+00:00, execution_date=20250223T213000, start_date=20250223T223553, end_date=20250223T223559
[2025-02-23T22:35:59.359+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T22:35:59.361+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 101 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2481)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T22:35:59.413+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-23T22:35:59.473+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T22:35:59.969+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T22:36:08.505+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T22:36:08.507+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.509+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T22:36:08.511+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.512+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T22:36:08.516+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.518+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T22:36:08.520+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.522+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T22:36:08.524+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.526+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T22:36:08.527+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.529+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T22:36:08.530+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.532+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T22:36:08.533+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.535+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T22:36:08.537+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.538+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T22:36:08.539+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.541+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T22:36:08.543+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.544+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T22:36:08.545+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.csv atualizado e enviado para MinIO!
[2025-02-23T22:36:08.676+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T22:36:08.678+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T22:36:08.733+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T22:36:08.734+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T22:20:00+00:00, execution_date=20250223T222000, start_date=20250223T223553, end_date=20250223T223608
[2025-02-23T22:36:08.810+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T22:36:08.871+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T22:36:08.897+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T22:36:10.883+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T22:36:10.927+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T22:20:00+00:00 [queued]>
[2025-02-23T22:36:10.947+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T22:20:00+00:00 [queued]>
[2025-02-23T22:36:10.949+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T22:36:10.996+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 22:20:00+00:00
[2025-02-23T22:36:11.007+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2550) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T22:36:11.010+0000] {standard_task_runner.py:72} INFO - Started process 2551 to run task
[2025-02-23T22:36:11.011+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T22:20:00+00:00', '--job-id', '103', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpnowc7mi8']
[2025-02-23T22:36:11.014+0000] {standard_task_runner.py:105} INFO - Job 103: Subtask process_bronze
[2025-02-23T22:36:11.109+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T22:20:00+00:00 [running]> on host b06401153325
[2025-02-23T22:36:11.341+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T22:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T22:20:00+00:00'
[2025-02-23T22:36:11.342+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T22:36:11.375+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T22:36:11.377+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T22:36:11.396+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T22:36:11.405+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T22:36:18.371+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db1.csv' baixado para 'temp_landzone_fornecedores_db1.csv'.
[2025-02-23T22:36:18.374+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db1.csv' enviado para 'datalake/bronze/db1/fornecedores_db1.csv'.
[2025-02-23T22:36:18.376+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db2.csv' baixado para 'temp_landzone_fornecedores_db2.csv'.
[2025-02-23T22:36:18.378+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db2.csv' enviado para 'datalake/bronze/db2/fornecedores_db2.csv'.
[2025-02-23T22:36:18.381+0000] {subprocess.py:106} INFO - Arquivo 'landzone/fornecedores_db3.csv' baixado para 'temp_landzone_fornecedores_db3.csv'.
[2025-02-23T22:36:18.386+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_fornecedores_db3.csv' enviado para 'datalake/bronze/db3/fornecedores_db3.csv'.
[2025-02-23T22:36:18.388+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db1.csv' baixado para 'temp_landzone_produtos_db1.csv'.
[2025-02-23T22:36:18.391+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db1.csv' enviado para 'datalake/bronze/db1/produtos_db1.csv'.
[2025-02-23T22:36:18.393+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db2.csv' baixado para 'temp_landzone_produtos_db2.csv'.
[2025-02-23T22:36:18.395+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db2.csv' enviado para 'datalake/bronze/db2/produtos_db2.csv'.
[2025-02-23T22:36:18.398+0000] {subprocess.py:106} INFO - Arquivo 'landzone/produtos_db3.csv' baixado para 'temp_landzone_produtos_db3.csv'.
[2025-02-23T22:36:18.400+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_produtos_db3.csv' enviado para 'datalake/bronze/db3/produtos_db3.csv'.
[2025-02-23T22:36:18.401+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db1.csv' baixado para 'temp_landzone_vendas_db1.csv'.
[2025-02-23T22:36:18.403+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db1.csv' enviado para 'datalake/bronze/db1/vendas_db1.csv'.
[2025-02-23T22:36:18.405+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db2.csv' baixado para 'temp_landzone_vendas_db2.csv'.
[2025-02-23T22:36:18.408+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db2.csv' enviado para 'datalake/bronze/db2/vendas_db2.csv'.
[2025-02-23T22:36:18.411+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendas_db3.csv' baixado para 'temp_landzone_vendas_db3.csv'.
[2025-02-23T22:36:18.415+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendas_db3.csv' enviado para 'datalake/bronze/db3/vendas_db3.csv'.
[2025-02-23T22:36:18.438+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db1.csv' baixado para 'temp_landzone_vendedores_db1.csv'.
[2025-02-23T22:36:18.440+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db1.csv' enviado para 'datalake/bronze/db1/vendedores_db1.csv'.
[2025-02-23T22:36:18.442+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db2.csv' baixado para 'temp_landzone_vendedores_db2.csv'.
[2025-02-23T22:36:18.444+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db2.csv' enviado para 'datalake/bronze/db2/vendedores_db2.csv'.
[2025-02-23T22:36:18.446+0000] {subprocess.py:106} INFO - Arquivo 'landzone/vendedores_db3.csv' baixado para 'temp_landzone_vendedores_db3.csv'.
[2025-02-23T22:36:18.448+0000] {subprocess.py:106} INFO - Arquivo 'temp_bronze_vendedores_db3.csv' enviado para 'datalake/bronze/db3/vendedores_db3.csv'.
[2025-02-23T22:36:18.638+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T22:36:18.639+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T22:36:18.697+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T22:36:18.698+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_bronze, run_id=scheduled__2025-02-23T22:20:00+00:00, execution_date=20250223T222000, start_date=20250223T223610, end_date=20250223T223618
[2025-02-23T22:36:18.824+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T22:36:18.879+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T22:36:18.904+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T22:36:24.311+0000] {processor.py:186} INFO - Started process (PID=688) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T22:36:24.326+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T22:36:24.333+0000] {logging_mixin.py:190} INFO - [2025-02-23T22:36:24.331+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T22:36:28.128+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T22:36:28.353+0000] {logging_mixin.py:190} INFO - [2025-02-23T22:36:28.352+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T22:36:28.710+0000] {logging_mixin.py:190} INFO - [2025-02-23T22:36:28.709+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 22:30:00+00:00, run_after=2025-02-23 22:40:00+00:00
[2025-02-23T22:36:28.852+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.592 seconds
[2025-02-23T22:36:29.149+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T22:20:00+00:00 [running]> on host b06401153325
[2025-02-23T22:36:29.605+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_silver' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T22:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T22:20:00+00:00'
[2025-02-23T22:36:29.610+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T22:36:29.682+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T22:36:29.687+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_2.sh']
[2025-02-23T22:36:29.727+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T22:36:29.753+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T23:13:07.275+0000] {processor.py:186} INFO - Started process (PID=690) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T23:13:07.283+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T23:13:07.287+0000] {logging_mixin.py:190} INFO - [2025-02-23T23:13:07.286+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T23:13:11.017+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T23:13:11.227+0000] {logging_mixin.py:190} INFO - [2025-02-23T23:13:11.226+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T23:13:11.603+0000] {logging_mixin.py:190} INFO - [2025-02-23T23:13:11.603+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 23:10:00+00:00, run_after=2025-02-23 23:20:00+00:00
[2025-02-23T23:13:11.742+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 4.488 seconds
[2025-02-23T23:13:15.496+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T23:13:15.768+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T23:13:15.819+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T22:20:00+00:00 [queued]>
[2025-02-23T23:13:15.878+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:30:00+00:00 [queued]>
[2025-02-23T23:13:15.884+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_silver scheduled__2025-02-23T22:20:00+00:00 [queued]>
[2025-02-23T23:13:15.895+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T23:13:15.911+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T21:30:00+00:00 [queued]>
[2025-02-23T23:13:15.917+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-23T23:13:16.170+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 21:30:00+00:00
[2025-02-23T23:13:16.175+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_silver> on 2025-02-23 22:20:00+00:00
[2025-02-23T23:13:16.193+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2570) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T23:13:16.219+0000] {standard_task_runner.py:72} INFO - Started process 2574 to run task
[2025-02-23T23:13:16.225+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_silver', 'scheduled__2025-02-23T22:20:00+00:00', '--job-id', '105', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp8a5fu6_p']
[2025-02-23T23:13:16.229+0000] {standard_task_runner.py:105} INFO - Job 105: Subtask process_silver
[2025-02-23T23:13:16.228+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T21:30:00+00:00', '--job-id', '106', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp97glbv9l']
[2025-02-23T23:13:16.234+0000] {standard_task_runner.py:105} INFO - Job 106: Subtask process_landzone
[2025-02-23T23:13:16.204+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2572) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T23:13:16.243+0000] {standard_task_runner.py:72} INFO - Started process 2575 to run task
[2025-02-23T23:13:16.291+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T23:13:16.370+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T22:30:00+00:00 [queued]>
[2025-02-23T23:13:16.400+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T22:30:00+00:00 [queued]>
[2025-02-23T23:13:16.402+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T23:13:17.245+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T23:13:17.257+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T23:13:17.274+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-23T23:13:17.306+0000] {subprocess.py:106} INFO - Em processamento UPLOAD SILVER...
[2025-02-23T23:13:17.369+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T23:13:17.393+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T23:13:28.604+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T23:13:28.606+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
[2025-02-23T23:13:28.608+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
"[2025-02-23T23:13:28.625+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 96, in <module>"
[2025-02-23T23:13:28.641+0000] {subprocess.py:106} INFO -     upsert_and_upload_csv(df_new, table, db_name)
[2025-02-23T23:13:28.633+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-23T23:13:28.651+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 96, in <module>"
"[2025-02-23T23:13:28.650+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 85, in upsert_and_upload_csv"
[2025-02-23T23:13:28.655+0000] {subprocess.py:106} INFO -     upsert_and_upload_csv(df_new, table, db_name)
"[2025-02-23T23:13:28.658+0000] {subprocess.py:106} INFO -     minio_client.fput_object(bucket_name, f""landzone/{csv_filename}"", temp_file)"
"[2025-02-23T23:13:28.658+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 85, in upsert_and_upload_csv"
"[2025-02-23T23:13:28.659+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1051, in fput_object"
"[2025-02-23T23:13:28.660+0000] {subprocess.py:106} INFO -     minio_client.fput_object(bucket_name, f""landzone/{csv_filename}"", temp_file)"
[2025-02-23T23:13:28.662+0000] {subprocess.py:106} INFO -     return self.put_object(
"[2025-02-23T23:13:28.663+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1051, in fput_object"
[2025-02-23T23:13:28.666+0000] {subprocess.py:106} INFO -     return self.put_object(
[2025-02-23T23:13:28.674+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^
[2025-02-23T23:13:28.664+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^
"[2025-02-23T23:13:28.684+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1996, in put_object"
[2025-02-23T23:13:28.686+0000] {subprocess.py:106} INFO -     raise exc
"[2025-02-23T23:13:28.680+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1996, in put_object"
"[2025-02-23T23:13:28.688+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1921, in put_object"
[2025-02-23T23:13:28.757+0000] {subprocess.py:106} INFO -     raise exc
[2025-02-23T23:13:28.760+0000] {subprocess.py:106} INFO -     raise IOError(
"[2025-02-23T23:13:28.763+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1921, in put_object"
[2025-02-23T23:13:28.763+0000] {subprocess.py:106} INFO - OSError: stream having not enough data;expected: 5242880, got: 614400 bytes
[2025-02-23T23:13:28.765+0000] {subprocess.py:106} INFO -     raise IOError(
[2025-02-23T23:13:28.767+0000] {subprocess.py:106} INFO - OSError: stream having not enough data;expected: 5242880, got: 0 bytes
[2025-02-23T23:13:29.022+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T23:13:29.029+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-23T23:13:29.034+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T23:13:29.035+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-23T23:13:29.083+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T23:13:29.083+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T23:13:29.103+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T21:30:00+00:00, execution_date=20250223T213000, start_date=20250223T231315, end_date=20250223T231329
[2025-02-23T23:13:29.117+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T22:30:00+00:00, execution_date=20250223T223000, start_date=20250223T231316, end_date=20250223T231329
[2025-02-23T23:13:29.286+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T23:13:29.288+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T23:13:29.290+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 107 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2578)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T23:13:29.294+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 106 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2575)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-23T23:13:29.474+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T23:13:29.478+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/fornecedores_db1.csv' baixado para 'temp_bronze_fornecedores_db1.csv'.
[2025-02-23T23:13:29.481+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.485+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/fornecedores_db1.csv
[2025-02-23T23:13:29.488+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/produtos_db1.csv' baixado para 'temp_bronze_produtos_db1.csv'.
[2025-02-23T23:13:29.490+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.495+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/produtos_db1.csv
[2025-02-23T23:13:29.496+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendas_db1.csv' baixado para 'temp_bronze_vendas_db1.csv'.
[2025-02-23T23:13:29.503+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.504+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendas_db1.csv
[2025-02-23T23:13:29.505+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T23:13:29.507+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T23:13:29.506+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db1/vendedores_db1.csv' baixado para 'temp_bronze_vendedores_db1.csv'.
[2025-02-23T23:13:29.510+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.513+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db1/vendedores_db1.csv
[2025-02-23T23:13:29.526+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/fornecedores_db2.csv' baixado para 'temp_bronze_fornecedores_db2.csv'.
[2025-02-23T23:13:29.533+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.535+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/fornecedores_db2.csv
[2025-02-23T23:13:29.531+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T23:13:29.537+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/produtos_db2.csv' baixado para 'temp_bronze_produtos_db2.csv'.
[2025-02-23T23:13:29.542+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.544+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/produtos_db2.csv
[2025-02-23T23:13:29.548+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendas_db2.csv' baixado para 'temp_bronze_vendas_db2.csv'.
[2025-02-23T23:13:29.550+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.552+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendas_db2.csv
[2025-02-23T23:13:29.554+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db2/vendedores_db2.csv' baixado para 'temp_bronze_vendedores_db2.csv'.
[2025-02-23T23:13:29.555+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.557+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db2/vendedores_db2.csv
[2025-02-23T23:13:29.559+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/fornecedores_db3.csv' baixado para 'temp_bronze_fornecedores_db3.csv'.
[2025-02-23T23:13:29.561+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.562+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/fornecedores_db3.csv
[2025-02-23T23:13:29.564+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/produtos_db3.csv' baixado para 'temp_bronze_produtos_db3.csv'.
[2025-02-23T23:13:29.567+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.569+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/produtos_db3.csv
[2025-02-23T23:13:29.572+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendas_db3.csv' baixado para 'temp_bronze_vendas_db3.csv'.
[2025-02-23T23:13:29.592+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.627+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendas_db3.csv
[2025-02-23T23:13:29.644+0000] {subprocess.py:106} INFO - Arquivo 'bronze/db3/vendedores_db3.csv' baixado para 'temp_bronze_vendedores_db3.csv'.
[2025-02-23T23:13:29.647+0000] {subprocess.py:106} INFO - Colunas do arquivo bronze: ['[2025-02-23T23:13:18.440+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325']
[2025-02-23T23:13:29.652+0000] {subprocess.py:106} INFO - Erro: A coluna 'id' no est presente no arquivo bronze/db3/vendedores_db3.csv
[2025-02-23T23:13:30.030+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T23:13:30.032+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T23:13:30.133+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T23:13:30.135+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_silver, run_id=scheduled__2025-02-23T22:20:00+00:00, execution_date=20250223T222000, start_date=20250223T231315, end_date=20250223T231330
[2025-02-23T23:13:30.325+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T23:13:30.398+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T23:13:30.468+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T23:13:33.367+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T23:13:33.450+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T22:20:00+00:00 [queued]>
[2025-02-23T23:13:33.489+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T22:20:00+00:00 [queued]>
[2025-02-23T23:13:33.491+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T23:13:33.652+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_analytics> on 2025-02-23 22:20:00+00:00
[2025-02-23T23:13:33.675+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2667) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T23:13:33.681+0000] {standard_task_runner.py:72} INFO - Started process 2677 to run task
[2025-02-23T23:13:33.682+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_analytics', 'scheduled__2025-02-23T22:20:00+00:00', '--job-id', '109', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpaic2fhz4']
[2025-02-23T23:13:33.686+0000] {standard_task_runner.py:105} INFO - Job 109: Subtask process_analytics
[2025-02-23T23:13:33.807+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_analytics scheduled__2025-02-23T22:20:00+00:00 [running]> on host b06401153325
[2025-02-23T23:13:34.003+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_analytics' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T22:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T22:20:00+00:00'
[2025-02-23T23:13:34.006+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T23:13:34.054+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T23:13:34.061+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_3.sh']
[2025-02-23T23:13:34.104+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T23:13:34.118+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T23:13:42.012+0000] {processor.py:186} INFO - Started process (PID=698) to work on /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T23:13:42.023+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v2.py for tasks to queue
[2025-02-23T23:13:42.037+0000] {logging_mixin.py:190} INFO - [2025-02-23T23:13:42.036+0000] {dagbag.py:588} INFO - Filling up the DagBag from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T23:13:42.615+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-23T23:13:42.707+0000] {logging_mixin.py:190} INFO - [2025-02-23T23:13:42.706+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-23T23:13:42.839+0000] {logging_mixin.py:190} INFO - [2025-02-23T23:13:42.838+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-23 23:10:00+00:00, run_after=2025-02-23 23:20:00+00:00
[2025-02-23T23:13:42.924+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 0.945 seconds
[2025-02-23T23:13:46.138+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T23:13:46.189+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T22:20:00+00:00 [queued]>
[2025-02-23T23:13:46.216+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T22:20:00+00:00 [queued]>
[2025-02-23T23:13:46.218+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T23:13:46.296+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_gold> on 2025-02-23 22:20:00+00:00
[2025-02-23T23:13:46.317+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2725) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T23:13:46.323+0000] {standard_task_runner.py:72} INFO - Started process 2732 to run task
[2025-02-23T23:13:46.325+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_gold', 'scheduled__2025-02-23T22:20:00+00:00', '--job-id', '110', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp57qutq0b']
[2025-02-23T23:13:46.332+0000] {standard_task_runner.py:105} INFO - Job 110: Subtask process_gold
[2025-02-23T23:13:46.499+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_gold scheduled__2025-02-23T22:20:00+00:00 [running]> on host b06401153325
[2025-02-23T23:13:47.180+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_gold' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T22:20:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T22:20:00+00:00'
[2025-02-23T23:13:47.183+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T23:13:47.262+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T23:13:47.265+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_4.sh']
[2025-02-23T23:13:47.288+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T23:13:47.302+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-23T23:13:52.075+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsFornecedor.csv' carregado.
[2025-02-23T23:13:52.077+0000] {subprocess.py:106} INFO -  'gold/goldFornecedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T23:13:52.080+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsProdutos.csv' carregado.
[2025-02-23T23:13:52.081+0000] {subprocess.py:106} INFO -  'gold/goldProdutos.csv' atualizado e enviado para 'datalake'.
[2025-02-23T23:13:52.083+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsVendas.csv' carregado.
[2025-02-23T23:13:52.085+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsVendedor.csv' carregado para *join*.
[2025-02-23T23:13:52.087+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsProdutos.csv' carregado para *join*.
[2025-02-23T23:13:52.089+0000] {subprocess.py:106} INFO -  'silveranalitics/silverAnaliticsFornecedor.csv' carregado para *join*.
[2025-02-23T23:13:52.091+0000] {subprocess.py:106} INFO -  'gold/goldVendas.csv' atualizado e enviado para 'datalake'.
[2025-02-23T23:13:52.093+0000] {subprocess.py:106} INFO -  Arquivo 'silveranalitics/silverAnaliticsVendedor.csv' carregado.
[2025-02-23T23:13:52.095+0000] {subprocess.py:106} INFO -  'gold/goldVendedor.csv' atualizado e enviado para 'datalake'.
[2025-02-23T23:13:52.274+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T23:13:52.276+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T23:13:52.367+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T23:13:52.368+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_gold, run_id=scheduled__2025-02-23T22:20:00+00:00, execution_date=20250223T222000, start_date=20250223T231346, end_date=20250223T231352
[2025-02-23T23:13:52.471+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T23:13:52.507+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-23T23:13:52.539+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T23:13:52.930+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-23T23:13:52.932+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:52.935+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db1
[2025-02-23T23:13:52.936+0000] {subprocess.py:106} INFO - Arquivo vendedores_db1.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:52.938+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db1
[2025-02-23T23:13:52.940+0000] {subprocess.py:106} INFO - Arquivo vendas_db1.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:52.942+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db1
[2025-02-23T23:13:52.944+0000] {subprocess.py:106} INFO - Arquivo produtos_db1.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:52.946+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db2
[2025-02-23T23:13:52.947+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:52.949+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db2
[2025-02-23T23:13:52.950+0000] {subprocess.py:106} INFO - Arquivo vendedores_db2.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:52.953+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db2
[2025-02-23T23:13:52.956+0000] {subprocess.py:106} INFO - Arquivo vendas_db2.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:52.958+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db2
[2025-02-23T23:13:52.960+0000] {subprocess.py:106} INFO - Arquivo produtos_db2.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:52.962+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db3
[2025-02-23T23:13:52.964+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:52.966+0000] {subprocess.py:106} INFO - TABLE : vendedores BANCO db3
[2025-02-23T23:13:52.968+0000] {subprocess.py:106} INFO - Arquivo vendedores_db3.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:52.970+0000] {subprocess.py:106} INFO - TABLE : vendas BANCO db3
[2025-02-23T23:13:52.972+0000] {subprocess.py:106} INFO - Arquivo vendas_db3.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:52.973+0000] {subprocess.py:106} INFO - TABLE : produtos BANCO db3
[2025-02-23T23:13:52.975+0000] {subprocess.py:106} INFO - Arquivo produtos_db3.csv atualizado e enviado para MinIO!
[2025-02-23T23:13:53.165+0000] {subprocess.py:106} INFO - Processo Finalizado
[2025-02-23T23:13:53.167+0000] {subprocess.py:110} INFO - Command exited with return code 0
[2025-02-23T23:13:53.244+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-23T23:13:53.245+0000] {taskinstance.py:353} INFO - Marking task as SUCCESS. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T23:00:00+00:00, execution_date=20250223T230000, start_date=20250223T231317, end_date=20250223T231353
[2025-02-23T23:13:53.361+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-02-23T23:13:53.460+0000] {taskinstance.py:3901} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-02-23T23:13:53.470+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-23T23:13:55.779+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-23T23:13:55.828+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T23:00:00+00:00 [queued]>
[2025-02-23T23:13:55.849+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T23:00:00+00:00 [queued]>
[2025-02-23T23:13:55.850+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-23T23:13:55.912+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 23:00:00+00:00
[2025-02-23T23:13:55.927+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2774) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-23T23:13:55.932+0000] {standard_task_runner.py:72} INFO - Started process 2775 to run task
[2025-02-23T23:13:55.933+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T23:00:00+00:00', '--job-id', '111', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpu6nqm65x']
[2025-02-23T23:13:55.936+0000] {standard_task_runner.py:105} INFO - Job 111: Subtask process_bronze
[2025-02-23T23:13:56.044+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T23:00:00+00:00 [running]> on host b06401153325
[2025-02-23T23:13:56.267+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_bronze' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T23:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T23:00:00+00:00'
[2025-02-23T23:13:56.270+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-23T23:13:56.303+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-23T23:13:56.305+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_1.sh']
[2025-02-23T23:13:56.325+0000] {subprocess.py:99} INFO - Output:
[2025-02-23T23:13:56.335+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-24T01:15:31.895+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T01:15:31.907+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T01:15:31.952+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T01:15:31.990+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T22:30:00+00:00 [queued]>
[2025-02-24T01:15:32.002+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:10:00+00:00 [queued]>
[2025-02-24T01:15:32.021+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T22:30:00+00:00 [queued]>
[2025-02-24T01:15:32.022+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-24T01:15:32.036+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T01:00:00+00:00 [queued]>
[2025-02-24T01:15:32.048+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:10:00+00:00 [queued]>
[2025-02-24T01:15:32.049+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-24T01:15:32.067+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T01:00:00+00:00 [queued]>
[2025-02-24T01:15:32.069+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-24T01:15:32.084+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 22:30:00+00:00
[2025-02-24T01:15:32.101+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 23:10:00+00:00
[2025-02-24T01:15:32.104+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2784) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-24T01:15:32.110+0000] {standard_task_runner.py:72} INFO - Started process 2787 to run task
[2025-02-24T01:15:32.118+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T22:30:00+00:00', '--job-id', '112', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpgu68r3bp']
[2025-02-24T01:15:32.121+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-24 01:00:00+00:00
[2025-02-24T01:15:32.122+0000] {standard_task_runner.py:105} INFO - Job 112: Subtask process_landzone
[2025-02-24T01:15:32.121+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2785) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-24T01:15:32.131+0000] {standard_task_runner.py:72} INFO - Started process 2789 to run task
[2025-02-24T01:15:32.135+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T23:10:00+00:00', '--job-id', '113', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpex4bm9si']
[2025-02-24T01:15:32.140+0000] {standard_task_runner.py:105} INFO - Job 113: Subtask process_landzone
[2025-02-24T01:15:32.140+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2786) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-24T01:15:32.146+0000] {standard_task_runner.py:72} INFO - Started process 2790 to run task
[2025-02-24T01:15:32.160+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-24T01:00:00+00:00', '--job-id', '114', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpvzprrmfr']
[2025-02-24T01:15:32.169+0000] {standard_task_runner.py:105} INFO - Job 114: Subtask process_landzone
[2025-02-24T01:15:32.318+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T22:30:00+00:00 [running]> on host b06401153325
[2025-02-24T01:15:32.337+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:10:00+00:00 [running]> on host b06401153325
[2025-02-24T05:15:47.715+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T05:15:47.912+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T01:10:00+00:00 [queued]>
[2025-02-24T05:15:47.958+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T01:10:00+00:00 [queued]>
[2025-02-24T05:15:47.962+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-24T05:15:47.975+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T05:15:48.194+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T05:15:48.237+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T01:00:00+00:00 [queued]>
[2025-02-24T05:15:48.245+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T05:15:48.288+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-24 01:10:00+00:00
[2025-02-24T05:15:48.313+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T01:00:00+00:00 [queued]>
[2025-02-24T05:15:48.332+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-24T05:15:48.366+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:10:00+00:00 [queued]>
[2025-02-24T05:15:48.377+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T05:00:00+00:00 [queued]>
[2025-02-24T05:15:48.377+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2823) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-24T05:15:48.394+0000] {standard_task_runner.py:72} INFO - Started process 2827 to run task
[2025-02-24T05:15:48.406+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-24T01:10:00+00:00', '--job-id', '115', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpl3yyngfg']
[2025-02-24T05:15:48.415+0000] {standard_task_runner.py:105} INFO - Job 115: Subtask process_landzone
[2025-02-24T05:15:48.421+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:10:00+00:00 [queued]>
[2025-02-24T05:15:48.423+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T05:00:00+00:00 [queued]>
[2025-02-24T05:15:48.423+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-24T05:15:48.425+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-24T05:15:48.487+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-24 05:00:00+00:00
[2025-02-24T05:15:48.501+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-24 01:00:00+00:00
[2025-02-24T05:15:48.507+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-23 23:10:00+00:00
[2025-02-24T05:15:48.530+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-24T05:00:00+00:00', '--job-id', '118', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpc00gg36u']
[2025-02-24T05:15:48.530+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2824) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-24T05:15:48.540+0000] {standard_task_runner.py:72} INFO - Started process 2830 to run task
[2025-02-24T05:15:48.521+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2826) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-24T05:15:48.545+0000] {standard_task_runner.py:72} INFO - Started process 2829 to run task
[2025-02-24T05:15:48.539+0000] {standard_task_runner.py:105} INFO - Job 118: Subtask process_landzone
[2025-02-24T05:15:48.542+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2825) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-24T05:15:48.552+0000] {standard_task_runner.py:72} INFO - Started process 2831 to run task
[2025-02-24T05:15:48.549+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-24T01:00:00+00:00', '--job-id', '116', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmphc10zpry']
[2025-02-24T05:15:48.561+0000] {standard_task_runner.py:105} INFO - Job 116: Subtask process_landzone
[2025-02-24T05:15:48.565+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-23T23:10:00+00:00', '--job-id', '117', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpqyw6v171']
[2025-02-24T05:15:48.577+0000] {standard_task_runner.py:105} INFO - Job 117: Subtask process_landzone
[2025-02-24T05:15:48.819+0000] {task_command.py:467} INFO - Running <TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-23T23:10:00+00:00 [running]> on host b06401153325
[2025-02-24T05:15:49.097+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-24T01:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-24T01:10:00+00:00'
[2025-02-24T05:15:49.101+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-24T05:15:49.172+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-24T01:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-24T01:00:00+00:00'
[2025-02-24T05:15:49.174+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-24T05:15:49.175+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-24T05:15:49.177+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-23T23:10:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-23T23:10:00+00:00'
[2025-02-24T05:15:49.178+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-24T05:15:49.186+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-24T05:15:49.225+0000] {subprocess.py:99} INFO - Output:
[2025-02-24T05:15:49.241+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-24T05:15:49.242+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-24T05:15:49.245+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-24T05:15:49.245+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-24T05:15:49.249+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-24T05:15:49.252+0000] {taskinstance.py:3134} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='unimedDadosv1' AIRFLOW_CTX_DAG_ID='datalake_pipeline' AIRFLOW_CTX_TASK_ID='process_landzone' AIRFLOW_CTX_EXECUTION_DATE='2025-02-24T05:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-02-24T05:00:00+00:00'
[2025-02-24T05:15:49.255+0000] {taskinstance.py:732} INFO - ::endgroup::
[2025-02-24T05:15:49.280+0000] {subprocess.py:99} INFO - Output:
[2025-02-24T05:15:49.286+0000] {subprocess.py:99} INFO - Output:
[2025-02-24T05:15:49.433+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-24T05:15:49.316+0000] {subprocess.py:78} INFO - Tmp dir root location: /tmp
[2025-02-24T05:15:49.433+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-24T05:15:49.438+0000] {subprocess.py:88} INFO - Running command: ['/usr/bin/bash', '-c', 'bash /Volumes/MACBACKUP/workspaceDlake/dlake/src/scheduler/scheduler_0.sh']
[2025-02-24T05:15:49.483+0000] {subprocess.py:99} INFO - Output:
[2025-02-24T05:15:49.503+0000] {subprocess.py:106} INFO - Em processamento UPLOAD BRONZE...
[2025-02-24T05:15:53.541+0000] {job.py:229} INFO - Heartbeat recovered after 11.45 seconds
[2025-02-24T05:15:56.018+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-24T05:15:56.020+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 93, in <module>"
[2025-02-24T05:15:56.025+0000] {subprocess.py:106} INFO -     df_new = read_duckdb_table(db_file, table)
[2025-02-24T05:15:56.029+0000] {subprocess.py:106} INFO -              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.032+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 37, in read_duckdb_table"
[2025-02-24T05:15:56.052+0000] {subprocess.py:106} INFO -     conn = duckdb.connect(db_file)
[2025-02-24T05:15:56.055+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.057+0000] {subprocess.py:106} INFO - duckdb.duckdb.IOException: IO Error: Could not set lock on file ""/Volumes/MACBACKUP/workspaceDlake/dlake/db1.duckdb"": Conflicting lock is held in /usr/local/bin/python3.12 (PID 2843). See also https://duckdb.org/docs/connect/concurrency"
[2025-02-24T05:15:56.142+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-24T05:15:56.150+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-24T05:15:56.152+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 96, in <module>"
[2025-02-24T05:15:56.154+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
[2025-02-24T05:15:56.155+0000] {subprocess.py:106} INFO -     upsert_and_upload_csv(df_new, table, db_name)
"[2025-02-24T05:15:56.158+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 93, in <module>"
[2025-02-24T05:15:56.164+0000] {subprocess.py:106} INFO -     df_new = read_duckdb_table(db_file, table)
[2025-02-24T05:15:56.166+0000] {subprocess.py:106} INFO -              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.168+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 37, in read_duckdb_table"
[2025-02-24T05:15:56.170+0000] {subprocess.py:106} INFO -     conn = duckdb.connect(db_file)
[2025-02-24T05:15:56.172+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.174+0000] {subprocess.py:106} INFO - duckdb.duckdb.IOException: IO Error: Could not set lock on file ""/Volumes/MACBACKUP/workspaceDlake/dlake/db1.duckdb"": Conflicting lock is held in /usr/local/bin/python3.12 (PID 2839). See also https://duckdb.org/docs/connect/concurrency"
"[2025-02-24T05:15:56.160+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 85, in upsert_and_upload_csv"
"[2025-02-24T05:15:56.177+0000] {subprocess.py:106} INFO -     minio_client.fput_object(bucket_name, f""landzone/{csv_filename}"", temp_file)"
"[2025-02-24T05:15:56.179+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1051, in fput_object"
[2025-02-24T05:15:56.183+0000] {subprocess.py:106} INFO -     return self.put_object(
[2025-02-24T05:15:56.200+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.202+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1996, in put_object"
[2025-02-24T05:15:56.206+0000] {subprocess.py:106} INFO -     raise exc
"[2025-02-24T05:15:56.208+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1942, in put_object"
[2025-02-24T05:15:56.210+0000] {subprocess.py:106} INFO -     return self._put_object(
[2025-02-24T05:15:56.212+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.214+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1781, in _put_object"
[2025-02-24T05:15:56.219+0000] {subprocess.py:106} INFO -     response = self._execute(
[2025-02-24T05:15:56.221+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.226+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 438, in _execute"
[2025-02-24T05:15:56.235+0000] {subprocess.py:106} INFO -     region = self._get_region(bucket_name)
[2025-02-24T05:15:56.241+0000] {subprocess.py:106} INFO -              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.243+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 495, in _get_region"
[2025-02-24T05:15:56.244+0000] {subprocess.py:106} INFO -     response = self._url_open(
[2025-02-24T05:15:56.246+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.249+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 424, in _url_open"
[2025-02-24T05:15:56.251+0000] {subprocess.py:106} INFO -     raise response_error
[2025-02-24T05:15:56.253+0000] {subprocess.py:106} INFO - minio.error.S3Error: S3 operation failed; code: RequestTimeTooSkewed, message: The difference between the request time and the server's time is too large., resource: /landzone, request_id: 1827139DE4C1FA28, host_id: 7987905dee74cdeb212432486a178e511309594cee7cb75f892cd53e35f09ea4
[2025-02-24T05:15:56.331+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-24T05:15:56.333+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-24T05:15:56.335+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 96, in <module>"
[2025-02-24T05:15:56.339+0000] {subprocess.py:106} INFO -     upsert_and_upload_csv(df_new, table, db_name)
"[2025-02-24T05:15:56.342+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 85, in upsert_and_upload_csv"
"[2025-02-24T05:15:56.355+0000] {subprocess.py:106} INFO -     minio_client.fput_object(bucket_name, f""landzone/{csv_filename}"", temp_file)"
"[2025-02-24T05:15:56.357+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1051, in fput_object"
[2025-02-24T05:15:56.360+0000] {subprocess.py:106} INFO -     return self.put_object(
[2025-02-24T05:15:56.362+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.370+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1996, in put_object"
[2025-02-24T05:15:56.372+0000] {subprocess.py:106} INFO -     raise exc
"[2025-02-24T05:15:56.375+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1942, in put_object"
[2025-02-24T05:15:56.377+0000] {subprocess.py:106} INFO -     return self._put_object(
[2025-02-24T05:15:56.380+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.382+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 1781, in _put_object"
[2025-02-24T05:15:56.385+0000] {subprocess.py:106} INFO -     response = self._execute(
[2025-02-24T05:15:56.397+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-24T05:15:56.399+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-24T05:15:56.397+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.402+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 438, in _execute"
[2025-02-24T05:15:56.407+0000] {subprocess.py:106} INFO -     region = self._get_region(bucket_name)
[2025-02-24T05:15:56.410+0000] {subprocess.py:106} INFO -              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.414+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 495, in _get_region"
[2025-02-24T05:15:56.416+0000] {subprocess.py:106} INFO -     response = self._url_open(
[2025-02-24T05:15:56.420+0000] {subprocess.py:106} INFO -                ^^^^^^^^^^^^^^^
"[2025-02-24T05:15:56.422+0000] {subprocess.py:106} INFO -   File ""/home/***/.local/lib/python3.12/site-packages/minio/api.py"", line 424, in _url_open"
[2025-02-24T05:15:56.439+0000] {subprocess.py:106} INFO -     raise response_error
[2025-02-24T05:15:56.442+0000] {subprocess.py:106} INFO - minio.error.S3Error: S3 operation failed; code: RequestTimeTooSkewed, message: The difference between the request time and the server's time is too large., resource: /landzone, request_id: 1827139DF09BBAC8, host_id: 7987905dee74cdeb212432486a178e511309594cee7cb75f892cd53e35f09ea4
[2025-02-24T05:15:56.474+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T05:15:56.492+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-23T23:10:00+00:00, execution_date=20250223T231000, start_date=20250224T051548, end_date=20250224T051556
[2025-02-24T05:15:56.544+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-24T05:15:56.546+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-24T05:15:56.555+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-24T05:15:56.561+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-24T05:15:56.624+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T05:15:56.652+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T05:15:56.681+0000] {taskinstance.py:1226} INFO - Marking task as FAILED. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-24T01:00:00+00:00, execution_date=20250224T010000, start_date=20250224T051548, end_date=20250224T051556
[2025-02-24T05:15:56.766+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-24T05:15:56.845+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-24T05:15:56.780+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 117 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2831)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T05:15:56.919+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-24T05:00:00+00:00, execution_date=20250224T050000, start_date=20250224T051548, end_date=20250224T051556
[2025-02-24T05:15:56.938+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-24T05:15:57.027+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-24T05:15:57.110+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-24T05:15:57.117+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T05:15:57.157+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-24T01:10:00+00:00, execution_date=20250224T011000, start_date=20250224T051547, end_date=20250224T051557
[2025-02-24T05:15:57.240+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-24T05:15:57.247+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-24T05:15:57.242+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 118 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2829)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T05:15:57.255+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 116 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2830)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T05:15:57.298+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-24T05:15:57.336+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-24T05:15:57.370+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-24T05:15:57.381+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 115 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2827)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T05:15:57.405+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-24T05:15:57.412+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-24T05:15:57.460+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-24T05:15:57.464+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-24T05:15:57.476+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-24T05:15:57.545+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-24T05:15:57.549+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-02-24T07:26:06.313+0000] {job.py:229} INFO - Heartbeat recovered after 7804.99 seconds
[2025-02-24T07:26:15.539+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T07:26:15.658+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T05:00:00+00:00 [queued]>
[2025-02-24T07:26:15.864+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T05:00:00+00:00 [queued]>
[2025-02-24T07:26:15.868+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-24T07:26:16.049+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-24 05:00:00+00:00
[2025-02-24T07:26:16.164+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2870) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-24T07:26:16.174+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-24T05:00:00+00:00', '--job-id', '119', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmplnaa1h79']
[2025-02-24T07:26:16.289+0000] {standard_task_runner.py:105} INFO - Job 119: Subtask process_landzone
[2025-02-24T07:26:16.276+0000] {standard_task_runner.py:72} INFO - Started process 2872 to run task
[2025-02-24T07:26:16.330+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T07:26:16.440+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T01:10:00+00:00 [queued]>
[2025-02-24T07:26:16.460+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T07:26:16.471+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T01:10:00+00:00 [queued]>
[2025-02-24T07:26:16.473+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-24T07:26:16.529+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-24 01:10:00+00:00
[2025-02-24T07:26:16.534+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T07:10:00+00:00 [queued]>
[2025-02-24T08:37:06.182+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T08:37:06.190+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T08:37:06.200+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T08:37:06.863+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T07:10:00+00:00 [queued]>
[2025-02-24T08:37:06.866+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T07:20:00+00:00 [queued]>
[2025-02-24T08:37:06.899+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T23:00:00+00:00 [queued]>
[2025-02-24T08:37:06.910+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T07:10:00+00:00 [queued]>
[2025-02-24T08:37:06.914+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-24T08:37:06.979+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_bronze scheduled__2025-02-23T23:00:00+00:00 [queued]>
[2025-02-24T08:37:06.988+0000] {taskinstance.py:2867} INFO - Starting attempt 2 of 2
[2025-02-24T08:37:06.984+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T07:20:00+00:00 [queued]>
[2025-02-24T08:37:06.991+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
[2025-02-24T08:37:07.091+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-24 07:10:00+00:00
[2025-02-24T08:37:07.119+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_landzone> on 2025-02-24 07:20:00+00:00
[2025-02-24T08:37:07.144+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2980) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-24T08:37:07.172+0000] {standard_task_runner.py:72} INFO - Started process 2984 to run task
[2025-02-24T08:37:07.183+0000] {taskinstance.py:2890} INFO - Executing <Task(BashOperator): process_bronze> on 2025-02-23 23:00:00+00:00
[2025-02-24T08:37:07.192+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-24T07:10:00+00:00', '--job-id', '123', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpneo8g17u']
[2025-02-24T08:37:07.207+0000] {standard_task_runner.py:105} INFO - Job 123: Subtask process_landzone
[2025-02-24T08:37:07.193+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2981) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-24T08:37:07.221+0000] {standard_task_runner.py:72} INFO - Started process 2985 to run task
[2025-02-24T08:37:07.204+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_landzone', 'scheduled__2025-02-24T07:20:00+00:00', '--job-id', '122', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmp_kugh366']
[2025-02-24T08:37:07.225+0000] {standard_task_runner.py:105} INFO - Job 122: Subtask process_landzone
[2025-02-24T08:37:07.229+0000] {warnings.py:112} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:70: DeprecationWarning: This process (pid=2982) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()
[2025-02-24T08:37:07.247+0000] {standard_task_runner.py:72} INFO - Started process 2987 to run task
[2025-02-24T08:37:07.256+0000] {standard_task_runner.py:104} INFO - Running: ['***', 'tasks', 'run', 'datalake_pipeline', 'process_bronze', 'scheduled__2025-02-23T23:00:00+00:00', '--job-id', '124', '--raw', '--subdir', 'DAGS_FOLDER/dag_scheduler_v2.py', '--cfg-path', '/tmp/tmpzhx5jvmy']
[2025-02-24T08:37:07.305+0000] {standard_task_runner.py:105} INFO - Job 124: Subtask process_bronze
[2025-02-24T08:37:07.325+0000] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-02-24T08:37:07.495+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T08:20:00+00:00 [queued]>
[2025-02-24T08:37:07.532+0000] {taskinstance.py:2614} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: datalake_pipeline.process_landzone scheduled__2025-02-24T08:20:00+00:00 [queued]>
[2025-02-24T08:37:07.538+0000] {taskinstance.py:2867} INFO - Starting attempt 1 of 2
""
[2025-02-24T08:37:34.817+0000] {processor.py:914} INFO - Processing file /opt/airflow/dags/dag_scheduler_v1.py for tasks to queue
""
""
""
""
""
""
""
""
[2025-02-24T08:37:36.483+0000] {subprocess.py:106} INFO - TABLE : fornecedores BANCO db1
[2025-02-24T08:37:36.488+0000] {subprocess.py:106} INFO - Arquivo fornecedores_db1.csv atualizado e enviado para MinIO!
[2025-02-24T08:37:36.491+0000] {subprocess.py:106} INFO - Traceback (most recent call last):
"[2025-02-24T08:37:36.493+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 93, in <module>"
[2025-02-24T08:37:36.501+0000] {subprocess.py:106} INFO -     df_new = read_duckdb_table(db_file, table)
[2025-02-24T08:37:36.503+0000] {subprocess.py:106} INFO -              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T08:37:36.506+0000] {subprocess.py:106} INFO -   File ""/Volumes/MACBACKUP/workspaceDlake/dlake/src/0_uploadLandzone/0_landzone.py"", line 37, in read_duckdb_table"
[2025-02-24T08:37:36.519+0000] {subprocess.py:106} INFO -     conn = duckdb.connect(db_file)
[2025-02-24T08:37:36.525+0000] {subprocess.py:106} INFO -            ^^^^^^^^^^^^^^^^^^^^^^^
"[2025-02-24T08:37:36.531+0000] {subprocess.py:106} INFO - duckdb.duckdb.IOException: IO Error: Could not set lock on file ""/Volumes/MACBACKUP/workspaceDlake/dlake/db1.duckdb"": Conflicting lock is held in /usr/local/bin/python3.12 (PID 2995). See also https://duckdb.org/docs/connect/concurrency"
[2025-02-24T08:37:37.076+0000] {subprocess.py:106} INFO - Error: O script Python encontrou um problema
[2025-02-24T08:37:37.112+0000] {subprocess.py:110} INFO - Command exited with return code 1
[2025-02-24T08:37:37.122+0000] {processor.py:925} INFO - DAG(s) 'datalake_pipeline' retrieved from /opt/airflow/dags/dag_scheduler_v2.py
[2025-02-24T08:37:37.367+0000] {taskinstance.py:3313} ERROR - Task failed with exception
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T08:37:37.442+0000] {logging_mixin.py:190} INFO - [2025-02-24T08:37:37.441+0000] {dag.py:3239} INFO - Sync 1 DAGs
[2025-02-24T08:37:37.518+0000] {taskinstance.py:1226} INFO - Marking task as UP_FOR_RETRY. dag_id=datalake_pipeline, task_id=process_landzone, run_id=scheduled__2025-02-24T07:20:00+00:00, execution_date=20250224T072000, start_date=20250224T083706, end_date=20250224T083737
[2025-02-24T08:37:37.746+0000] {taskinstance.py:341} INFO - ::group::Post task execution logs
[2025-02-24T08:37:37.758+0000] {logging_mixin.py:190} INFO - [2025-02-24T08:37:37.757+0000] {dag.py:4180} INFO - Setting next_dagrun for datalake_pipeline to 2025-02-24 08:30:00+00:00, run_after=2025-02-24 08:40:00+00:00
[2025-02-24T08:37:37.840+0000] {standard_task_runner.py:124} ERROR - Failed to execute job 122 for task process_landzone (Bash command failed. The command returned a non-zero exit code 1.; 2985)
Traceback (most recent call last):
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py"", line 117, in _start_by_fork"
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/cli_config.py"", line 49, in command"
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/cli.py"", line 116, in wrapper"
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 483, in task_run"
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 256, in _run_task_by_selected_method"
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/cli/commands/task_command.py"", line 341, in _run_raw_task"
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/session.py"", line 97, in wrapper"
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3006, in _run_raw_task"
    return _run_raw_task(
           ^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 274, in _run_raw_task"
    TaskInstance._execute_task_with_callbacks(
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3161, in _execute_task_with_callbacks"
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 3185, in _execute_task"
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 768, in _execute_task"
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py"", line 734, in _execute_callable"
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/utils/operator_helpers.py"", line 252, in run"
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py"", line 424, in wrapper"
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
"  File ""/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/bash.py"", line 276, in execute"
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-02-24T08:37:37.965+0000] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-02-24T08:37:37.984+0000] {processor.py:208} INFO - Processing /opt/airflow/dags/dag_scheduler_v2.py took 2.699 seconds
[2025-02-24T08:37:38.037+0000] {taskinstance.py:3901} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-02-24T08:37:38.181+0000] {local_task_job_runner.py:245} INFO - ::endgroup::
